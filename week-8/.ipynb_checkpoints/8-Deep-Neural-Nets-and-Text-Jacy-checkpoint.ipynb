{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week - 8 - Deep Neural Nets and Text\n",
    "\n",
    "In this week we introduce the use of Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network--the one-layer perceptron--to classify text. It performed quite well, but comes up short in more sophisticated classification tasks, such as in predicting intent. We have also seen slightly deeper, 2-level neural nets in the form of word embeddings such as Word2Vec. While they work well, they have some drawbacks, such as representing words with multiple meanings in a singular space. \n",
    "\n",
    "BERT, which is a language model built using bidirectional encoders, allows us to take advantage of a powerful pre-trained model which we can then use to perform our own tasks based on data we analyze. \n",
    "\n",
    "In this notebook we use ```huggingface/transformers```, a python package that allows for easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, but to reach state-of-the-art results we will stick with using BERT and similar models that can be tuned to extremely high performance on specific language understanding and generation tasks.\n",
    "\n",
    "To demonstrate this, we begin by using the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also use BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences. There are a number of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
    "\n",
    "## NOTE\n",
    "\n",
    "This notebook **requires** GPUs for training models in section 1 and section 3. To train models, please use this [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) to create the models. Note that I have only given you view access: please create your own colab file to train your models, using the code and instructions I have given in the Colab file. So while you have to do the homework on this notebook, the models which you will train should be done on Google Colab, which has GPU access. If you happen to have GPU access on your personal machines or some other way to train the models, you are welcome to do that too.\n",
    "\n",
    "Note that if you run the computationally intensive models on your local computer they will take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.4.1\n",
      "  Using cached transformers-2.4.1-py3-none-any.whl (475 kB)\n",
      "Collecting tokenizers==0.0.11\n",
      "  Using cached tokenizers-0.0.11-cp38-cp38-win_amd64.whl (797 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (1.19.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (0.1.95)\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (1.16.53)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (2020.10.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (4.50.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (0.0.43)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.4.1) (0.3.4)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.53 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.4.1) (1.19.53)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.4.1) (0.10.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.4.1) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.4.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.4.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.4.1) (2020.6.20)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.4.1) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.4.1) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\jacy\\appdata\\roaming\\python\\python38\\site-packages (from sacremoses->transformers==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\jacy\\appdata\\roaming\\python\\python38\\site-packages (from botocore<1.20.0,>=1.19.53->boto3->transformers==2.4.1) (2.8.1)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.1\n",
      "    Uninstalling tokenizers-0.10.1:\n",
      "      Successfully uninstalled tokenizers-0.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\~~kenizers\\\\tokenizers.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: sentence-transformers in c:\\programdata\\anaconda3\\lib\\site-packages (0.4.1.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.50.2)\n",
      "Requirement already satisfied, skipping upgrade: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.5)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.95)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.23.2)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.10.15)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.1-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied, skipping upgrade: packaging in c:\\users\\jacy\\appdata\\roaming\\python\\python38\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.8)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\jacy\\appdata\\roaming\\python\\python38\\site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\jacy\\appdata\\roaming\\python\\python38\\site-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.0.11\n",
      "    Uninstalling tokenizers-0.0.11:\n",
      "      Successfully uninstalled tokenizers-0.0.11\n",
      "Successfully installed tokenizers-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pip install torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertConfig # pip install tranformers==2.4.1\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoLA Dataset and pre-processing\n",
    "\n",
    "We start with loading our dataset and pre-processing it. The pre-processing follows similar steps as we have done in the past, but we will be using pre-written modules offered by the transformers package. These are some of the things we have to take care of when using this particular BERT model.\n",
    "\n",
    "    -special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "    -tokens that conforms with the fixed vocabulary used in BERT\n",
    "    -token IDs from BERTâ€™s tokenizer\n",
    "    -mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    -segment IDs used to distinguish different sentences\n",
    "    -positional embeddings used to show token position within the sequence\n",
    "\n",
    "\n",
    "We will be using parts of the code from [this notebook](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=BJR6t_gCQe_x) which walks us through the process of using a pre-trained BERT model. The interface to use these models comes from the package [huggingface/transformers](https://github.com/huggingface/transformers). \n",
    "\n",
    "We start by setting up our GPU if we can - this may not work on your machine, so it has been commented out.\n",
    "\n",
    "An aside: check out this tutorial too - https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Deep Neural Nets\n",
    "\n",
    "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
    "\n",
    "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "241/241 [==============================] - 11s 39ms/step - loss: 0.6210 - accuracy: 0.6923\n",
      "Epoch 2/10\n",
      "241/241 [==============================] - 9s 39ms/step - loss: 0.6059 - accuracy: 0.7073\n",
      "Epoch 3/10\n",
      "241/241 [==============================] - 9s 39ms/step - loss: 0.6096 - accuracy: 0.7030\n",
      "Epoch 4/10\n",
      "241/241 [==============================] - 9s 39ms/step - loss: 0.6105 - accuracy: 0.7023\n",
      "Epoch 5/10\n",
      "241/241 [==============================] - 9s 39ms/step - loss: 0.6103 - accuracy: 0.7016\n",
      "Epoch 6/10\n",
      "241/241 [==============================] - 10s 40ms/step - loss: 0.6043 - accuracy: 0.7091\n",
      "Epoch 7/10\n",
      "241/241 [==============================] - 10s 41ms/step - loss: 0.6048 - accuracy: 0.7075\n",
      "Epoch 8/10\n",
      "241/241 [==============================] - 10s 43ms/step - loss: 0.6071 - accuracy: 0.7072\n",
      "Epoch 9/10\n",
      "241/241 [==============================] - 10s 43ms/step - loss: 0.6117 - accuracy: 0.6995\n",
      "Epoch 10/10\n",
      "241/241 [==============================] - 10s 41ms/step - loss: 0.6082 - accuracy: 0.7045\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] Our friends won't buy this analysis, let alone the next one we propose. [SEP]\",\n",
       " \"[CLS] One more pseudo generalization and I'm giving up. [SEP]\",\n",
       " \"[CLS] One more pseudo generalization or I'm giving up. [SEP]\",\n",
       " '[CLS] The more we study verbs, the crazier they get. [SEP]',\n",
       " '[CLS] Day by day the facts are getting murkier. [SEP]',\n",
       " \"[CLS] I'll fix you a drink. [SEP]\",\n",
       " '[CLS] Fred watered the plants flat. [SEP]',\n",
       " '[CLS] Bill coughed his way out of the restaurant. [SEP]',\n",
       " \"[CLS] We're dancing the night away. [SEP]\",\n",
       " '[CLS] Herman hammered the metal flat. [SEP]',\n",
       " '[CLS] The critics laughed the play off the stage. [SEP]',\n",
       " '[CLS] The pond froze solid. [SEP]',\n",
       " '[CLS] Bill rolled out of the room. [SEP]',\n",
       " '[CLS] The gardener watered the flowers flat. [SEP]',\n",
       " '[CLS] The gardener watered the flowers. [SEP]',\n",
       " '[CLS] Bill broke the bathtub into pieces. [SEP]',\n",
       " '[CLS] Bill broke the bathtub. [SEP]',\n",
       " '[CLS] They drank the pub dry. [SEP]',\n",
       " '[CLS] They drank the pub. [SEP]',\n",
       " '[CLS] The professor talked us into a stupor. [SEP]',\n",
       " '[CLS] The professor talked us. [SEP]',\n",
       " '[CLS] We yelled ourselves hoarse. [SEP]',\n",
       " '[CLS] We yelled ourselves. [SEP]',\n",
       " '[CLS] We yelled Harry hoarse. [SEP]',\n",
       " '[CLS] Harry coughed himself into a fit. [SEP]',\n",
       " '[CLS] Harry coughed himself. [SEP]',\n",
       " '[CLS] Harry coughed us into a fit. [SEP]',\n",
       " '[CLS] Bill followed the road into the forest. [SEP]',\n",
       " '[CLS] We drove Highway 5 from SD to SF. [SEP]',\n",
       " '[CLS] Fred tracked the leak to its source. [SEP]',\n",
       " '[CLS] John danced waltzes across the room. [SEP]',\n",
       " '[CLS] Bill urinated out the window. [SEP]',\n",
       " '[CLS] Bill coughed out the window. [SEP]',\n",
       " '[CLS] Bill bled on the floor. [SEP]',\n",
       " '[CLS] The toilet leaked through the floor into the kitchen below. [SEP]',\n",
       " '[CLS] Bill ate off the floor. [SEP]',\n",
       " '[CLS] Bill drank from the hose. [SEP]',\n",
       " '[CLS] This metal hammers flat easily. [SEP]',\n",
       " '[CLS] They made him president. [SEP]',\n",
       " '[CLS] They made him angry. [SEP]',\n",
       " '[CLS] They caused him to become angry by making him. [SEP]',\n",
       " '[CLS] They caused him to become president by making him. [SEP]',\n",
       " '[CLS] They made him to exhaustion. [SEP]',\n",
       " '[CLS] They made him into a monster. [SEP]',\n",
       " '[CLS] The trolley rumbled through the tunnel. [SEP]',\n",
       " '[CLS] The wagon rumbled down the road. [SEP]',\n",
       " '[CLS] The bullets whistled past the house. [SEP]',\n",
       " '[CLS] The knee replacement candidate groaned up the stairs. [SEP]',\n",
       " '[CLS] The car honked down the road. [SEP]',\n",
       " '[CLS] The dog barked out of the room. [SEP]',\n",
       " '[CLS] The dog barked its way out of the room. [SEP]',\n",
       " '[CLS] Bill whistled his way past the house. [SEP]',\n",
       " '[CLS] The witch vanished into the forest. [SEP]',\n",
       " '[CLS] Bill disappeared down the road. [SEP]',\n",
       " '[CLS] The witch went into the forest by vanishing. [SEP]',\n",
       " '[CLS] The witch went into the forest and thereby vanished. [SEP]',\n",
       " '[CLS] The building is tall and wide. [SEP]',\n",
       " '[CLS] The building is tall and tall. [SEP]',\n",
       " '[CLS] This building is taller and wider than that one. [SEP]',\n",
       " '[CLS] This building got taller and wider than that one. [SEP]',\n",
       " '[CLS] This building got taller and taller. [SEP]',\n",
       " '[CLS] This building is taller and taller. [SEP]',\n",
       " '[CLS] This building got than that one. [SEP]',\n",
       " '[CLS] This building is than that one. [SEP]',\n",
       " '[CLS] Bill floated into the cave. [SEP]',\n",
       " '[CLS] Bill floated into the cave for hours. [SEP]',\n",
       " '[CLS] Bill pushed Harry off the sofa for hours. [SEP]',\n",
       " '[CLS] Bill floated down the river for hours. [SEP]',\n",
       " '[CLS] Bill floated down the river. [SEP]',\n",
       " '[CLS] Bill pushed Harry along the trail for hours. [SEP]',\n",
       " '[CLS] Bill pushed Harry along the trail. [SEP]',\n",
       " '[CLS] The road zigzagged down the hill. [SEP]',\n",
       " '[CLS] The rope stretched over the pulley. [SEP]',\n",
       " '[CLS] The weights stretched the rope over the pulley. [SEP]',\n",
       " '[CLS] The weights kept the rope stretched over the pulley. [SEP]',\n",
       " '[CLS] Sam cut himself free. [SEP]',\n",
       " '[CLS] Sam got free by cutting his finger. [SEP]',\n",
       " '[CLS] Bill cried himself to sleep. [SEP]',\n",
       " '[CLS] Bill cried Sue to sleep. [SEP]',\n",
       " '[CLS] Bill squeezed himself through the hole. [SEP]',\n",
       " '[CLS] Bill sang himself to sleep. [SEP]',\n",
       " '[CLS] Bill squeezed the puppet through the hole. [SEP]',\n",
       " '[CLS] Bill sang Sue to sleep. [SEP]',\n",
       " '[CLS] The elevator rumbled itself to the ground. [SEP]',\n",
       " '[CLS] If the telephone rang, it could ring itself silly. [SEP]',\n",
       " '[CLS] She yelled hoarse. [SEP]',\n",
       " '[CLS] Ted cried to sleep. [SEP]',\n",
       " '[CLS] The tiger bled to death. [SEP]',\n",
       " '[CLS] He coughed awake and we were all overjoyed, especially Sierra. [SEP]',\n",
       " '[CLS] John coughed awake, rubbing his nose and cursing under his breath. [SEP]',\n",
       " '[CLS] John coughed himself awake on the bank of the lake where he and Bill had their play. [SEP]',\n",
       " '[CLS] Ron yawned himself awake. [SEP]',\n",
       " '[CLS] She coughed herself awake as the leaf landed on her nose. [SEP]',\n",
       " '[CLS] The worm wriggled onto the carpet. [SEP]',\n",
       " '[CLS] The chocolate melted onto the carpet. [SEP]',\n",
       " '[CLS] The ball wriggled itself loose. [SEP]',\n",
       " '[CLS] Bill wriggled himself loose. [SEP]',\n",
       " '[CLS] Aliza wriggled her tooth loose. [SEP]',\n",
       " '[CLS] The off center spinning flywheel shook itself loose. [SEP]',\n",
       " '[CLS] The more you eat, the less you want. [SEP]',\n",
       " '[CLS] If you eat more, you want correspondingly less. [SEP]',\n",
       " '[CLS] When you eat more, you want correspondingly less. [SEP]',\n",
       " '[CLS] As you eat more, you want correspondingly less. [SEP]',\n",
       " '[CLS] The most you want, the least you eat. [SEP]',\n",
       " '[CLS] The angrier Sue gets, the more Fred admires her. [SEP]',\n",
       " '[CLS] The more that you eat, the less that you want. [SEP]',\n",
       " '[CLS] The angrier that Sue gets, the more that Fred admires her. [SEP]',\n",
       " '[CLS] I think that the more you eat, the less you want. [SEP]',\n",
       " \"[CLS] I'm not shocked by the idea that the more you eat, the less you want. [SEP]\",\n",
       " '[CLS] It is obvious that the more you eat, the less you want. [SEP]',\n",
       " '[CLS] It is not entirely clear if the more you eat, the less you want. [SEP]',\n",
       " '[CLS] I want to explain exactly why the more you eat, the less you want. [SEP]',\n",
       " '[CLS] I demand that the more John eats, the more he pays. [SEP]',\n",
       " '[CLS] I demand that the more John eat, the more he pay. [SEP]',\n",
       " '[CLS] I demand that John pay more, the more he eats. [SEP]',\n",
       " '[CLS] I demand that John pays more, the more he eat. [SEP]',\n",
       " \"[CLS] You get angrier, the more we eat, don't you. [SEP]\",\n",
       " \"[CLS] You get angrier, the more we eat, don't we. [SEP]\",\n",
       " '[CLS] The harder it has rained, how much faster a flow that appears in the river? [SEP]',\n",
       " '[CLS] The harder it has rained, how much faster a flow appears in the river? [SEP]',\n",
       " '[CLS] The harder it rains, how much faster that do you run? [SEP]',\n",
       " '[CLS] The harder it rains, how much faster do you run? [SEP]',\n",
       " '[CLS] The harder it rains, how much faster a flow do you see in the river? [SEP]',\n",
       " '[CLS] The harder it rains, how much faster a flow that do you see in the river? [SEP]',\n",
       " '[CLS] When it rains harder, how much faster a flow appears in the river? [SEP]',\n",
       " '[CLS] As it rains harder, how much faster a flow appears in the river? [SEP]',\n",
       " '[CLS] As it rains harder, how much faster a flow that appears in the river? [SEP]',\n",
       " '[CLS] When it rains harder, how much faster a flow that appears in the river? [SEP]',\n",
       " '[CLS] How much harder has it rained, the faster a flow you see in the river? [SEP]',\n",
       " '[CLS] How much harder has it rained, when you see a faster flow in the river? [SEP]',\n",
       " '[CLS] The more John eats, the tighter keep your mouth shut about it. [SEP]',\n",
       " '[CLS] The more everyone eat, the more John keeps his big mouth shut about it, OK? [SEP]',\n",
       " '[CLS] When John eats more, keep your mouth shut tighter, OK? [SEP]',\n",
       " '[CLS] As John eats more, keep your mouth shut tighter, OK? [SEP]',\n",
       " '[CLS] Keep your mouth shut tighter, the more John eats, OK? [SEP]',\n",
       " '[CLS] Everyone keep your mouth shut tighter, the more John eats, OK? [SEP]',\n",
       " '[CLS] I can well imagine the more him eating, the fatter him getting. [SEP]',\n",
       " '[CLS] Bill can well imagine getting fat. [SEP]',\n",
       " '[CLS] Bill can well imagine the more he eats, the fatter getting. [SEP]',\n",
       " '[CLS] Fred can well imagine Joe getting fatter, the more he eats. [SEP]',\n",
       " '[CLS] It is important the more you eat, the more careful to be. [SEP]',\n",
       " '[CLS] It is important for the more you eat, the more careful to be. [SEP]',\n",
       " '[CLS] It is important the more you to eat, the more careful to be. [SEP]',\n",
       " '[CLS] It is important the more you eat, the more careful you to be. [SEP]',\n",
       " '[CLS] It is important the more you eat, the more careful for you to be. [SEP]',\n",
       " '[CLS] It is important for the more you to eat, the more careful to be. [SEP]',\n",
       " '[CLS] It is important for the more you to eat, the more careful for you to be. [SEP]',\n",
       " '[CLS] It is important the more you to eat, the more careful for you to be. [SEP]',\n",
       " '[CLS] It is important for the more you eat, the more careful you to be. [SEP]',\n",
       " '[CLS] It is important for you to be more careful, the more you eat. [SEP]',\n",
       " '[CLS] It is important to be more careful, the more you eat. [SEP]',\n",
       " '[CLS] I can well imagine quickly Mary answering the question. [SEP]',\n",
       " '[CLS] I can well imagine with a hatchet Mary destroying the Jeep. [SEP]',\n",
       " '[CLS] I can well imagine if he eats more, him getting fat. [SEP]',\n",
       " '[CLS] It is not entirely obvious if, Mary listens to the Grateful Dead, she gets depressed. [SEP]',\n",
       " '[CLS] It is not entirely obvious whether, Mary listens to the Grateful Dead, she gets depressed. [SEP]',\n",
       " '[CLS] Mary listens to the Grateful Dead and she gets depressed. [SEP]',\n",
       " '[CLS] If Mary listens to the Grateful Dead, she gets depressed. [SEP]',\n",
       " '[CLS] When Mary listens to the Grateful Dead, she gets depressed. [SEP]',\n",
       " '[CLS] Mary gets depressed if she listens to the Grateful Dead. [SEP]',\n",
       " '[CLS] Mary gets depressed when she listens to the Grateful Dead. [SEP]',\n",
       " '[CLS] The more she looked at pictures, the angrier Mary got. [SEP]',\n",
       " '[CLS] The more pictures Mary looked at, she got angrier and angrier. [SEP]',\n",
       " '[CLS] Mary gets depressed and she listens to the Grateful Dead. [SEP]',\n",
       " '[CLS] The higher the stakes are, the lower his expectations are. [SEP]',\n",
       " '[CLS] The higher the stakes, the lower his expectations. [SEP]',\n",
       " '[CLS] His expectations are lower, the higher the stakes. [SEP]',\n",
       " '[CLS] His expectations are lower, the higher the stakes are. [SEP]',\n",
       " '[CLS] His expectations lower, the higher the stakes. [SEP]',\n",
       " '[CLS] His expectations lower, the higher the stakes are. [SEP]',\n",
       " '[CLS] The more obnoxious Fred is, the less attention you should pay to him. [SEP]',\n",
       " '[CLS] The more obnoxious Fred, the less attention you should pay to him. [SEP]',\n",
       " '[CLS] The more Fred is obnoxious, the less you should pay attention to him. [SEP]',\n",
       " '[CLS] The more obnoxious Fred, the less you should pay attention to him. [SEP]',\n",
       " '[CLS] His expectations are always lower than mine. [SEP]',\n",
       " '[CLS] John was lots more obnoxious than Fred was. [SEP]',\n",
       " '[CLS] You should always lock your door, no matter how fancy the hotel might be. [SEP]',\n",
       " '[CLS] You should always lock your door, no matter how fancy the hotel. [SEP]',\n",
       " \"[CLS] I don't plan to lock the door, no matter how fancy this hotel is. [SEP]\",\n",
       " \"[CLS] I don't plan to lock the door, no matter how fancy this hotel. [SEP]\",\n",
       " \"[CLS] I'm going out, whatever the weather. [SEP]\",\n",
       " \"[CLS] I'm going out, wherever that hurricane might be. [SEP]\",\n",
       " \"[CLS] I'm going out, wherever that hurricane. [SEP]\",\n",
       " '[CLS] The more examples Mary says that Bill has helped Fred to discover the less I believe her. [SEP]',\n",
       " '[CLS] The more food Mary knows a man that eats the poorer she gets. [SEP]',\n",
       " '[CLS] The fatter he goes to a doctor when he gets the more he eats. [SEP]',\n",
       " '[CLS] The fatter that that he gets bothers him, the more he eats. [SEP]',\n",
       " '[CLS] The more books I ask to whom he will give, the more he reads. [SEP]',\n",
       " '[CLS] The more people I ask what he will give to the more he reads. [SEP]',\n",
       " \"[CLS] The more carefully he words the letter the safer he'll be. [SEP]\",\n",
       " \"[CLS] The more carefully he knows a man that worded the letter the safer he'll be. [SEP]\",\n",
       " '[CLS] The more geniuses John meets, the angrier he gets. [SEP]',\n",
       " '[CLS] The more John meets geniuses, the angrier he gets. [SEP]',\n",
       " \"[CLS] The more people you say will buy tickets, the happier I'll be. [SEP]\",\n",
       " \"[CLS] The more people you say that will buy tickets, the happier I'll be. [SEP]\",\n",
       " \"[CLS] The more people you say that right after the show opens will buy tickets, the happier I'll be. [SEP]\",\n",
       " '[CLS] The more I talk to Joe, the less about linguistics I am inclined to think Sally has taught him to appreciate. [SEP]',\n",
       " '[CLS] The more he eats, the poorer he knows a woman that gets. [SEP]',\n",
       " '[CLS] The more he eats, the fatter he goes to a doctor when he gets. [SEP]',\n",
       " '[CLS] The more he eats, the fatter that that he gets really bothers me. [SEP]',\n",
       " '[CLS] The more he reads, the more books I wonder to whom he will give. [SEP]',\n",
       " '[CLS] The more he reads, the more people I wonder what he will give to. [SEP]',\n",
       " '[CLS] The sooner you call, the more carefully I know a man that will word the letter. [SEP]',\n",
       " '[CLS] The richer John gets, the more geniuses John meets. [SEP]',\n",
       " '[CLS] The richer he gets, the more John meets geniuses. [SEP]',\n",
       " '[CLS] The more articles he reads, the fewer people he thinks will go into linguistics. [SEP]',\n",
       " '[CLS] The more articles he reads, the fewer people he thinks that will go into linguistics. [SEP]',\n",
       " '[CLS] The more articles he reads, the fewer people he thinks that under the current circumstances will go into linguistics. [SEP]',\n",
       " '[CLS] The more articles he reads, the fewer people he thinks under the current circumstances will go into linguistics. [SEP]',\n",
       " '[CLS] The more people that arrive, the louder that it gets. [SEP]',\n",
       " '[CLS] The more people that arrive, the louder it gets. [SEP]',\n",
       " '[CLS] The more people you give beer to, the more people that get sick. [SEP]',\n",
       " '[CLS] The more people that you give beer to, the more people that get sick. [SEP]',\n",
       " '[CLS] The more people arrive, the louder that it gets. [SEP]',\n",
       " '[CLS] The more people arrive, the louder it gets. [SEP]',\n",
       " '[CLS] The more people that you give beer to, the more people get sick. [SEP]',\n",
       " '[CLS] The more pictures of John that he buys the more arrogant he becomes. [SEP]',\n",
       " '[CLS] The more pictures of himself that John buys the more arrogant he becomes. [SEP]',\n",
       " '[CLS] The man that arrived on the train was my brother. [SEP]',\n",
       " '[CLS] The man arrived on the train was my brother. [SEP]',\n",
       " '[CLS] The more people everyone who likes pays attention to, the happier we all are. [SEP]',\n",
       " '[CLS] The later it gets, the more people everyone who likes pays attention to. [SEP]',\n",
       " '[CLS] Whenever Bill smokes, Susan hates him all the more. [SEP]',\n",
       " '[CLS] Whenever Bill smokes, Susan hates him much more. [SEP]',\n",
       " '[CLS] Whenever Bill smokes, Susan hates him far more. [SEP]',\n",
       " '[CLS] Whenever Bill smokes, Susan hates him a lot more. [SEP]',\n",
       " '[CLS] Once Janet left, Fred became all the crazier. [SEP]',\n",
       " '[CLS] Once Janet left, Fred became much crazier. [SEP]',\n",
       " '[CLS] Once Janet left, Fred became far crazier. [SEP]',\n",
       " '[CLS] Fred became all the crazier, the more often Janet left. [SEP]',\n",
       " '[CLS] When Bill smokes, all the more does Susan hate him. [SEP]',\n",
       " '[CLS] When Bill smokes, much more does Susan hate him. [SEP]',\n",
       " '[CLS] When Bill smokes, all the more Susan hates him. [SEP]',\n",
       " '[CLS] So much did you eat that everyone gasped. [SEP]',\n",
       " '[CLS] So fast did you run that everyone gasped. [SEP]',\n",
       " '[CLS] So intelligent a dog did you buy that everyone gasped. [SEP]',\n",
       " '[CLS] I know how much you ate. [SEP]',\n",
       " '[CLS] I know how fast you ran. [SEP]',\n",
       " '[CLS] I know how intelligent a dog you bought. [SEP]',\n",
       " '[CLS] He ate so much that he got sick. [SEP]',\n",
       " '[CLS] So much did he eat that he got sick. [SEP]',\n",
       " '[CLS] The more you eat, the more you want. [SEP]',\n",
       " '[CLS] You eat the more, the more you want. [SEP]',\n",
       " '[CLS] The more you eat, you want the more. [SEP]',\n",
       " '[CLS] I wonder you ate how much. [SEP]',\n",
       " '[CLS] I wonder to how many people Bill talks. [SEP]',\n",
       " '[CLS] The longer he has to wait, the angrier John gets. [SEP]',\n",
       " '[CLS] If he has to wait, John gets angry. [SEP]',\n",
       " '[CLS] He gets angry, the longer John has to wait. [SEP]',\n",
       " '[CLS] He gets angry if John has to wait. [SEP]',\n",
       " '[CLS] The more that pictures of him appear in the news, the more embarrassed John becomes. [SEP]',\n",
       " '[CLS] The more pictures of himself that appear in the news, the more embarrassed John becomes. [SEP]',\n",
       " '[CLS] The more that pictures of himself appear in the news, the more embarrassed John becomes. [SEP]',\n",
       " '[CLS] The more pictures of him appear in the news, the more likely John is to get arrested. [SEP]',\n",
       " '[CLS] The more pictures of himself appear in the news, the more likely John is to get arrested. [SEP]',\n",
       " '[CLS] The more that pictures of him appear in the news, the more likely John is to get arrested. [SEP]',\n",
       " '[CLS] The more that pictures of himself appear in the news, the more likely John is to get arrested. [SEP]',\n",
       " '[CLS] The more that John gets upset by them, the more that stories about him seem to show up in the news. [SEP]',\n",
       " '[CLS] The more that John gets upset by them, the more that stories about himself seem to show up in the news. [SEP]',\n",
       " '[CLS] John is more embarrassed, the more pictures of him appear in the news. [SEP]',\n",
       " '[CLS] John is more embarrassed, the more pictures of him that appear in the news. [SEP]',\n",
       " '[CLS] John is more embarrassed, the more pictures of himself appear in the news. [SEP]',\n",
       " '[CLS] John is more embarrassed, the more pictures of himself that appear in the news. [SEP]',\n",
       " '[CLS] Stories about him seem to show up more on the evening news, the more that John gets upset by them. [SEP]',\n",
       " '[CLS] Stories about himself seem to show up more on the evening news, the more that John gets upset by them. [SEP]',\n",
       " '[CLS] If you give him enough opportunity, every senator will succumb to corruption. [SEP]',\n",
       " '[CLS] You give him enough opportunity and every senator will succumb to corruption. [SEP]',\n",
       " '[CLS] We gave him enough opportunity and, sure enough, every senator succumbed to corruption. [SEP]',\n",
       " '[CLS] If you give any senator enough opportunity, he will succumb to corruption. [SEP]',\n",
       " '[CLS] You give any senator enough opportunity and he will succumb to corruption. [SEP]',\n",
       " '[CLS] You give every senator enough opportunity and he will succumb to corruption. [SEP]',\n",
       " '[CLS] We gave any senator enough opportunity and, sure enough, he succumbed to corruption. [SEP]',\n",
       " '[CLS] We gave every senator enough opportunity and, sure enough, he succumbed to corruption. [SEP]',\n",
       " '[CLS] The more lobbyists he talks to, the more corrupt every senator seems to become. [SEP]',\n",
       " '[CLS] The more lobbyists wine and dine him, the more every senator is susceptible to corruption. [SEP]',\n",
       " '[CLS] The more time that every senator spends with lobbyists, the more likely he succumbs to corruption. [SEP]',\n",
       " '[CLS] Every senator becomes more corrupt, the more lobbyists he talks to. [SEP]',\n",
       " '[CLS] Any senator becomes more corrupt, the more lobbyists he talks to. [SEP]',\n",
       " '[CLS] He seems to become more corrupt, the more lobbyists any senator talks to. [SEP]',\n",
       " '[CLS] He seems to become more corrupt, the more lobbyists every senator talks to. [SEP]',\n",
       " '[CLS] Every senator seems to become more corrupt, if he talks to more lobbyists. [SEP]',\n",
       " '[CLS] Any senator seems to become more corrupt, if he talks to more lobbyists. [SEP]',\n",
       " '[CLS] Any senator seems to become more corrupt, as he talks to more lobbyists. [SEP]',\n",
       " '[CLS] He seems to become more corrupt, if any senator talks to more lobbyists. [SEP]',\n",
       " '[CLS] He seems to become more corrupt, if every senator talks to more lobbyists. [SEP]',\n",
       " '[CLS] He seems to become more corrupt, as every senator talks to more lobbyists. [SEP]',\n",
       " '[CLS] He seems to become more corrupt, as any senator talks to more lobbyists. [SEP]',\n",
       " \"[CLS] The sooner you solve this problem, the more easily you'll satisfy the folks up at corporate headquarters. [SEP]\",\n",
       " \"[CLS] This is the sort of problem which the sooner you solve the more easily you'll satisfy the folks up at corporate headquarters. [SEP]\",\n",
       " \"[CLS] The folks up at corporate headquarters are the sort of people who the sooner you solve this problem, the more easily you'll satisfy. [SEP]\",\n",
       " \"[CLS] This problem, the sooner you solve the more easily you'll satisfy the folks up at corporate headquarters. [SEP]\",\n",
       " '[CLS] Who did you give pictures of to friends of? [SEP]',\n",
       " \"[CLS] It is this problem that the sooner you solve the more easily you'll satisfy the folks up at corporate headquarters. [SEP]\",\n",
       " \"[CLS] It is the folks up at corporate headquarters who the sooner you solve this problem, the more easily you'll satisfy. [SEP]\",\n",
       " '[CLS] Which problem the sooner you solve, will the more easily you satisfy the folks up at corporate headquarters? [SEP]',\n",
       " \"[CLS] Which problem does the sooner that you solve, the more easily you'll satisfy the folks up at corporate headquarters? [SEP]\",\n",
       " '[CLS] Which problem the sooner that you solve, will the more easily you satisfy the folks up at corporate headquarters? [SEP]',\n",
       " '[CLS] The harder it rains, the faster who runs? [SEP]',\n",
       " '[CLS] The louder who talks, the angrier you get? [SEP]',\n",
       " '[CLS] The harder that it rains, how much faster a flow do you see in the river? [SEP]',\n",
       " '[CLS] They failed to tell me which problem the sooner I solve, the quicker the folks up at corporate headquarters. [SEP]',\n",
       " \"[CLS] I finally worked up enough courage to ask which people up at corporate headquarters the sooner I solve this problem, the quicker I'll get free of. [SEP]\",\n",
       " \"[CLS] Which folks up at corporate headquarters do you think that the sooner you solve this problem, the quicker you'll be able to tell t to buzz off? [SEP]\",\n",
       " \"[CLS] This is a problem that you'll be able to tell the folks up at corporate headquarters to buzz off if you solve. [SEP]\",\n",
       " \"[CLS] This is a problem that you'll be able to tell the folks up at corporate headquarters to buzz off if you solve it. [SEP]\",\n",
       " \"[CLS] This is a problem that you solve it and you'll be able to tell the folks up at corporate headquarters to buzz off. [SEP]\",\n",
       " \"[CLS] Those are the folks that you just solve this problem and you'll be able to put them on ice. [SEP]\",\n",
       " \"[CLS] They failed to tell me which problem I'll beat the competition more easily, the sooner I solve. [SEP]\",\n",
       " \"[CLS] This is the problem that you'll beat the competition more easily, the sooner you solve. [SEP]\",\n",
       " '[CLS] John saw the man in the room. [SEP]',\n",
       " '[CLS] Which room did John see the man in? [SEP]',\n",
       " '[CLS] Who did John think that Bill claimed that Mary suspected that everybody liked? [SEP]',\n",
       " '[CLS] John could not visit Sally. [SEP]',\n",
       " '[CLS] What John could do is not visit Sally. [SEP]',\n",
       " \"[CLS] John couldn't visit Sally. [SEP]\",\n",
       " '[CLS] Why did John leave? [SEP]',\n",
       " '[CLS] I hit the ball. [SEP]',\n",
       " '[CLS] You hit the ball. [SEP]',\n",
       " '[CLS] He hit the ball. [SEP]',\n",
       " '[CLS] She hit the ball. [SEP]',\n",
       " '[CLS] They hit the ball. [SEP]',\n",
       " '[CLS] Am not I going? [SEP]',\n",
       " '[CLS] I am not going. [SEP]',\n",
       " \"[CLS] Aren't I going? [SEP]\",\n",
       " \"[CLS] I aren't going. [SEP]\",\n",
       " \"[CLS] Louise is unhappy, isn't she? [SEP]\",\n",
       " \"[CLS] Louise likes not being happy, doesn't she? [SEP]\",\n",
       " '[CLS] Not many books survived the fire, did they? [SEP]',\n",
       " '[CLS] No books survived the fire, did they? [SEP]',\n",
       " \"[CLS] He hasn't often paid taxes, has he? [SEP]\",\n",
       " \"[CLS] He can't pay taxes, can he? [SEP]\",\n",
       " '[CLS] She does not see him. [SEP]',\n",
       " '[CLS] She kept not seeing him. [SEP]',\n",
       " '[CLS] She could not have been working. [SEP]',\n",
       " '[CLS] Marianne not left. [SEP]',\n",
       " '[CLS] Marianne left not. [SEP]',\n",
       " '[CLS] He could not have been working. [SEP]',\n",
       " '[CLS] He cannot have been working. [SEP]',\n",
       " '[CLS] He can simply not have been working. [SEP]',\n",
       " '[CLS] You must not simply not work. [SEP]',\n",
       " '[CLS] He may not just not have been working. [SEP]',\n",
       " \"[CLS] He can't have been working. [SEP]\",\n",
       " \"[CLS] Can't he have been working? [SEP]\",\n",
       " '[CLS] Can he not have been working? [SEP]',\n",
       " '[CLS] Can he not have been working? [SEP]',\n",
       " '[CLS] John wrote books. [SEP]',\n",
       " '[CLS] John write books. [SEP]',\n",
       " '[CLS] John wrote books. [SEP]',\n",
       " '[CLS] John did not write books. [SEP]',\n",
       " '[CLS] John seems that is nice. [SEP]',\n",
       " \"[CLS] `` I am so happy '', thought John. [SEP]\",\n",
       " '[CLS] down the hill rolled John. [SEP]',\n",
       " '[CLS] John kisses often Mary. [SEP]',\n",
       " '[CLS] John often kisses Mary. [SEP]',\n",
       " '[CLS] Who do you think Mary said John likes? [SEP]',\n",
       " '[CLS] Who did you ask whether Mary knows why John likes? [SEP]',\n",
       " '[CLS] Who do you think that Mary said that John likes? [SEP]',\n",
       " '[CLS] How do you wonder whether Mary solved the problem? [SEP]',\n",
       " '[CLS] How do you think that Mary solved the problem? [SEP]',\n",
       " '[CLS] How do you wonder whether John said that Mary solved the problem? [SEP]',\n",
       " '[CLS] How do you wonder whether John said Mary solved the problem? [SEP]',\n",
       " '[CLS] Which problem do you wonder whether John said that Mary solved? [SEP]',\n",
       " '[CLS] How did you think that Mary solved the problem? [SEP]',\n",
       " '[CLS] Mary hired someone. [SEP]',\n",
       " '[CLS] I heard that Mary hired someone. [SEP]',\n",
       " '[CLS] I resigned because Mary hired someone. [SEP]',\n",
       " '[CLS] Mary wondered which picture of himself Bill saw? [SEP]',\n",
       " '[CLS] Which picture of himself does Mary think that John said that Susan likes? [SEP]',\n",
       " '[CLS] Mary thinks that John said that Susan likes pictures of himself? [SEP]',\n",
       " '[CLS] Mary thinks that John said that pictures of himself, Susan likes? [SEP]',\n",
       " \"[CLS] If you don't believe me, you will the weatherman? [SEP]\",\n",
       " '[CLS] I rolled up a newspaper, and Lynn did a magazine? [SEP]',\n",
       " \"[CLS] Kathy likes astronomy, but she doesn't meteorology? [SEP]\",\n",
       " '[CLS] The DA proved Jones guilty and the Assistant DA will prove Smith. [SEP]',\n",
       " '[CLS] Mary will believe Susan, and you will Bob. [SEP]',\n",
       " '[CLS] You might not believe me but you will Bob. [SEP]',\n",
       " '[CLS] You will Bob believe. [SEP]',\n",
       " '[CLS] How did you solve the problem? [SEP]',\n",
       " '[CLS] I wonder who could solve the problem in this way. [SEP]',\n",
       " '[CLS] How do you wonder who could solve this problem. [SEP]',\n",
       " '[CLS] No candidate can predict how many people will vote for him. [SEP]',\n",
       " '[CLS] Every politician is worried when the press starts attacking him. [SEP]',\n",
       " '[CLS] Which politician appointed the journalist who supported him? [SEP]',\n",
       " '[CLS] The fact that no candidate was elected shows that he was inadequate. [SEP]',\n",
       " '[CLS] John sells books, Mary buys records and Bill V newspapers. [SEP]',\n",
       " '[CLS] The question of whether John met Mary worries the people who support. [SEP]',\n",
       " '[CLS] They have left. [SEP]',\n",
       " '[CLS] Have they left? [SEP]',\n",
       " '[CLS] Could they have left? [SEP]',\n",
       " '[CLS] He has often seen Mary. [SEP]',\n",
       " '[CLS] He I often sees Mary. [SEP]',\n",
       " '[CLS] He sees often Mary. [SEP]',\n",
       " '[CLS] Sees he I often Mary? [SEP]',\n",
       " '[CLS] It seems that it is likely that John will win. [SEP]',\n",
       " '[CLS] It seems that John is likely to win. [SEP]',\n",
       " '[CLS] John seems to be likely to win. [SEP]',\n",
       " '[CLS] John seems that it is likely to win. [SEP]',\n",
       " '[CLS] John seems will win. [SEP]',\n",
       " '[CLS] How do you wonder which problem to solve? [SEP]',\n",
       " '[CLS] How intelligent do you consider John? [SEP]',\n",
       " '[CLS] How many people do you wonder whether I consider intelligent? [SEP]',\n",
       " '[CLS] How intelligent do you wonder whether I consider John? [SEP]',\n",
       " '[CLS] What the hell do you wonder how to say? [SEP]',\n",
       " '[CLS] He has left. [SEP]',\n",
       " '[CLS] His book is nice. [SEP]',\n",
       " '[CLS] Bill saw him. [SEP]',\n",
       " '[CLS] Bill works with him. [SEP]',\n",
       " '[CLS] John believes him to be a nice guy. [SEP]',\n",
       " '[CLS] John considers him a nice guy. [SEP]',\n",
       " '[CLS] For him to do that would be a mistake. [SEP]',\n",
       " '[CLS] With him sick, the team is in trouble. [SEP]',\n",
       " '[CLS] A man to be in the garden is unlikely. [SEP]',\n",
       " '[CLS] A man to come is unlikely. [SEP]',\n",
       " '[CLS] John to call would be unlikely. [SEP]',\n",
       " '[CLS] This conclusion to be arrived at is surprising. [SEP]',\n",
       " '[CLS] John believes that he is sick. [SEP]',\n",
       " '[CLS] John believes that him is sick. [SEP]',\n",
       " '[CLS] John tries him to win. [SEP]',\n",
       " '[CLS] John wonders where him to go. [SEP]',\n",
       " '[CLS] Who do you think that Bill likes? [SEP]',\n",
       " '[CLS] Who do you think that Bill believes to be innocent? [SEP]',\n",
       " '[CLS] Who do you think that believes John to be innocent? [SEP]',\n",
       " '[CLS] Who would you prefer for to win the race? [SEP]',\n",
       " '[CLS] Someone stole my car. [SEP]',\n",
       " '[CLS] My car was stolen. [SEP]',\n",
       " '[CLS] The children eat all chocolate. [SEP]',\n",
       " '[CLS] John has often kissed Mary. [SEP]',\n",
       " '[CLS] The kids have all eaten the chocolate. [SEP]',\n",
       " \"[CLS] In general, he understands what's going on. [SEP]\",\n",
       " \"[CLS] It's probable that in general he understands what's going on. [SEP]\",\n",
       " \"[CLS] It's probable in general that he understands what's going on. [SEP]\",\n",
       " \"[CLS] In general that he understands what's going on is surprising. [SEP]\",\n",
       " '[CLS] I explained how to fix the sink. [SEP]',\n",
       " '[CLS] I explained how we should fix the sink. [SEP]',\n",
       " '[CLS] I explained that we should fix the sink. [SEP]',\n",
       " '[CLS] I explained to fix the sink. [SEP]',\n",
       " '[CLS] Mickey looked up the reference. [SEP]',\n",
       " '[CLS] Mickey looked the reference up. [SEP]',\n",
       " '[CLS] Mickey looked up them. [SEP]',\n",
       " '[CLS] Mickey teamed up with the women. [SEP]',\n",
       " '[CLS] Mickey teamed with the women up. [SEP]',\n",
       " '[CLS] Mickey pointed out that Gary had left. [SEP]',\n",
       " '[CLS] Mickey pointed that Gary had left out. [SEP]',\n",
       " '[CLS] Mickey slips up all the time. [SEP]',\n",
       " '[CLS] Mickey slips all the time up. [SEP]',\n",
       " '[CLS] What does John think Mary bought? [SEP]',\n",
       " '[CLS] John thinks what Mary bought. [SEP]',\n",
       " '[CLS] John wonders what Mary bought. [SEP]',\n",
       " '[CLS] What does John wonder Mary bought? [SEP]',\n",
       " '[CLS] Who is he reading a book that criticizes? [SEP]',\n",
       " '[CLS] What do you remember where we bought? [SEP]',\n",
       " '[CLS] Who bought what? [SEP]',\n",
       " '[CLS] Who is reading a book that criticizes who? [SEP]',\n",
       " '[CLS] Who remembers where we bought what? [SEP]',\n",
       " '[CLS] I wonder who what bought? [SEP]',\n",
       " '[CLS] I wonder what who bought? [SEP]',\n",
       " \"[CLS] There aren't many linguistics students here. [SEP]\",\n",
       " \"[CLS] I haven't met many linguistics students. [SEP]\",\n",
       " '[CLS] What does every student buy? [SEP]',\n",
       " '[CLS] I need Sally to be there. [SEP]',\n",
       " '[CLS] The boat sank to collect the insurance. [SEP]',\n",
       " '[CLS] The boat was sunk to collect the insurance. [SEP]',\n",
       " '[CLS] John wants to win. [SEP]',\n",
       " '[CLS] The bed was unmade. [SEP]',\n",
       " '[CLS] Headway was unmade. [SEP]',\n",
       " '[CLS] John was unknown. [SEP]',\n",
       " '[CLS] John was unknown to be the murderer. [SEP]',\n",
       " '[CLS] We knew John to be the murderer. [SEP]',\n",
       " '[CLS] He fed the children. [SEP]',\n",
       " '[CLS] The children were uneducated. [SEP]',\n",
       " '[CLS] The children were undisciplined. [SEP]',\n",
       " '[CLS] I believed these students all to like John. [SEP]',\n",
       " '[CLS] They tried to all like John. [SEP]',\n",
       " '[CLS] I believed these students to all like John. [SEP]',\n",
       " '[CLS] Did he try ever to talk to the student? [SEP]',\n",
       " '[CLS] Did you believe him ever to have made an effort to talk to the student? [SEP]',\n",
       " '[CLS] Did he try to ever be attentive to the needs of students? [SEP]',\n",
       " '[CLS] Did you believe him to ever have made an effort to talk to the student? [SEP]',\n",
       " '[CLS] work out an analysis that is typical of this view of understood subjects. [SEP]',\n",
       " '[CLS] They were believed all to be quite diligent. [SEP]',\n",
       " '[CLS] Was he believed ever to fail students? [SEP]',\n",
       " '[CLS] There is tending to be more and more discussion of these issues. [SEP]',\n",
       " '[CLS] John seemed to be a great linguist. [SEP]',\n",
       " '[CLS] There promises to be a storm tonight. [SEP]',\n",
       " '[CLS] John strived to be successful. [SEP]',\n",
       " '[CLS] John wanted to improve his lot in life. [SEP]',\n",
       " '[CLS] John expected to win. [SEP]',\n",
       " '[CLS] This book is too dense to be read in one sitting. [SEP]',\n",
       " '[CLS] There is too likely to be a riot to be a serious discussion of the issues. [SEP]',\n",
       " '[CLS] John tried. [SEP]',\n",
       " '[CLS] John remembered. [SEP]',\n",
       " '[CLS] John is refused. [SEP]',\n",
       " '[CLS] John forgot. [SEP]',\n",
       " \"[CLS] Bill seems to be obnoxious, but I don't think that Sam happens. [SEP]\",\n",
       " \"[CLS] Bill seems to be obnoxious, but I don't think that Sam turns out. [SEP]\",\n",
       " \"[CLS] Bill seems to be obnoxious, but I don't think that Sam tends. [SEP]\",\n",
       " '[CLS] They tried all to like John. [SEP]',\n",
       " '[CLS] They seemed all to like John. [SEP]',\n",
       " '[CLS] John believes Sally to be polite. [SEP]',\n",
       " '[CLS] I believe John with all my heart to be a fine person. [SEP]',\n",
       " '[CLS] John is wanted to win. [SEP]',\n",
       " '[CLS] John would be liked to win. [SEP]',\n",
       " '[CLS] We would like John to win. [SEP]',\n",
       " '[CLS] John would be hated to win. [SEP]',\n",
       " '[CLS] John would be preferred to be the candidate. [SEP]',\n",
       " '[CLS] We would prefer John to be the candidate. [SEP]',\n",
       " '[CLS] I would like for John to win. [SEP]',\n",
       " '[CLS] I would hate for John to win. [SEP]',\n",
       " '[CLS] I would prefer for John to be the candidate. [SEP]',\n",
       " '[CLS] John destroyed the house. [SEP]',\n",
       " '[CLS] The electrode emitted ions into the medium. [SEP]',\n",
       " '[CLS] Ions struck the electrode. [SEP]',\n",
       " '[CLS] The medium contains ions. [SEP]',\n",
       " '[CLS] The house destroyed John. [SEP]',\n",
       " '[CLS] Ions left the electrode. [SEP]',\n",
       " '[CLS] The electrode was left by ions. [SEP]',\n",
       " '[CLS] The electrode was struck by ions. [SEP]',\n",
       " '[CLS] The ball lies in the box. [SEP]',\n",
       " '[CLS] The ball rolled from the bush to the tree. [SEP]',\n",
       " '[CLS] The box contains the ball. [SEP]',\n",
       " '[CLS] The tree dropped fruit to the ground. [SEP]',\n",
       " '[CLS] Fruit hit the ground from the tree. [SEP]',\n",
       " '[CLS] The stone knocked against the pole into the road. [SEP]',\n",
       " '[CLS] The stone knocked the pole into the road. [SEP]',\n",
       " '[CLS] The box contained the ball. [SEP]',\n",
       " '[CLS] The box gradually contained the ball. [SEP]',\n",
       " '[CLS] The box at once contained the ball. [SEP]',\n",
       " '[CLS] The box contained the ball to the ground. [SEP]',\n",
       " '[CLS] The tree gradually dropped its fruit to the ground. [SEP]',\n",
       " '[CLS] The tree dropped its fruit to the ground. [SEP]',\n",
       " '[CLS] Fruit hit the roof. [SEP]',\n",
       " '[CLS] Fruit hit the roof from the tree. [SEP]',\n",
       " '[CLS] Fruit at once hit the roof from the tree. [SEP]',\n",
       " '[CLS] Fruit hit the roof against the ground. [SEP]',\n",
       " '[CLS] Fruit at once hit the roof against the ground. [SEP]',\n",
       " '[CLS] Fruit dropped from the tree. [SEP]',\n",
       " '[CLS] Fruit dropped from the tree from the clouds. [SEP]',\n",
       " '[CLS] Fruit fell against the house. [SEP]',\n",
       " '[CLS] Fruit fell against the house against the ground. [SEP]',\n",
       " '[CLS] The tree changed into an oak. [SEP]',\n",
       " '[CLS] The tree changed from a maple into an oak. [SEP]',\n",
       " '[CLS] The maple changed into an oak from a cedar. [SEP]',\n",
       " '[CLS] The maple changed into an oak from a cedar. [SEP]',\n",
       " '[CLS] The maple changed into an oak. [SEP]',\n",
       " '[CLS] The oak developed out of a maple. [SEP]',\n",
       " '[CLS] The train reached the station. [SEP]',\n",
       " '[CLS] The branches knocked against the wall. [SEP]',\n",
       " '[CLS] The child became a man. [SEP]',\n",
       " '[CLS] The party lasted till midnight. [SEP]',\n",
       " '[CLS] The dog went crazy. [SEP]',\n",
       " '[CLS] It struck John that it was so. [SEP]',\n",
       " '[CLS] It came to John that it was so. [SEP]',\n",
       " '[CLS] The snake saw into the nest. [SEP]',\n",
       " '[CLS] Hard work resulted in high grades. [SEP]',\n",
       " '[CLS] The farm passed to John. [SEP]',\n",
       " '[CLS] John is touching the wall. [SEP]',\n",
       " '[CLS] The wall is being touched by John. [SEP]',\n",
       " '[CLS] A bear occupies the cave. [SEP]',\n",
       " '[CLS] A bear inhabits the cave. [SEP]',\n",
       " '[CLS] Water fills the tub. [SEP]',\n",
       " '[CLS] The electric main joins the house circuit in the basement. [SEP]',\n",
       " '[CLS] The house circuit is joined by the electric main in the basement. [SEP]',\n",
       " '[CLS] The fence straddles the sidewalk. [SEP]',\n",
       " '[CLS] The sidewalk is straddled by the fence. [SEP]',\n",
       " '[CLS] The man with a book. [SEP]',\n",
       " '[CLS] Gas escaped the tube. [SEP]',\n",
       " '[CLS] The terrorist escaped the prison cell. [SEP]',\n",
       " '[CLS] The prison cell was escaped by the terrorist. [SEP]',\n",
       " '[CLS] The rolling stone avoided the river. [SEP]',\n",
       " '[CLS] The river was avoided by the rolling stone. [SEP]',\n",
       " '[CLS] The agents caught the terrorist. [SEP]',\n",
       " '[CLS] The sponge soaked up the water. [SEP]',\n",
       " '[CLS] The tub filled with water. [SEP]',\n",
       " '[CLS] John received a book. [SEP]',\n",
       " '[CLS] John learned a lesson. [SEP]',\n",
       " '[CLS] The parcel reached John. [SEP]',\n",
       " '[CLS] John received the parcel. [SEP]',\n",
       " '[CLS] The farm finally got to John after much litigation. [SEP]',\n",
       " '[CLS] The farm finally reached John after much litigation. [SEP]',\n",
       " '[CLS] Water filled the cup high. [SEP]',\n",
       " '[CLS] Water filled the cup. [SEP]',\n",
       " '[CLS] Water emptied the cup. [SEP]',\n",
       " '[CLS] The cup filled the water high. [SEP]',\n",
       " '[CLS] The cup filled of water. [SEP]',\n",
       " '[CLS] The cup filled with water. [SEP]',\n",
       " '[CLS] The cup emptied with water. [SEP]',\n",
       " '[CLS] the barge piled high with logs. [SEP]',\n",
       " '[CLS] the road blocked with a stone. [SEP]',\n",
       " '[CLS] the branch dropped bare of its apple. [SEP]',\n",
       " '[CLS] the logs piled the barge high. [SEP]',\n",
       " '[CLS] a stone blocked the road. [SEP]',\n",
       " '[CLS] the bottle drained the liquid free. [SEP]',\n",
       " '[CLS] the branch dropped its apple free. [SEP]',\n",
       " '[CLS] Some branches broke off of the tree. [SEP]',\n",
       " '[CLS] The tree broke off some branches. [SEP]',\n",
       " '[CLS] The tree dropped some branches. [SEP]',\n",
       " '[CLS] The tree lost some branches. [SEP]',\n",
       " '[CLS] Water bubbled out of the kettle. [SEP]',\n",
       " '[CLS] The kettle bubbled water up. [SEP]',\n",
       " '[CLS] The kettle bubbled water. [SEP]',\n",
       " '[CLS] The cup filled water. [SEP]',\n",
       " '[CLS] The stone knocked the pole into the road. [SEP]',\n",
       " '[CLS] The tub leaked empty of water. [SEP]',\n",
       " '[CLS] The stone knocked against the pole into the road. [SEP]',\n",
       " '[CLS] Hail stones broke the window. [SEP]',\n",
       " '[CLS] The force of the wind broke the window. [SEP]',\n",
       " '[CLS] The window broke from hail stones. [SEP]',\n",
       " '[CLS] The window broke from the force of the wind. [SEP]',\n",
       " '[CLS] What the force of the wind did to the window was break it. [SEP]',\n",
       " '[CLS] John hit the stone against the wall. [SEP]',\n",
       " '[CLS] John hit the wall with the stone. [SEP]',\n",
       " '[CLS] John tapped some wine from a barrel. [SEP]',\n",
       " '[CLS] John tapped a barrel of some wine. [SEP]',\n",
       " '[CLS] John laid the book on the table. [SEP]',\n",
       " '[CLS] John included his name in the list. [SEP]',\n",
       " '[CLS] John loaded the bricks onto the truck. [SEP]',\n",
       " '[CLS] John loaded the truck with bricks. [SEP]',\n",
       " '[CLS] John fed rice to the baby. [SEP]',\n",
       " '[CLS] John fed the baby rice. [SEP]',\n",
       " '[CLS] John fed the baby up with rice. [SEP]',\n",
       " '[CLS] John fed the baby rice up. [SEP]',\n",
       " '[CLS] The ball lies completely in the box. [SEP]',\n",
       " '[CLS] The box completely contains the ball. [SEP]',\n",
       " '[CLS] The train got to the station fully. [SEP]',\n",
       " '[CLS] The train reached the station fully. [SEP]',\n",
       " '[CLS] Press the stamp against the pad completely. [SEP]',\n",
       " '[CLS] Press the pad with the stamp completely. [SEP]',\n",
       " '[CLS] Spray the paint onto the wall completely. [SEP]',\n",
       " '[CLS] Spray all the paint onto the wall completely. [SEP]',\n",
       " '[CLS] Spray the wall with all the paint. [SEP]',\n",
       " '[CLS] Spray the whole wall with the paint. [SEP]',\n",
       " '[CLS] What John did to the wall was paint it. [SEP]',\n",
       " '[CLS] What John did to the whole wall was paint it. [SEP]',\n",
       " '[CLS] What John did to the wall was hit it. [SEP]',\n",
       " '[CLS] What the stone did to the wall was hit it. [SEP]',\n",
       " '[CLS] What the stone did to the whole wall was hit it. [SEP]',\n",
       " '[CLS] John took Bill to be a fool. [SEP]',\n",
       " '[CLS] John concluded Bill to be a fool. [SEP]',\n",
       " '[CLS] Give the bottle to the baby full. [SEP]',\n",
       " '[CLS] Give the bottle to the baby awake. [SEP]',\n",
       " '[CLS] Give the baby the bottle full. [SEP]',\n",
       " '[CLS] Give the baby the bottle awake. [SEP]',\n",
       " '[CLS] Rub the cloth on the baby torn. [SEP]',\n",
       " '[CLS] Rub the cloth on the baby asleep. [SEP]',\n",
       " '[CLS] Rub the baby with the cloth torn. [SEP]',\n",
       " '[CLS] Rub the baby with the cloth asleep. [SEP]',\n",
       " '[CLS] Dry the baby with the cloth asleep. [SEP]',\n",
       " '[CLS] Dry the baby with the cloth torn. [SEP]',\n",
       " '[CLS] The cup knocked the stone apart. [SEP]',\n",
       " '[CLS] The stone knocked the cup apart. [SEP]',\n",
       " '[CLS] The cup smashed apart against the stone. [SEP]',\n",
       " '[CLS] The stone smashed the cup apart. [SEP]',\n",
       " '[CLS] The tank filled with petrol out of the pump. [SEP]',\n",
       " '[CLS] The cup emptied of water onto the ground. [SEP]',\n",
       " '[CLS] John included her name in the list. [SEP]',\n",
       " '[CLS] John rolled the ball from the tree to the bush. [SEP]',\n",
       " '[CLS] John tapped the bottle of some water. [SEP]',\n",
       " '[CLS] John gave Bill the book. [SEP]',\n",
       " '[CLS] John got the book from Bill. [SEP]',\n",
       " '[CLS] John gave Bill of the book. [SEP]',\n",
       " '[CLS] We have someone in the living room. [SEP]',\n",
       " '[CLS] John is very fond of Mary. [SEP]',\n",
       " '[CLS] Mary laughed at John. [SEP]',\n",
       " '[CLS] The ship sank beneath the waves. [SEP]',\n",
       " '[CLS] Mary considers John a fool and Bill a wimp. [SEP]',\n",
       " '[CLS] John regards professors as strange and politicians as creepy. [SEP]',\n",
       " '[CLS] Sue put the books on the table and the records on the chair. [SEP]',\n",
       " '[CLS] Harriet gave a mug to John and a scarf to Vivien. [SEP]',\n",
       " '[CLS] I expect John to win and Harry to lose. [SEP]',\n",
       " '[CLS] You eat the fish raw and the beef cooked. [SEP]',\n",
       " '[CLS] They told Sue who to talk to and Virginia when to leave. [SEP]',\n",
       " '[CLS] Smith loaned, and his widow later donated, a valuable collection of manuscripts to the library. [SEP]',\n",
       " '[CLS] Sue moved, and Mary also transferred, her business to a different location. [SEP]',\n",
       " '[CLS] I succeeded in convincing, even though John had failed to persuade, Mary not to leave. [SEP]',\n",
       " \"[CLS] We didn't particularly like, but nevertheless ate, the fish raw. [SEP]\",\n",
       " \"[CLS] Flo desperately wants, though she doesn't really expect, the Miami Dolphins to be in the play-offs. [SEP]\",\n",
       " '[CLS] John learned French perfectly. [SEP]',\n",
       " '[CLS] Bill recited his lines poorly. [SEP]',\n",
       " '[CLS] Mary plays the violin beautifully. [SEP]',\n",
       " '[CLS] John perfectly learned French. [SEP]',\n",
       " '[CLS] Bill poorly recited his lines. [SEP]',\n",
       " '[CLS] John learned French immediately. [SEP]',\n",
       " '[CLS] Bill recited his lines slowly. [SEP]',\n",
       " '[CLS] Mary will play the violin soon. [SEP]',\n",
       " '[CLS] John immediately learned French. [SEP]',\n",
       " '[CLS] Bill slowly recited his lines. [SEP]',\n",
       " '[CLS] Mary will soon play the violin. [SEP]',\n",
       " '[CLS] John immediately learned French perfectly. [SEP]',\n",
       " '[CLS] John learned French perfectly almost immediately. [SEP]',\n",
       " '[CLS] John learned French perfectly immediately. [SEP]',\n",
       " '[CLS] John perfectly learned French immediately. [SEP]',\n",
       " '[CLS] John learned French immediately perfectly. [SEP]',\n",
       " '[CLS] Clearly, John immediately will probably learn French perfectly. [SEP]',\n",
       " '[CLS] Immediately, John probably will clearly learn French perfectly. [SEP]',\n",
       " '[CLS] Clearly, John perfectly will immediately learn French probably. [SEP]',\n",
       " '[CLS] John perfectly rolled the ball down the hill. [SEP]',\n",
       " '[CLS] John rolled the ball perfectly down the hill. [SEP]',\n",
       " '[CLS] John rolled the ball down the hill perfectly. [SEP]',\n",
       " '[CLS] John perfectly shot the ball. [SEP]',\n",
       " '[CLS] John shot the ball perfectly. [SEP]',\n",
       " '[CLS] John intimately spoke to Mary. [SEP]',\n",
       " '[CLS] John spoke intimately to Mary. [SEP]',\n",
       " '[CLS] John spoke to Mary intimately. [SEP]',\n",
       " '[CLS] John spoke French intimately to Mary. [SEP]',\n",
       " '[CLS] John spoke French to Mary intimately. [SEP]',\n",
       " '[CLS] Mary jumped the horse perfectly over the last fence. [SEP]',\n",
       " '[CLS] Mary jumped the horse over the last fence perfectly. [SEP]',\n",
       " '[CLS] John spoke intimately French to Mary. [SEP]',\n",
       " '[CLS] John spoke to Mary French. [SEP]',\n",
       " '[CLS] Mary persuaded to leave John. [SEP]',\n",
       " '[CLS] The lions ate raw the meat. [SEP]',\n",
       " '[CLS] Mary persuaded that he should rest Bill. [SEP]',\n",
       " '[CLS] We consider the men all fools. [SEP]',\n",
       " '[CLS] We consider the men all totally crazy. [SEP]',\n",
       " '[CLS] I saw the men all. [SEP]',\n",
       " '[CLS] The men were arrested all. [SEP]',\n",
       " '[CLS] The men arrived all. [SEP]',\n",
       " '[CLS] The teacher ordered the two boys both to pay close attention. [SEP]',\n",
       " '[CLS] They returned the books all to their owners. [SEP]',\n",
       " '[CLS] We painted the chairs all red. [SEP]',\n",
       " '[CLS] The trainer fed the steaks all to the lions. [SEP]',\n",
       " \"[CLS] Bill proud of himself John doesn't consider. [SEP]\",\n",
       " '[CLS] Home was gone by John. [SEP]',\n",
       " '[CLS] Mary left the room angry. [SEP]',\n",
       " '[CLS] The room was left angry by Mary. [SEP]',\n",
       " '[CLS] The room was left angry. [SEP]',\n",
       " '[CLS] John resembles Bill. [SEP]',\n",
       " '[CLS] Bill is resembled by John. [SEP]',\n",
       " '[CLS] The package weighed 10 lb. [SEP]',\n",
       " '[CLS] 10 lb was weighed by the package. [SEP]',\n",
       " '[CLS] This book cost $10. [SEP]',\n",
       " '[CLS] $10 was cost by this book. [SEP]',\n",
       " '[CLS] The book cost John $10. [SEP]',\n",
       " '[CLS] John was cost $10 by the book. [SEP]',\n",
       " '[CLS] John is impressed by Bill as pompous. [SEP]',\n",
       " '[CLS] The boys were made a good mother. [SEP]',\n",
       " '[CLS] The boys were made a good mother by Aunt Mary. [SEP]',\n",
       " '[CLS] The kids were failed by Max as a father. [SEP]',\n",
       " '[CLS] The kids were failed as a father. [SEP]',\n",
       " '[CLS] The men were struck by the idea as nonsense. [SEP]',\n",
       " '[CLS] The men were promised to leave. [SEP]',\n",
       " '[CLS] He impresses his friends all as pompous. [SEP]',\n",
       " '[CLS] Aunt Mary made the boys all a good mother. [SEP]',\n",
       " '[CLS] Max failed the kids all as a father. [SEP]',\n",
       " '[CLS] Frank promised the men all to leave. [SEP]',\n",
       " '[CLS] We proclaimed to the public John to be a hero. [SEP]',\n",
       " '[CLS] We proclaimed John to the public to be a hero. [SEP]',\n",
       " '[CLS] We proclaimed sincerely John to be a hero. [SEP]',\n",
       " '[CLS] We proclaimed John sincerely to be a hero. [SEP]',\n",
       " '[CLS] We proclaimed sincerely to the public John to be a hero. [SEP]',\n",
       " '[CLS] We proclaimed John sincerely to the public to be a hero. [SEP]',\n",
       " '[CLS] They represented to the dean Mary as a genuine linguist. [SEP]',\n",
       " '[CLS] They represented Mary to the dean as a genuine linguist. [SEP]',\n",
       " '[CLS] They represented seriously Mary as a genuine linguist. [SEP]',\n",
       " '[CLS] They represented Mary seriously as a genuine linguist. [SEP]',\n",
       " '[CLS] They represented Mary seriously to the dean as a genuine linguist. [SEP]',\n",
       " '[CLS] We proved to the authorities Smith to be the thief. [SEP]',\n",
       " '[CLS] We proved conclusively Smith to be the thief. [SEP]',\n",
       " '[CLS] We proved Smith conclusively to be the thief. [SEP]',\n",
       " '[CLS] We proved conclusively to the authorities Smith to be the thief. [SEP]',\n",
       " '[CLS] We proved Smith conclusively to the authorities to be the thief. [SEP]',\n",
       " '[CLS] The gardener watered the tulips flat. [SEP]',\n",
       " '[CLS] The grocer ground the coffee beans to a fine powder. [SEP]',\n",
       " '[CLS] They painted their house a hideous shade of green. [SEP]',\n",
       " '[CLS] The joggers ran their Nikes threadbare. [SEP]',\n",
       " '[CLS] The kids laughed themselves into a frenzy. [SEP]',\n",
       " '[CLS] He coughed his handkerchief completely soggy. [SEP]',\n",
       " '[CLS] They fed the meat to the lions raw. [SEP]',\n",
       " '[CLS] The lions ate at the meat raw. [SEP]',\n",
       " '[CLS] We love them. [SEP]',\n",
       " '[CLS] We love they. [SEP]',\n",
       " '[CLS] We love their. [SEP]',\n",
       " '[CLS] Us love their. [SEP]',\n",
       " '[CLS] Our love they. [SEP]',\n",
       " '[CLS] Our love them. [SEP]',\n",
       " '[CLS] Our love their. [SEP]',\n",
       " '[CLS] he belief that Mary kissed Bill is mistaken. [SEP]',\n",
       " '[CLS] him belief that Mary kissed Bill is mistaken. [SEP]',\n",
       " '[CLS] his belief that Mary kissed Bill is mistaken. [SEP]',\n",
       " '[CLS] Mary loves him. [SEP]',\n",
       " '[CLS] Mary is fond of him. [SEP]',\n",
       " '[CLS] Mary is fond him. [SEP]',\n",
       " '[CLS] Mary criticized him. [SEP]',\n",
       " \"[CLS] Mary's criticism him was cruel. [SEP]\",\n",
       " \"[CLS] Mary's criticism of him was cruel. [SEP]\",\n",
       " '[CLS] That John loves Mary is doubtful. [SEP]',\n",
       " '[CLS] John to love Mary would be doubtful. [SEP]',\n",
       " '[CLS] For John to love Mary would be doubtful. [SEP]',\n",
       " '[CLS] To go abroad would be nice. [SEP]',\n",
       " \"[CLS] John's plan to go abroad is nice. [SEP]\",\n",
       " '[CLS] Mary believed John to have loved her. [SEP]',\n",
       " '[CLS] Mary considered John to have loved her. [SEP]',\n",
       " '[CLS] Mary reported John to have loved her. [SEP]',\n",
       " '[CLS] Mary considered to have loved her. [SEP]',\n",
       " '[CLS] Mary tried to go abroad. [SEP]',\n",
       " '[CLS] Mary intended to go abroad. [SEP]',\n",
       " '[CLS] Mary managed to go abroad. [SEP]',\n",
       " '[CLS] Mary desired to go abroad. [SEP]',\n",
       " '[CLS] Mary tried John to go abroad. [SEP]',\n",
       " '[CLS] Mary managed John to go abroad. [SEP]',\n",
       " '[CLS] Mary desired John to go abroad. [SEP]',\n",
       " '[CLS] Mary believed him to have loved her. [SEP]',\n",
       " '[CLS] Mary considered him to have loved her. [SEP]',\n",
       " '[CLS] Mary believed he to have loved her. [SEP]',\n",
       " '[CLS] Mary considered he to have loved her. [SEP]',\n",
       " '[CLS] Mary reported he to have loved her. [SEP]',\n",
       " '[CLS] Mary believed his to have loved her. [SEP]',\n",
       " '[CLS] Mary considered his to have loved her. [SEP]',\n",
       " '[CLS] Mary reported his to have loved her. [SEP]',\n",
       " '[CLS] It is certain that John has loved Mary. [SEP]',\n",
       " '[CLS] It is likely that John has loved Mary. [SEP]',\n",
       " '[CLS] There are strangers in that garden. [SEP]',\n",
       " '[CLS] There is strangers in that garden. [SEP]',\n",
       " '[CLS] There is arriving three men at that station. [SEP]',\n",
       " '[CLS] There are arriving three men at that station. [SEP]',\n",
       " '[CLS] I consider there to be a man in that garden. [SEP]',\n",
       " '[CLS] I consider there a man in that garden. [SEP]',\n",
       " '[CLS] They alleged there to have been many strangers in that garden. [SEP]',\n",
       " '[CLS] They alleged many strangers to have been in that garden. [SEP]',\n",
       " '[CLS] John wagered there to have been a stranger in that haunted house. [SEP]',\n",
       " '[CLS] John wagered a stranger to have been in that haunted house. [SEP]',\n",
       " '[CLS] John tried to kiss Mary. [SEP]',\n",
       " '[CLS] John persuaded Mary to kiss him. [SEP]',\n",
       " '[CLS] John told Mary to kiss him. [SEP]',\n",
       " '[CLS] It is illegal to park here. [SEP]',\n",
       " '[CLS] I remembered him having kissed Mary. [SEP]',\n",
       " '[CLS] I reported him having kissed Mary. [SEP]',\n",
       " '[CLS] I reported having kissed Mary. [SEP]',\n",
       " '[CLS] I enjoy taking a bath. [SEP]',\n",
       " '[CLS] I detest taking a bath. [SEP]',\n",
       " '[CLS] I enjoy him taking a bath. [SEP]',\n",
       " '[CLS] I detest him taking a bath. [SEP]',\n",
       " '[CLS] I saw him kissing Mary. [SEP]',\n",
       " '[CLS] I noticed him kissing Mary. [SEP]',\n",
       " '[CLS] I noticed kissing Mary. [SEP]',\n",
       " '[CLS] There was known to everyone. [SEP]',\n",
       " \"[CLS] John's refusing the offer is shocking. [SEP]\",\n",
       " \"[CLS] the enemy's destroying the city was horrific. [SEP]\",\n",
       " \"[CLS] John's refusal of the offer was shocking. [SEP]\",\n",
       " \"[CLS] the enemy's destruction of the city was horrific. [SEP]\",\n",
       " '[CLS] John wanted to leave the room happy and leave the room he did happy. [SEP]',\n",
       " '[CLS] I often send Mary home drunk, and she gets there just fine. [SEP]',\n",
       " '[CLS] I raw eat fish drunk. [SEP]',\n",
       " '[CLS] I only eat fish drunk raw. [SEP]',\n",
       " \"[CLS] I don't think Fred will, either. [SEP]\",\n",
       " '[CLS] JosÃ© likes cabbage, and Holly does too. [SEP]',\n",
       " '[CLS] JosÃ© ate cabbage, and Holly has too. [SEP]',\n",
       " '[CLS] JosÃ© is eating cabbage, and Holly is too. [SEP]',\n",
       " \"[CLS] John is leaving but Mary's not. [SEP]\",\n",
       " '[CLS] I consider Bill intelligent and I consider Sally not. [SEP]',\n",
       " '[CLS] Sally started running down the street, but only after JosÃ© started. [SEP]',\n",
       " '[CLS] Sally made Bill laugh, and then JosÃ© made. [SEP]',\n",
       " \"[CLS] Mary came to read Fred's story, and I also came to. [SEP]\",\n",
       " \"[CLS] John wants to go on vacation, but he doesn't know when to. [SEP]\",\n",
       " '[CLS] Mary was told to bring something to the party, so she asked Sue what to. [SEP]',\n",
       " '[CLS] We might go on vacation if we can ever figure out when to. [SEP]',\n",
       " \"[CLS] Ron wanted to wear a tuxedo to the party, but Caspar couldn't decide whether to. [SEP]\",\n",
       " \"[CLS] You shouldn't play with rifles because to is dangerous. [SEP]\",\n",
       " '[CLS] John is being discussed and Sally is being too. [SEP]',\n",
       " '[CLS] I remember John being discussed, but you recall Sally being. [SEP]',\n",
       " \"[CLS] Sally might have eaten cabbage, but Holly shouldn't. [SEP]\",\n",
       " '[CLS] JosÃ© asks that we go to the meeting, and Sally will tell us when. [SEP]',\n",
       " \"[CLS] It's we go to the meeting, that Sally will tell us when. [SEP]\",\n",
       " \"[CLS] It's to Mary that Joe said Holly can talk. [SEP]\",\n",
       " \"[CLS] Mary claimed that eaten cabbage, Holly hasn't. [SEP]\",\n",
       " \"[CLS] Mary claimed that eating cabbage, Holly's not. [SEP]\",\n",
       " '[CLS] Mary claimed that eat cabbage, Holly wants to. [SEP]',\n",
       " '[CLS] Mary claimed that would eat cabbage, Holly. [SEP]',\n",
       " \"[CLS] Mary claimed that hasn't eaten cabbage, Holly. [SEP]\",\n",
       " '[CLS] Mary claimed that eating cabbage, Holly started. [SEP]',\n",
       " '[CLS] Mary claimed that eat cabbage, Holly made me. [SEP]',\n",
       " '[CLS] Mary claimed that have eaten cabbage, Holly should. [SEP]',\n",
       " '[CLS] Mary claimed that intelligent, I consider Holly not. [SEP]',\n",
       " '[CLS] Lilly recounted a story to remember because Holly had also recounted a story to. [SEP]',\n",
       " \"[CLS] I reviewed Joe's attempt to find Holly while you reviewed JosÃ©'s attempt to. [SEP]\",\n",
       " \"[CLS] Mary questioned Joe's desire to eat cabbage, but only after I had questioned Sally's desire to. [SEP]\",\n",
       " '[CLS] Sally explained the attempt to arrest Holly, but only after I had denied the decision to. [SEP]',\n",
       " \"[CLS] John didn't hit a home run, but I know a woman who did. [SEP]\",\n",
       " \"[CLS] That Betsy won the batting crown is not surprising, but that Peter didn't know she did is surprising. [SEP]\",\n",
       " \"[CLS] You shouldn't have played with rifles because to have is dangerous. [SEP]\",\n",
       " \"[CLS] Ron wanted to be wearing a tuxedo to the party, but Caspar didn't know whether to be. [SEP]\",\n",
       " '[CLS] Lilly recounted a story to be remembered because Holly had recounted a story to be. [SEP]',\n",
       " '[CLS] Lilly decided that eating cabbage, she should be. [SEP]',\n",
       " '[CLS] Lilly decided eating cabbage, to be. [SEP]',\n",
       " \"[CLS] Read Fred's story, I also want to. [SEP]\",\n",
       " \"[CLS] You shouldn't play with rifles because play with rifles to is dangerous. [SEP]\",\n",
       " \"[CLS] Ron wanted to wear a tuxedo to the party, but wear a tuxedo to the party Caspar couldn't decide whether to. [SEP]\",\n",
       " '[CLS] Lucy Barnes recounted a story to remember because remember Holly had recounted a story to. [SEP]',\n",
       " '[CLS] Mag Wildwood came to introduce the bartender but I came not to. [SEP]',\n",
       " '[CLS] Mag Wildwood came to introduce the bartender but I came precisely not to. [SEP]',\n",
       " '[CLS] You should unload rifles because not to s is dangerous. [SEP]',\n",
       " '[CLS] If Ron knows whether to wear a tuxedo, and Caspar knows whether not to, do they know different things? [SEP]',\n",
       " '[CLS] Lucy recounted a story to remember because Holly had recounted as story not to. [SEP]',\n",
       " '[CLS] I will, if I can work on it. [SEP]',\n",
       " '[CLS] Did Harry leave? [SEP]',\n",
       " '[CLS] Does Joe sing? [SEP]',\n",
       " '[CLS] A proof that God exist does. [SEP]',\n",
       " '[CLS] A proof that God does exists. [SEP]',\n",
       " '[CLS] I visited every town in every country I had to. [SEP]',\n",
       " '[CLS] Every man who said he would buy some salmon did. [SEP]',\n",
       " '[CLS] I visited every town I had to. [SEP]',\n",
       " '[CLS] Every town in every country I had to I visited. [SEP]',\n",
       " '[CLS] Every man who said he would buy some salmon did buy some salmon. [SEP]',\n",
       " '[CLS] Lilly should buy salmon and Mary should too. [SEP]',\n",
       " '[CLS] Lilly should buy salmon and Mary should buy salmon too. [SEP]',\n",
       " \"[CLS] Joe's neuroses bother his patrons, and Sally's neuroses do too. [SEP]\",\n",
       " \"[CLS] Joe likes his bar, and Sally's patrons do too. [SEP]\",\n",
       " '[CLS] Every picture of itself arrived. [SEP]',\n",
       " \"[CLS] My uncle doesn't have a spouse but your aunt does and he is lying on the floor. [SEP]\",\n",
       " \"[CLS] My uncle didn't buy anything for Christmas, but my aunt did it for him and it was bright red. [SEP]\",\n",
       " \"[CLS] I know which book Max read, and which book Oscar didn't. [SEP]\",\n",
       " \"[CLS] This is the book of which Bill approves, and this is the one of which he doesn't. [SEP]\",\n",
       " \"[CLS] I know which book Mag read, and which book Bob asked why you hadn't. [SEP]\",\n",
       " '[CLS] I know which book Mag read, and which book Bob discussed after I had. [SEP]',\n",
       " '[CLS] Dulles suspected everyone who Angleton did. [SEP]',\n",
       " \"[CLS] While Bob read Fred, he didn't Dickens. [SEP]\",\n",
       " \"[CLS] Sally suspected Joe, but he didn't Holly. [SEP]\",\n",
       " \"[CLS] Although Mag doesn't eggplants, Sally eats cabbage. [SEP]\",\n",
       " \"[CLS] Although I don't know which book Sam did, I do know which book Sally read. [SEP]\",\n",
       " '[CLS] Near everyone Angleton did, Dulles stood. [SEP]',\n",
       " \"[CLS] Sally will stand near Mag, but he won't Holly. [SEP]\",\n",
       " \"[CLS] While Holly didn't discuss a report about every boy, she did every girl. [SEP]\",\n",
       " '[CLS] Sally will stand near every woman that you will. [SEP]',\n",
       " \"[CLS] I know which woman Holly will discuss a report about, but I don't know which woman you will. [SEP]\",\n",
       " \"[CLS] Sam stood near yesterday every one of the women we'd been discussing. [SEP]\",\n",
       " '[CLS] Truman visited yesterday you. [SEP]',\n",
       " '[CLS] Truman told the story Bob. [SEP]',\n",
       " \"[CLS] While Truman didn't visit me, he did you. [SEP]\",\n",
       " \"[CLS] While Truman didn't tell me a story, he did Rusty. [SEP]\",\n",
       " \"[CLS] While JosÃ© won't talk about Mag, he might about Holly. [SEP]\",\n",
       " \"[CLS] Although Doc might tell it to you, he won't to me. [SEP]\",\n",
       " '[CLS] I think you need to show yourself more than you do anyone else. [SEP]',\n",
       " \"[CLS] While Truman doesn't want to visit every city, he does Barcelona. [SEP]\",\n",
       " \"[CLS] While Rusty might leave in order to please Mag, he won't his father. [SEP]\",\n",
       " \"[CLS] While Doc might claim that Bob had read his book, he won't the paper. [SEP]\",\n",
       " \"[CLS] I'll turn the radio down, but I won't up. [SEP]\",\n",
       " '[CLS] Fred likes eggplants, although he likes cabbage too. [SEP]',\n",
       " '[CLS] Although he likes cabbage too, Fred likes eggplants. [SEP]',\n",
       " '[CLS] Fred gave flowers to his sweetie because Frank had. [SEP]',\n",
       " '[CLS] China is a country that Joe wants to visit, and he will too, if he gets enough money. [SEP]',\n",
       " \"[CLS] Jerry wouldn't read a book by Babel, but Meryl has done so and it was pretty good. [SEP]\",\n",
       " \"[CLS] I know which book Max read, and which book Oscar hasn't done so. [SEP]\",\n",
       " \"[CLS] Joe might wish he had, but this isn't a country he has visited. [SEP]\",\n",
       " '[CLS] While I might want to, this is the kind of thing that Harris has already suggested. [SEP]',\n",
       " '[CLS] We like our friends and they do too. [SEP]',\n",
       " '[CLS] We like our friends and they like our friends too. [SEP]',\n",
       " '[CLS] We like our friends and they like their friends, too. [SEP]',\n",
       " '[CLS] Rusty talked about himself only after Holly did. [SEP]',\n",
       " '[CLS] Rusty talked about himself only after Mary did talk about himself. [SEP]',\n",
       " '[CLS] I could find no solution, but Holly might. [SEP]',\n",
       " '[CLS] Fred talked about everything before Rusty did. [SEP]',\n",
       " '[CLS] Joe will go to the store, even though Fred already has. [SEP]',\n",
       " '[CLS] Today there is little or no official harassment of lesbians and gays by the national government, although autonomous governments might. [SEP]',\n",
       " '[CLS] The candidate was dogged by charges of infidelity and avoiding the draft, or at least trying to. [SEP]',\n",
       " '[CLS] David is a great artist, and when he does, his eyes squint at you. [SEP]',\n",
       " '[CLS] The candidate was dogged by charges of infidelity, or at least trying to. [SEP]',\n",
       " '[CLS] This information could have been released by Gorbachev, but he chose not to. [SEP]',\n",
       " '[CLS] A lot of this material can be presented in a fairly informal and accessible fashion, and often I do. [SEP]',\n",
       " '[CLS] John likes not Mary. [SEP]',\n",
       " '[CLS] John does not like Mary. [SEP]',\n",
       " '[CLS] John meets often Mary. [SEP]',\n",
       " '[CLS] John tries to often meet Mary. [SEP]',\n",
       " '[CLS] John tries to meet often Mary. [SEP]',\n",
       " '[CLS] John tries not to meet Mary. [SEP]',\n",
       " '[CLS] John tries to meet not Mary. [SEP]',\n",
       " '[CLS] Is Mary running the marathon? [SEP]',\n",
       " '[CLS] Runs Mary the marathon? [SEP]',\n",
       " '[CLS] Mary is often running the marathon. [SEP]',\n",
       " '[CLS] Mary runs often the marathon. [SEP]',\n",
       " '[CLS] Mary is not running the marathon. [SEP]',\n",
       " \"[CLS] I didn't, as Bill had thought, go to the store. [SEP]\",\n",
       " '[CLS] I did, as Bill had thought, go to the store. [SEP]',\n",
       " '[CLS] I did not, as Bill had thought, go to the store. [SEP]',\n",
       " '[CLS] The writers could so believe the boy. [SEP]',\n",
       " '[CLS] The writers so believed the boy. [SEP]',\n",
       " '[CLS] The writers did so believe the boy. [SEP]',\n",
       " \"[CLS] The writers didn't so believe the boy. [SEP]\",\n",
       " '[CLS] Rome destroyed Carthage. [SEP]',\n",
       " \"[CLS] Rome's destruction of Carthage was horrific. [SEP]\",\n",
       " '[CLS] John bought the picture of himself that Bill saw. [SEP]',\n",
       " '[CLS] The perception of the problem is quite thorough. [SEP]',\n",
       " '[CLS] The knowledge of the problem is quite thorough. [SEP]',\n",
       " \"[CLS] The problem's perception is quite thorough. [SEP]\",\n",
       " \"[CLS] The problem's knowledge is quite thorough. [SEP]\",\n",
       " '[CLS] The problem knows easily. [SEP]',\n",
       " '[CLS] The ship sank to collect the insurance. [SEP]',\n",
       " '[CLS] The sinking of the ship was very devious. [SEP]',\n",
       " '[CLS] The sinking of the ship to collect the insurance was very devious. [SEP]',\n",
       " \"[CLS] The ship's sinking was very devious. [SEP]\",\n",
       " \"[CLS] The ship's sinking to collect the insurance was very devious. [SEP]\",\n",
       " '[CLS] The testing of such drugs on oneself is too risky. [SEP]',\n",
       " \"[CLS] This drug's testing on oneself is too risky. [SEP]\",\n",
       " '[CLS] The ship was sunk to collect the insurance. [SEP]',\n",
       " '[CLS] This drug must first be tested on oneself. [SEP]',\n",
       " \"[CLS] The president's moral destruction is complete. [SEP]\",\n",
       " '[CLS] The moral destruction of the president was certainly not helpful. [SEP]',\n",
       " '[CLS] Mary wants to wear nice blue German dress. [SEP]',\n",
       " '[CLS] Tomatoes were introduced in Europe after 1492. [SEP]',\n",
       " '[CLS] We rich have impeccable taste. [SEP]',\n",
       " '[CLS] Rich we have impeccable taste. [SEP]',\n",
       " ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels,dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of this model isn't terrible, but it still hovers around 70%. Below there is code for a slightly modified neural network - how does this one perform? Note that in this model, I have added another LSTM layer. You are encouraged to explore the [Keras documentaion](https://keras.io/layers/about-keras-layers/) to explore what kind of layers you can add and how they change performances for different tasks. Different kinds of losses, optimizers, activations and layers can change the flavour of your net dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lstm2 = Sequential()\n",
    "# model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "# model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# history_lstm2 = model_lstm2.fit(input_data_train, labels, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On with BERT!\n",
    "\n",
    "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how a bidirectional transformer embedding like BERT might do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Models\n",
    "\n",
    "### Train Model\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "### Structure of Fine-Tuning Model\n",
    "\n",
    "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "### The Fine-Tuning Process\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
    "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "Credit to Michel Kana's [tutorial](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) and the [tutorial](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=GuE5BqICAne2) by Chris McCormick and Nick Ryan who describe the workings of BERT and the way it is used by the ```transformers``` package. \n",
    "\n",
    "## WARNING: SHIFT TO A GPU ENABLED MACHINE (e.g., Google Colab)\n",
    "\n",
    "Note that you only want to run the following code if you have a GPU. Otherwise, rerun the **same** cells we just ran on the Colab file to train your model, download it to your local, and load it by running\n",
    "```model = BertForSequenceClassification.from_pretrained(\"my_model_directory\", num_labels=2)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following cell can take upto 12 hours or longer if run without a GPU. The [Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) demonstrates how to fine-tune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # This training code is based on the `run_glue.py` script here:\n",
    "# # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# # Set the seed value all over the place to make this reproducible.\n",
    "# seed_val = 42\n",
    "\n",
    "# random.seed(seed_val)\n",
    "# np.random.seed(seed_val)\n",
    "# torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# # Store the average loss after each epoch so we can plot them.\n",
    "# loss_values = []\n",
    "\n",
    "# # For each epoch...\n",
    "# for epoch_i in range(0, epochs):\n",
    "    \n",
    "#     # ========================================\n",
    "#     #               Training\n",
    "#     # ========================================\n",
    "    \n",
    "#     # Perform one full pass over the training set.\n",
    "\n",
    "#     print(\"\")\n",
    "#     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "#     print('Training...')\n",
    "\n",
    "#     # Measure how long the training epoch takes.\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # Reset the total loss for this epoch.\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # Put the model into training mode. Don't be mislead--the call to \n",
    "#     # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "#     # `dropout` and `batchnorm` layers behave differently during training\n",
    "#     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "#     model.train()\n",
    "\n",
    "#     # For each batch of training data...\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "#         # Progress update every 40 batches.\n",
    "#         if step % 40 == 0 and not step == 0:\n",
    "#             # Calculate elapsed time in minutes.\n",
    "#             elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "#             # Report progress.\n",
    "#             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "#         # Unpack this training batch from our dataloader. \n",
    "#         #\n",
    "#         # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "#         # `to` method.\n",
    "#         #\n",
    "#         # `batch` contains three pytorch tensors:\n",
    "#         #   [0]: input ids \n",
    "#         #   [1]: attention masks\n",
    "#         #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "#         # Always clear any previously calculated gradients before performing a\n",
    "#         # backward pass. PyTorch doesn't do this automatically because \n",
    "#         # accumulating the gradients is \"convenient while training RNNs\". \n",
    "#         # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "#         model.zero_grad()        \n",
    "\n",
    "#         # Perform a forward pass (evaluate the model on this training batch).\n",
    "#         # This will return the loss (rather than the model output) because we\n",
    "#         # have provided the `labels`.\n",
    "#         # The documentation for this `model` function is here: \n",
    "#         # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "#         outputs = model(b_input_ids, \n",
    "#                     token_type_ids=None, \n",
    "#                     attention_mask=b_input_mask, \n",
    "#                     labels=b_labels)\n",
    "        \n",
    "#         # The call to `model` always returns a tuple, so we need to pull the \n",
    "#         # loss value out of the tuple.\n",
    "#         loss = outputs[0]\n",
    "\n",
    "#         # Accumulate the training loss over all of the batches so that we can\n",
    "#         # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "#         # single value; the `.item()` function just returns the Python value \n",
    "#         # from the tensor.\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Perform a backward pass to calculate the gradients.\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Clip the norm of the gradients to 1.0.\n",
    "#         # This is to help prevent the \"exploding gradients\" problem.\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "#         # Update parameters and take a step using the computed gradient.\n",
    "#         # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "#         # modified based on their gradients, the learning rate, etc.\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the learning rate.\n",
    "#         scheduler.step()\n",
    "\n",
    "#     # Calculate the average loss over the training data.\n",
    "#     avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "#     # Store the loss value for plotting the learning curve.\n",
    "#     loss_values.append(avg_train_loss)\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "#     print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "#     # ========================================\n",
    "#     #               Validation\n",
    "#     # ========================================\n",
    "#     # After the completion of each training epoch, measure our performance on\n",
    "#     # our validation set.\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"Running Validation...\")\n",
    "\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # Put the model in evaluation mode--the dropout layers behave differently\n",
    "#     # during evaluation.\n",
    "#     model.eval()\n",
    "\n",
    "#     # Tracking variables \n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "#     # Evaluate data for one epoch\n",
    "#     for batch in validation_dataloader:\n",
    "        \n",
    "#         # Add batch to GPU\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "#         # Unpack the inputs from our dataloader\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "#         # Telling the model not to compute or store gradients, saving memory and\n",
    "#         # speeding up validation\n",
    "#         with torch.no_grad():        \n",
    "\n",
    "#             # Forward pass, calculate logit predictions.\n",
    "#             # This will return the logits rather than the loss because we have\n",
    "#             # not provided labels.\n",
    "#             # token_type_ids is the same as the \"segment ids\", which \n",
    "#             # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "#             # The documentation for this `model` function is here: \n",
    "#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "#             outputs = model(b_input_ids, \n",
    "#                             token_type_ids=None, \n",
    "#                             attention_mask=b_input_mask)\n",
    "        \n",
    "#         # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "#         # values prior to applying an activation function like the softmax.\n",
    "#         logits = outputs[0]\n",
    "\n",
    "#         # Move logits and labels to CPU\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "#         # Calculate the accuracy for this batch of test sentences.\n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "#         # Accumulate the total accuracy.\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#         # Track the number of batches\n",
    "#         nb_eval_steps += 1\n",
    "\n",
    "#     # Report the final accuracy for this validation run.\n",
    "#     print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "#     print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Use plot styling from seaborn.\n",
    "# sns.set(style='darkgrid')\n",
    "\n",
    "# # Increase the plot size and font size.\n",
    "# sns.set(font_scale=1.5)\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# # Plot the learning curve.\n",
    "# plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# # Label the plot.\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "Once you tune your model on Colab (or on your own machine if you decided to do that instead), you load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"C://Downloads/model_save\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"../data/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 516 test sentences...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.IntTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-6b924c3d5378>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m       \u001b[1;31m# Forward pass, calculate logit predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m       outputs = model(b_input_ids, token_type_ids=None, \n\u001b[0m\u001b[0;32m     24\u001b[0m                       attention_mask=b_input_mask)\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1494\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1497\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    960\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.IntTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good performance. Note that we used [Matthews Correlation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) to meausure this. It ranges from -1 to 1, with +1 being the best. The Google BERT model has a similar score too, so this model performed quite well. It took a long time though, approximately a day with no GPU. It would be significantly faster if a CUDA enabled machine ran this. Hence, we recommend that you run this on the Collab notebook.\n",
    "\n",
    "The following lines save the model to disk, if you would like to: note that we ran this in the colab file to save it to disk there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# output_dir = './model_save/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# # They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# # Good practice: save your training arguments together with the trained model\n",
    "# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
    "\n",
    "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C://NOW/aiTextsWithTokens.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>publisher</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>reduced_tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1336549</td>\n",
       "      <td>984.0</td>\n",
       "      <td>10-01-04</td>\n",
       "      <td>US</td>\n",
       "      <td>Project Syndicate</td>\n",
       "      <td>http://www.project-syndicate.org/commentary/gr...</td>\n",
       "      <td>Grandmasters and Global Growth</td>\n",
       "      <td>&lt;p&gt; Kenneth Rogoff , Professor of Economics an...</td>\n",
       "      <td>[&lt;, p, &gt;, Kenneth, Rogoff, Professor, of, Econ...</td>\n",
       "      <td>[&lt;, p, &gt;, kenneth, rogoff, professor, economic...</td>\n",
       "      <td>[professor, public, policy, university, bank, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[&lt;, p], [&gt;], [Kenneth, Rogoff, Professor, of,...</td>\n",
       "      <td>[[&lt;, p], [&gt;], [kenneth, rogoff, professor, eco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1336930</td>\n",
       "      <td>958.0</td>\n",
       "      <td>10-01-04</td>\n",
       "      <td>GB</td>\n",
       "      <td>HiddenWires</td>\n",
       "      <td>http://hiddenwires.co.uk/resourcesarticles2010...</td>\n",
       "      <td>The Hydra Project - Middleware for Easy Networ...</td>\n",
       "      <td>&lt;p&gt; Networked sensors and devices have huge po...</td>\n",
       "      <td>[&lt;, p, &gt;, Networked, sensors, and, devices, ha...</td>\n",
       "      <td>[&lt;, p, &gt;, network, sensor, device, huge, poten...</td>\n",
       "      <td>[network, device, huge, potential, ensure, dif...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[&lt;, p, &gt;], [Networked, sensors, and, devices,...</td>\n",
       "      <td>[[&lt;, p, &gt;], [networked, sensors, devices, huge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1338856</td>\n",
       "      <td>4781.0</td>\n",
       "      <td>10-01-06</td>\n",
       "      <td>US</td>\n",
       "      <td>Westword</td>\n",
       "      <td>http://www.westword.com/news/lego-universe-col...</td>\n",
       "      <td>LEGO Universe - colorful plastic's answer to W...</td>\n",
       "      <td>&lt;h&gt; LEGO Universe ? colorful plastic 's answer...</td>\n",
       "      <td>[&lt;, h, &gt;, LEGO, Universe, colorful, plastic', ...</td>\n",
       "      <td>[&lt;, h, &gt;, lego, universe, colorful, plastic, a...</td>\n",
       "      <td>[answer, base, black, way, form, little, build...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[[&lt;, h], [&gt;, LEGO, Universe], [colorful, plast...</td>\n",
       "      <td>[[&lt;, h], [&gt;, lego, universe], [colorful, plast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1340445</td>\n",
       "      <td>1569.0</td>\n",
       "      <td>10-01-07</td>\n",
       "      <td>US</td>\n",
       "      <td>Popular Science</td>\n",
       "      <td>http://www.popsci.com/gadgets/article/2010-01/...</td>\n",
       "      <td>Exclusive: Inside Project Natal's Brain</td>\n",
       "      <td>&lt;h&gt; Exclusive : Inside Project Natal 's Brain ...</td>\n",
       "      <td>[&lt;, h, &gt;, Exclusive, Inside, Project, Natal', ...</td>\n",
       "      <td>[&lt;, h, &gt;, exclusive, inside, project, natal, b...</td>\n",
       "      <td>[project, brain, project, let, control, game, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[[&lt;, h], [&gt;], [Exclusive], [Inside, Project, N...</td>\n",
       "      <td>[[&lt;, h], [&gt;], [exclusive], [inside, project, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1342810</td>\n",
       "      <td>280.0</td>\n",
       "      <td>10-01-09</td>\n",
       "      <td>US</td>\n",
       "      <td>PC Magazine</td>\n",
       "      <td>http://www.pcmag.com/article2/0,2817,2357928,0...</td>\n",
       "      <td>Roxxxy the 'Sex Robot' Debuts at AVN Porn Show</td>\n",
       "      <td>&lt;h&gt; Roxxxy the ' Sex Robot ' Debuts at AVN Por...</td>\n",
       "      <td>[&lt;, h, &gt;, Roxxxy, the, Sex, Robot, Debuts, at,...</td>\n",
       "      <td>[&lt;, h, &gt;, roxxxy, sex, robot, debut, avn, porn...</td>\n",
       "      <td>[robot, robot, robot, meet, engine, programme,...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>[[&lt;, h], [&gt;], [Roxxxy], [the, Sex, Robot, Debu...</td>\n",
       "      <td>[[&lt;, h], [&gt;], [roxxxy], [sex, robot, debuts, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72946</th>\n",
       "      <td>62224699</td>\n",
       "      <td>636.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>ZA</td>\n",
       "      <td>capetownetc.com</td>\n",
       "      <td>https://www.capetownetc.com/news/4ir-commissio...</td>\n",
       "      <td>4IR commission recommends radical changes to e...</td>\n",
       "      <td>&lt;p&gt; President Cyril Ramaphosa 's 4IR commissio...</td>\n",
       "      <td>[&lt;, p, &gt;, President, Cyril, Ramaphosa', \"'s\", ...</td>\n",
       "      <td>[&lt;, p, &gt;, president, cyril, ramaphosa, 4ir, co...</td>\n",
       "      <td>[president, commission, release, statement, we...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, p, &gt;, President, Cyril, Ramaphosa, 's, 4I...</td>\n",
       "      <td>[[&lt;, p, &gt;, president, cyril, ramaphosa, 4ir, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72947</th>\n",
       "      <td>62224797</td>\n",
       "      <td>457.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2oceansvibe.com</td>\n",
       "      <td>https://www.2oceansvibe.com/2020/10/30/how-sas...</td>\n",
       "      <td>How SA's New `Buy Now, Pay Later' Business Mod...</td>\n",
       "      <td>&lt;p&gt; While it might seem like a good idea to pa...</td>\n",
       "      <td>[&lt;, p, &gt;, While, it, might, seem, like, a, goo...</td>\n",
       "      <td>[&lt;, p, &gt;, like, good, idea, pay, credit, cours...</td>\n",
       "      <td>[good, idea, pay, credit, course, month, year,...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, p], [&gt;, While, it, might, seem, like, a, ...</td>\n",
       "      <td>[[&lt;, p], [&gt;, like, good, idea, pay, credit, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72948</th>\n",
       "      <td>62224321</td>\n",
       "      <td>649.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>ZA</td>\n",
       "      <td>themediaonline.co.za</td>\n",
       "      <td>https://themediaonline.co.za/2020/10/nothing-s...</td>\n",
       "      <td>Wavemaker's new bespoke OS delivers 'provocati...</td>\n",
       "      <td>&lt;h&gt; Wavemaker 's new bespoke OS delivers ' pro...</td>\n",
       "      <td>[&lt;, h, &gt;, Wavemaker', \"'s\", 'new, bespoke, OS,...</td>\n",
       "      <td>[&lt;, h, &gt;, wavemaker, new, bespeak, os, deliver...</td>\n",
       "      <td>[deliver, plan, launch, news, different, point...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, h], [&gt;], [Wavemaker, 's, new, bespoke, OS...</td>\n",
       "      <td>[[&lt;, h], [&gt;], [wavemaker, new, bespoke, os, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72949</th>\n",
       "      <td>52056036</td>\n",
       "      <td>547.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>NG</td>\n",
       "      <td>independent.ng</td>\n",
       "      <td>https://www.independent.ng/the-increasing-infl...</td>\n",
       "      <td>The Increasing Influence of Cryptocurrency in ...</td>\n",
       "      <td>&lt;h&gt; The Increasing Influence of Cryptocurrency...</td>\n",
       "      <td>[&lt;, h, &gt;, The, Increasing, Influence, of, Cryp...</td>\n",
       "      <td>[&lt;, h, &gt;, increase, influence, cryptocurrency,...</td>\n",
       "      <td>[increase, economic, economic, make, shift, me...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, h], [&gt;], [The, Increasing, Influence, of,...</td>\n",
       "      <td>[[&lt;, h], [&gt;], [increasing, influence, cryptocu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72950</th>\n",
       "      <td>52056515</td>\n",
       "      <td>2028.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>NG</td>\n",
       "      <td>proshareng.com</td>\n",
       "      <td>https://www.proshareng.com/news/DATA%20&amp;amp;%2...</td>\n",
       "      <td>AI in Foreign Exchange Trading (Forex) - Curre...</td>\n",
       "      <td>&lt;h&gt; AI in Foreign Exchange Trading ( Forex ) -...</td>\n",
       "      <td>[&lt;, h, &gt;, AI, in, Foreign, Exchange, Trading, ...</td>\n",
       "      <td>[&lt;, h, &gt;, have, foreign, exchange, trade, fore...</td>\n",
       "      <td>[foreign, exchange, trade, current, state, sec...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, h], [&gt;, AI, in, Foreign, Exchange, Tradin...</td>\n",
       "      <td>[[&lt;, h], [&gt;, ai, foreign, exchange, trading], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72951 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  length      date country             publisher  \\\n",
       "0       1336549   984.0  10-01-04      US     Project Syndicate   \n",
       "1       1336930   958.0  10-01-04      GB           HiddenWires   \n",
       "2       1338856  4781.0  10-01-06      US              Westword   \n",
       "3       1340445  1569.0  10-01-07      US       Popular Science   \n",
       "4       1342810   280.0  10-01-09      US           PC Magazine   \n",
       "...         ...     ...       ...     ...                   ...   \n",
       "72946  62224699   636.0  20-10-31      ZA       capetownetc.com   \n",
       "72947  62224797   457.0  20-10-31      ZA       2oceansvibe.com   \n",
       "72948  62224321   649.0  20-10-31      ZA  themediaonline.co.za   \n",
       "72949  52056036   547.0  20-10-31      NG        independent.ng   \n",
       "72950  52056515  2028.0  20-10-31      NG        proshareng.com   \n",
       "\n",
       "                                                     url  \\\n",
       "0      http://www.project-syndicate.org/commentary/gr...   \n",
       "1      http://hiddenwires.co.uk/resourcesarticles2010...   \n",
       "2      http://www.westword.com/news/lego-universe-col...   \n",
       "3      http://www.popsci.com/gadgets/article/2010-01/...   \n",
       "4      http://www.pcmag.com/article2/0,2817,2357928,0...   \n",
       "...                                                  ...   \n",
       "72946  https://www.capetownetc.com/news/4ir-commissio...   \n",
       "72947  https://www.2oceansvibe.com/2020/10/30/how-sas...   \n",
       "72948  https://themediaonline.co.za/2020/10/nothing-s...   \n",
       "72949  https://www.independent.ng/the-increasing-infl...   \n",
       "72950  https://www.proshareng.com/news/DATA%20&amp;%2...   \n",
       "\n",
       "                                                   title  \\\n",
       "0                         Grandmasters and Global Growth   \n",
       "1      The Hydra Project - Middleware for Easy Networ...   \n",
       "2      LEGO Universe - colorful plastic's answer to W...   \n",
       "3                Exclusive: Inside Project Natal's Brain   \n",
       "4         Roxxxy the 'Sex Robot' Debuts at AVN Porn Show   \n",
       "...                                                  ...   \n",
       "72946  4IR commission recommends radical changes to e...   \n",
       "72947  How SA's New `Buy Now, Pay Later' Business Mod...   \n",
       "72948  Wavemaker's new bespoke OS delivers 'provocati...   \n",
       "72949  The Increasing Influence of Cryptocurrency in ...   \n",
       "72950  AI in Foreign Exchange Trading (Forex) - Curre...   \n",
       "\n",
       "                                                    text  \\\n",
       "0      <p> Kenneth Rogoff , Professor of Economics an...   \n",
       "1      <p> Networked sensors and devices have huge po...   \n",
       "2      <h> LEGO Universe ? colorful plastic 's answer...   \n",
       "3      <h> Exclusive : Inside Project Natal 's Brain ...   \n",
       "4      <h> Roxxxy the ' Sex Robot ' Debuts at AVN Por...   \n",
       "...                                                  ...   \n",
       "72946  <p> President Cyril Ramaphosa 's 4IR commissio...   \n",
       "72947  <p> While it might seem like a good idea to pa...   \n",
       "72948  <h> Wavemaker 's new bespoke OS delivers ' pro...   \n",
       "72949  <h> The Increasing Influence of Cryptocurrency...   \n",
       "72950  <h> AI in Foreign Exchange Trading ( Forex ) -...   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "0      [<, p, >, Kenneth, Rogoff, Professor, of, Econ...   \n",
       "1      [<, p, >, Networked, sensors, and, devices, ha...   \n",
       "2      [<, h, >, LEGO, Universe, colorful, plastic', ...   \n",
       "3      [<, h, >, Exclusive, Inside, Project, Natal', ...   \n",
       "4      [<, h, >, Roxxxy, the, Sex, Robot, Debuts, at,...   \n",
       "...                                                  ...   \n",
       "72946  [<, p, >, President, Cyril, Ramaphosa', \"'s\", ...   \n",
       "72947  [<, p, >, While, it, might, seem, like, a, goo...   \n",
       "72948  [<, h, >, Wavemaker', \"'s\", 'new, bespoke, OS,...   \n",
       "72949  [<, h, >, The, Increasing, Influence, of, Cryp...   \n",
       "72950  [<, h, >, AI, in, Foreign, Exchange, Trading, ...   \n",
       "\n",
       "                                       normalized_tokens  \\\n",
       "0      [<, p, >, kenneth, rogoff, professor, economic...   \n",
       "1      [<, p, >, network, sensor, device, huge, poten...   \n",
       "2      [<, h, >, lego, universe, colorful, plastic, a...   \n",
       "3      [<, h, >, exclusive, inside, project, natal, b...   \n",
       "4      [<, h, >, roxxxy, sex, robot, debut, avn, porn...   \n",
       "...                                                  ...   \n",
       "72946  [<, p, >, president, cyril, ramaphosa, 4ir, co...   \n",
       "72947  [<, p, >, like, good, idea, pay, credit, cours...   \n",
       "72948  [<, h, >, wavemaker, new, bespeak, os, deliver...   \n",
       "72949  [<, h, >, increase, influence, cryptocurrency,...   \n",
       "72950  [<, h, >, have, foreign, exchange, trade, fore...   \n",
       "\n",
       "                                          reduced_tokens  year  month  day  \\\n",
       "0      [professor, public, policy, university, bank, ...    10      1    4   \n",
       "1      [network, device, huge, potential, ensure, dif...    10      1    4   \n",
       "2      [answer, base, black, way, form, little, build...    10      1    6   \n",
       "3      [project, brain, project, let, control, game, ...    10      1    7   \n",
       "4      [robot, robot, robot, meet, engine, programme,...    10      1    9   \n",
       "...                                                  ...   ...    ...  ...   \n",
       "72946  [president, commission, release, statement, we...    20     10   31   \n",
       "72947  [good, idea, pay, credit, course, month, year,...    20     10   31   \n",
       "72948  [deliver, plan, launch, news, different, point...    20     10   31   \n",
       "72949  [increase, economic, economic, make, shift, me...    20     10   31   \n",
       "72950  [foreign, exchange, trade, current, state, sec...    20     10   31   \n",
       "\n",
       "                                         tokenized_sents  \\\n",
       "0      [[<, p], [>], [Kenneth, Rogoff, Professor, of,...   \n",
       "1      [[<, p, >], [Networked, sensors, and, devices,...   \n",
       "2      [[<, h], [>, LEGO, Universe], [colorful, plast...   \n",
       "3      [[<, h], [>], [Exclusive], [Inside, Project, N...   \n",
       "4      [[<, h], [>], [Roxxxy], [the, Sex, Robot, Debu...   \n",
       "...                                                  ...   \n",
       "72946  [[<, p, >, President, Cyril, Ramaphosa, 's, 4I...   \n",
       "72947  [[<, p], [>, While, it, might, seem, like, a, ...   \n",
       "72948  [[<, h], [>], [Wavemaker, 's, new, bespoke, OS...   \n",
       "72949  [[<, h], [>], [The, Increasing, Influence, of,...   \n",
       "72950  [[<, h], [>, AI, in, Foreign, Exchange, Tradin...   \n",
       "\n",
       "                                        normalized_sents  \n",
       "0      [[<, p], [>], [kenneth, rogoff, professor, eco...  \n",
       "1      [[<, p, >], [networked, sensors, devices, huge...  \n",
       "2      [[<, h], [>, lego, universe], [colorful, plast...  \n",
       "3      [[<, h], [>], [exclusive], [inside, project, n...  \n",
       "4      [[<, h], [>], [roxxxy], [sex, robot, debuts, a...  \n",
       "...                                                  ...  \n",
       "72946  [[<, p, >, president, cyril, ramaphosa, 4ir, c...  \n",
       "72947  [[<, p], [>, like, good, idea, pay, credit, co...  \n",
       "72948  [[<, h], [>], [wavemaker, new, bespoke, os, de...  \n",
       "72949  [[<, h], [>], [increasing, influence, cryptocu...  \n",
       "72950  [[<, h], [>, ai, foreign, exchange, trading], ...  \n",
       "\n",
       "[72951 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jacy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2015 = []\n",
    "sentences2017 = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['year'] == 15:\n",
    "        for sentence in nltk.tokenize.sent_tokenize(row['text']):\n",
    "            sentences2015.append(sentence)\n",
    "            \n",
    "for index, row in df.iterrows():\n",
    "    if row['year'] == 17:\n",
    "        for sentence in nltk.tokenize.sent_tokenize(row['text']):\n",
    "            sentences2017.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39343"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282260"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sample_size = 10000\n",
    "\n",
    "sentences2015 = random.sample(sentences2015,sample_size)\n",
    "sentences2017 = random.sample(sentences2017,sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences2015 + sentences2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] <p> As the figure shows , the TFP slowdown is concentrated in industries that @ @ @ @ @ @ @ @ @ @ of IT producers was inordinately high in the late 1990s , accounting for over half of overall TFP growth in this period -- even though they account for only 5% of the economy . [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2508]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] <p> We believe that at least three ingredients are necessary to meet the demand for frictionless IT : <p> Ease of use <p> Speed <p> Integration <p> Ease of use <p> A key enabler for a Frictionless IT is a smooth user experience ( UX ) . [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] <p> In 1996 , when the government communication apparatus has become more sophisticated , Access was quietly shelved . [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] With its 360 smart bed , Sleep Number promises to make your nights ( and days ) more comfortable . [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[17002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0] * sample_size + [1] * sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C://NOW/sentences.txt', 'wb') as fp:\n",
    "    pickle.dump(sentences, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C://NOW/labels.txt', 'wb') as fp:\n",
    "    pickle.dump(labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C://NOW/sentences.txt', 'rb') as fp:\n",
    "    sentences = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels,dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', '<', 'p', '>', \"'\", 'and', 'for', 'the', 'last', 'two', 'years', 'we', \"'\", 've', 'been', 'talking', 'this', 'idea', 'through', 'and', 'planning', 'the', 'operation', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/563 [==============================] - 24s 41ms/step - loss: 0.6940 - accuracy: 0.4992\n",
      "Epoch 2/10\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.6933 - accuracy: 0.4984\n",
      "Epoch 3/10\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.6932 - accuracy: 0.4998\n",
      "Epoch 4/10\n",
      "563/563 [==============================] - 23s 41ms/step - loss: 0.6934 - accuracy: 0.4957\n",
      "Epoch 5/10\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.6929 - accuracy: 0.5066\n",
      "Epoch 6/10\n",
      "563/563 [==============================] - 23s 42ms/step - loss: 0.6925 - accuracy: 0.5067\n",
      "Epoch 7/10\n",
      "563/563 [==============================] - 24s 42ms/step - loss: 0.6925 - accuracy: 0.4987\n",
      "Epoch 8/10\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.6923 - accuracy: 0.5051\n",
      "Epoch 9/10\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.6924 - accuracy: 0.5009\n",
      "Epoch 10/10\n",
      "563/563 [==============================] - 24s 44ms/step - loss: 0.6920 - accuracy: 0.4977\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The LSTM seems stuck at 50%, failing to learn the difference between AI articles from 2015 versus 2017.\n",
    "#This is surprising, but trying some different settings, I couldn't increase it.\n",
    "#Training BERT in Google Colab file, it achieves 66% accuracy, much better than LSTM and than the ML methods used before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings, Context Words\n",
    "\n",
    "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTS model uses a WordPiece technique to do its tokenizing, as described in the paper. That's why the word embedding is split up the way it is.\n",
    "A quick peek at what the voabulary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peninsula',\n",
       " 'adults',\n",
       " 'novels',\n",
       " 'emerged',\n",
       " 'vienna',\n",
       " 'metro',\n",
       " 'debuted',\n",
       " 'shoes',\n",
       " 'tamil',\n",
       " 'songwriter',\n",
       " 'meets',\n",
       " 'prove',\n",
       " 'beating',\n",
       " 'instance',\n",
       " 'heaven',\n",
       " 'scared',\n",
       " 'sending',\n",
       " 'marks',\n",
       " 'artistic',\n",
       " 'passage',\n",
       " 'superior',\n",
       " '03',\n",
       " 'significantly',\n",
       " 'shopping',\n",
       " '##tive',\n",
       " 'retained',\n",
       " '##izing',\n",
       " 'malaysia',\n",
       " 'technique',\n",
       " 'cheeks']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[6000:6030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment ID\n",
    "\n",
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in â€œtokenized_text,â€ we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
    "\n",
    "If you want to process two sentences, assign each word in the first sentence plus the â€˜[SEP]â€™ token a 0, and all tokens of the second sentence a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did for classification, we now convert these segments to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is the hidden state layer, and this is what we pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_embedding(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0][0][0]), len(output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
    "\n",
    "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = output[0]\n",
    "sentence_embedding = output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "        [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "        [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "        ...,\n",
       "        [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "        [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "        [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s take a quick look at the range of values for a given layer and token.\n",
    "\n",
    "Youâ€™ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = word_embeddings[0][0]\n",
    "vec = vec.detach().numpy()\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the later looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "So each of those tokens have embedding values - let us try and compare them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to create the vectors is to sum the last four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Vector\n",
    "\n",
    "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_embedding_0), len(sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    [ 0.90010506 -0.53804076 -0.16690888  0.224162    0.6896582 ]\n",
      "bank robber   [ 0.79771274 -0.52172726 -0.19837011  0.18898581  0.59409297]\n",
      "river bank    [ 0.29608896 -0.2856335  -0.03818353  0.16736226  0.77126306]\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.95\n",
      "Vector similarity for *different* meanings:  0.70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00831317063421011"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings = model(tokens_tensor)[0]\n",
    "    sentence_embeddings = model(tokens_tensor)[1]\n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_10 = word_vector(text, 6, model_embedding, tokenizer)\n",
    "word_6 = word_vector(text, 10, model_embedding, tokenizer)\n",
    "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings = model(tokens_tensor)[0]\n",
    "    sentence_embeddings = model(tokens_tensor)[1]\n",
    "    token_vecs = []\n",
    "    \n",
    "    for embedding in word_embeddings[0]:\n",
    "        cat_vec = embedding.detach().numpy()\n",
    "        token_vecs.append(cat_vec)\n",
    "        \n",
    "    if method == \"average\":\n",
    "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
    "    if method == \"model\":\n",
    "        sentence_embedding = sentence_embeddings\n",
    "    # do something\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
    "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metrics\n",
    "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river â€œbankâ€ and not a financial institution â€œbankâ€. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
    "\n",
    "### Using the Vectors\n",
    "\n",
    "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
    "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
    "\n",
    "### Using Transformers Pipelines\n",
    "\n",
    "The context vectors make the other pipeline functions which transformers has built in a lot more powerful. \n",
    "\n",
    "### NOTE\n",
    "The pipeline functionality in transformers is currently being worked on and might be broken, so it is an optional part of the exercise. Do try to uncomment the lines of code and try to see if it works, though! If you have managed to get transformers v2.5.1 installed, it will work - I managed to get it to work sometimes, it can be annoying to get it to work but when it works it works well.\n",
    "\n",
    "Consider the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85055f77b626427ab1848a56b9e92745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=629.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e85e735eba4eb5bc90e9b6b31757bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=267844284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dede1275eed540d4aa7964c27352ebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bffd7c714949348596f24b1931f4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997735023498535}]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a strong positive sentiment, which we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.999719500541687}]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"I'm so sad that I have to spend this weekend just doing HW and readings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative label, bingo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14b29ddb3414e4398cefa9e00f77526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c168d3b64be44cd29947b2a4074b41f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=260793700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a6b0642d6e4dc9825abe36952e2ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dd0871ed694e0796795d037d8e7ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Allocate a pipeline for question-answering\n",
    "nlp_question = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.986053466796875,\n",
       " 'start': 34,\n",
       " 'end': 64,\n",
       " 'answer': 'analysing complex textual data'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "    'question': 'What is my favorite thing to do on weekends ?',\n",
    "    'context': 'There is nothing I like more than analysing complex textual data all weekend '\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also great at question-answering tasks!\n",
    "We can also extract features, as we manually did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36b2e9c3f1d4abda172d620f6578da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=411.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0d19206a9b440cbe561b01bd7dc5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=263273408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccea0fb274d54b6a9702efb5e695c390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73894741c68f4810961cf4c6ce6c8b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_feature = pipeline('feature-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = nlp_feature(\"Just sitting here exploring data all day long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.28079739212989807,\n",
       "   0.06644454598426819,\n",
       "   -0.1676396280527115,\n",
       "   -0.1689605712890625,\n",
       "   -0.22660814225673676,\n",
       "   -0.14127133786678314,\n",
       "   0.4210294485092163,\n",
       "   -0.09199243783950806,\n",
       "   -0.031907856464385986,\n",
       "   -0.8537281155586243,\n",
       "   -0.2683808207511902,\n",
       "   0.0856994241476059,\n",
       "   -0.13404318690299988,\n",
       "   -0.03496028482913971,\n",
       "   -0.5356058478355408,\n",
       "   -0.029006775468587875,\n",
       "   0.2011086642742157,\n",
       "   0.08040318638086319,\n",
       "   -0.08755529671907425,\n",
       "   -0.08443020284175873,\n",
       "   0.05363833159208298,\n",
       "   -0.18426793813705444,\n",
       "   0.5852551460266113,\n",
       "   -0.1716003119945526,\n",
       "   0.05120455473661423,\n",
       "   0.12319860607385635,\n",
       "   0.37301185727119446,\n",
       "   0.161519393324852,\n",
       "   -0.17832434177398682,\n",
       "   0.5117566585540771,\n",
       "   -0.020196348428726196,\n",
       "   0.19047735631465912,\n",
       "   0.06237804889678955,\n",
       "   0.057217247784137726,\n",
       "   -0.36198362708091736,\n",
       "   0.22840596735477448,\n",
       "   -0.1847357451915741,\n",
       "   -0.21770460903644562,\n",
       "   -0.07018128782510757,\n",
       "   -0.09863685071468353,\n",
       "   -0.5033516883850098,\n",
       "   0.11645970493555069,\n",
       "   0.5387322902679443,\n",
       "   -0.1830195039510727,\n",
       "   0.04567192494869232,\n",
       "   -0.4805179238319397,\n",
       "   0.07694827020168304,\n",
       "   0.03903816640377045,\n",
       "   -0.08155325800180435,\n",
       "   0.1363123208284378,\n",
       "   -0.09813515096902847,\n",
       "   0.09356562048196793,\n",
       "   -0.19313709437847137,\n",
       "   -0.04096965119242668,\n",
       "   0.15128129720687866,\n",
       "   0.08282728493213654,\n",
       "   -0.06292302906513214,\n",
       "   0.13535535335540771,\n",
       "   -0.5575191974639893,\n",
       "   0.26149630546569824,\n",
       "   -0.07125746458768845,\n",
       "   0.03385502099990845,\n",
       "   0.33907952904701233,\n",
       "   0.06560904532670975,\n",
       "   -0.22296351194381714,\n",
       "   0.07824642956256866,\n",
       "   0.04569856449961662,\n",
       "   0.2158837914466858,\n",
       "   -0.19988062977790833,\n",
       "   -0.21166878938674927,\n",
       "   0.0912008211016655,\n",
       "   0.22718776762485504,\n",
       "   0.30304351449012756,\n",
       "   0.8904079794883728,\n",
       "   0.22981885075569153,\n",
       "   -0.2115686982870102,\n",
       "   0.33373385667800903,\n",
       "   -0.04453232139348984,\n",
       "   -0.027119390666484833,\n",
       "   -0.0959383100271225,\n",
       "   0.05773613601922989,\n",
       "   0.19910456240177155,\n",
       "   -0.11624036729335785,\n",
       "   -0.22660034894943237,\n",
       "   -0.028790391981601715,\n",
       "   -0.13013115525245667,\n",
       "   0.10612837225198746,\n",
       "   -0.11570253223180771,\n",
       "   -0.08082118630409241,\n",
       "   0.15435487031936646,\n",
       "   0.16014021635055542,\n",
       "   -0.15008312463760376,\n",
       "   -0.2460642009973526,\n",
       "   0.04691625386476517,\n",
       "   -0.2078040987253189,\n",
       "   0.1048409715294838,\n",
       "   -0.04270905256271362,\n",
       "   0.07523734867572784,\n",
       "   6.085381507873535,\n",
       "   -0.061667684465646744,\n",
       "   -0.23195797204971313,\n",
       "   0.06998211145401001,\n",
       "   0.09388401359319687,\n",
       "   -0.11683951318264008,\n",
       "   0.24049417674541473,\n",
       "   -0.3288646638393402,\n",
       "   -0.0349486768245697,\n",
       "   -0.46981382369995117,\n",
       "   0.034410007297992706,\n",
       "   0.4979921579360962,\n",
       "   0.39733439683914185,\n",
       "   0.08457016944885254,\n",
       "   0.1202297955751419,\n",
       "   -0.10584186017513275,\n",
       "   -0.09611070901155472,\n",
       "   -0.32829272747039795,\n",
       "   0.0967196449637413,\n",
       "   0.049704212695360184,\n",
       "   0.07413192838430405,\n",
       "   -0.21588198840618134,\n",
       "   0.1565270721912384,\n",
       "   0.005386412143707275,\n",
       "   0.9506381154060364,\n",
       "   0.10336960107088089,\n",
       "   -0.11216200143098831,\n",
       "   -0.0011061746627092361,\n",
       "   -0.0011707106605172157,\n",
       "   -0.2742651104927063,\n",
       "   0.10748107731342316,\n",
       "   -0.16263100504875183,\n",
       "   -0.48938649892807007,\n",
       "   -0.22806021571159363,\n",
       "   -0.07158304750919342,\n",
       "   -0.14923249185085297,\n",
       "   0.10891927778720856,\n",
       "   0.008096262812614441,\n",
       "   -0.04215297847986221,\n",
       "   -0.03626151382923126,\n",
       "   -0.8590203523635864,\n",
       "   0.11949525773525238,\n",
       "   0.0970349982380867,\n",
       "   -0.09249446541070938,\n",
       "   0.09470698237419128,\n",
       "   -0.16255396604537964,\n",
       "   -0.28433606028556824,\n",
       "   2.6244137287139893,\n",
       "   -0.12164174020290375,\n",
       "   0.030225686728954315,\n",
       "   -0.12604986131191254,\n",
       "   0.0535709373652935,\n",
       "   -0.08468535542488098,\n",
       "   -0.1874813735485077,\n",
       "   -0.24895018339157104,\n",
       "   0.23051658272743225,\n",
       "   -0.2518126964569092,\n",
       "   -0.21527042984962463,\n",
       "   0.183339923620224,\n",
       "   0.2167528122663498,\n",
       "   0.05431940034031868,\n",
       "   0.09860699623823166,\n",
       "   -1.02021324634552,\n",
       "   -0.00730133056640625,\n",
       "   -0.22835968434810638,\n",
       "   0.23675411939620972,\n",
       "   0.1346423625946045,\n",
       "   -0.23988938331604004,\n",
       "   -0.0015056915581226349,\n",
       "   -0.6380895972251892,\n",
       "   0.023236026987433434,\n",
       "   0.3602570593357086,\n",
       "   0.049168866127729416,\n",
       "   0.03163653612136841,\n",
       "   -2.1947343349456787,\n",
       "   0.3170281946659088,\n",
       "   0.13879570364952087,\n",
       "   0.1367826610803604,\n",
       "   0.0033983923494815826,\n",
       "   0.13875310122966766,\n",
       "   0.00503048300743103,\n",
       "   -0.28041306138038635,\n",
       "   -0.17104019224643707,\n",
       "   0.3840469419956207,\n",
       "   0.1368083506822586,\n",
       "   0.08550233393907547,\n",
       "   -0.40323561429977417,\n",
       "   0.01595005951821804,\n",
       "   0.12988652288913727,\n",
       "   -0.19909214973449707,\n",
       "   -0.03933049365878105,\n",
       "   0.007921174168586731,\n",
       "   -0.13287608325481415,\n",
       "   -0.3336023986339569,\n",
       "   -0.12062512338161469,\n",
       "   0.19588850438594818,\n",
       "   0.19319672882556915,\n",
       "   0.13310924172401428,\n",
       "   0.05858069285750389,\n",
       "   -0.09095718711614609,\n",
       "   -0.061363860964775085,\n",
       "   0.17067165672779083,\n",
       "   -0.18688462674617767,\n",
       "   -0.12096941471099854,\n",
       "   -0.14287243783473969,\n",
       "   0.1442723274230957,\n",
       "   0.36059868335723877,\n",
       "   0.19136767089366913,\n",
       "   -0.03786735609173775,\n",
       "   0.11734577268362045,\n",
       "   0.17427387833595276,\n",
       "   -0.06699492037296295,\n",
       "   -0.1957787275314331,\n",
       "   0.025702495127916336,\n",
       "   0.08491037786006927,\n",
       "   0.4213598966598511,\n",
       "   0.5814640522003174,\n",
       "   -0.39639273285865784,\n",
       "   -0.0331435352563858,\n",
       "   -0.08839753270149231,\n",
       "   0.09495881199836731,\n",
       "   0.04652809724211693,\n",
       "   -0.10590063035488129,\n",
       "   0.12356510013341904,\n",
       "   -0.17063230276107788,\n",
       "   -0.11698159575462341,\n",
       "   -0.23107433319091797,\n",
       "   0.055374834686517715,\n",
       "   0.2940722405910492,\n",
       "   0.20254641771316528,\n",
       "   0.011805985122919083,\n",
       "   -0.3329519033432007,\n",
       "   0.20288273692131042,\n",
       "   0.18493351340293884,\n",
       "   0.09737390279769897,\n",
       "   -0.29335328936576843,\n",
       "   -0.014713019132614136,\n",
       "   -0.03792170435190201,\n",
       "   -0.01711210049688816,\n",
       "   0.08761607110500336,\n",
       "   -0.030811890959739685,\n",
       "   -0.08504302054643631,\n",
       "   0.07453301548957825,\n",
       "   -0.0010541193187236786,\n",
       "   0.3211292028427124,\n",
       "   0.024368198588490486,\n",
       "   0.007990086451172829,\n",
       "   0.06423593312501907,\n",
       "   0.34073495864868164,\n",
       "   0.2983768880367279,\n",
       "   0.07283438742160797,\n",
       "   -0.02352602779865265,\n",
       "   0.6100216507911682,\n",
       "   0.10051986575126648,\n",
       "   -0.18003559112548828,\n",
       "   -0.2914907932281494,\n",
       "   -0.534946858882904,\n",
       "   -0.14874981343746185,\n",
       "   -0.04322604462504387,\n",
       "   -1.1983941793441772,\n",
       "   -0.1127440556883812,\n",
       "   -0.333660751581192,\n",
       "   0.27879229187965393,\n",
       "   -2.9348483085632324,\n",
       "   0.024573909118771553,\n",
       "   0.2400948405265808,\n",
       "   -0.00154054444283247,\n",
       "   -0.10958177596330643,\n",
       "   0.29140979051589966,\n",
       "   0.0977490022778511,\n",
       "   -0.4551195204257965,\n",
       "   0.23169508576393127,\n",
       "   -0.46620821952819824,\n",
       "   0.02112683653831482,\n",
       "   -0.6199561953544617,\n",
       "   0.0625053197145462,\n",
       "   -0.12774643301963806,\n",
       "   0.2570406496524811,\n",
       "   -0.06207545846700668,\n",
       "   -0.05455480143427849,\n",
       "   -0.23015956580638885,\n",
       "   -0.21192118525505066,\n",
       "   -0.14584368467330933,\n",
       "   0.05948442220687866,\n",
       "   -0.3123905658721924,\n",
       "   0.4359769821166992,\n",
       "   -0.2780083417892456,\n",
       "   0.3652206063270569,\n",
       "   0.34398317337036133,\n",
       "   0.19921992719173431,\n",
       "   0.10600961744785309,\n",
       "   3.681917190551758,\n",
       "   0.27654993534088135,\n",
       "   -0.15455567836761475,\n",
       "   0.09237831830978394,\n",
       "   -0.30224043130874634,\n",
       "   0.013119963929057121,\n",
       "   0.043090589344501495,\n",
       "   -0.4590860903263092,\n",
       "   0.030745282769203186,\n",
       "   -0.07204127311706543,\n",
       "   -0.08520261198282242,\n",
       "   -0.23873865604400635,\n",
       "   0.05929214507341385,\n",
       "   -0.44445255398750305,\n",
       "   -0.054805684834718704,\n",
       "   -0.43976718187332153,\n",
       "   0.045696962624788284,\n",
       "   0.03808179497718811,\n",
       "   0.10586255043745041,\n",
       "   -0.6242749691009521,\n",
       "   -0.11423903703689575,\n",
       "   0.07951873540878296,\n",
       "   0.08646097779273987,\n",
       "   -0.040216557681560516,\n",
       "   0.06479848176240921,\n",
       "   -0.0052052512764930725,\n",
       "   -0.2825011909008026,\n",
       "   -0.3815919756889343,\n",
       "   0.2738403379917145,\n",
       "   0.016155295073986053,\n",
       "   -0.7521315813064575,\n",
       "   0.25282713770866394,\n",
       "   0.1729852855205536,\n",
       "   -0.3031434714794159,\n",
       "   -0.05463679879903793,\n",
       "   0.06651036441326141,\n",
       "   -0.06294246017932892,\n",
       "   -0.04816779866814613,\n",
       "   -0.1764613837003708,\n",
       "   -0.23466043174266815,\n",
       "   -0.01383470743894577,\n",
       "   -0.16199521720409393,\n",
       "   0.09453590959310532,\n",
       "   -0.300756573677063,\n",
       "   -0.17989663779735565,\n",
       "   -0.195979505777359,\n",
       "   -0.14192023873329163,\n",
       "   -0.10136270523071289,\n",
       "   0.26560544967651367,\n",
       "   -0.21154847741127014,\n",
       "   0.030699465423822403,\n",
       "   -0.19061225652694702,\n",
       "   0.04466816782951355,\n",
       "   -0.16349688172340393,\n",
       "   -0.008528419770300388,\n",
       "   0.26784637570381165,\n",
       "   -0.07709279656410217,\n",
       "   0.3252994418144226,\n",
       "   -0.053822651505470276,\n",
       "   0.07142578065395355,\n",
       "   0.18285983800888062,\n",
       "   -0.02894607186317444,\n",
       "   0.09669433534145355,\n",
       "   0.11392445117235184,\n",
       "   0.10665477812290192,\n",
       "   -0.04228644818067551,\n",
       "   0.05124228447675705,\n",
       "   -0.08662649989128113,\n",
       "   -0.24653716385364532,\n",
       "   -0.01491432636976242,\n",
       "   0.14906872808933258,\n",
       "   -0.06340408325195312,\n",
       "   -1.8984965085983276,\n",
       "   0.1799720972776413,\n",
       "   0.05148749426007271,\n",
       "   -0.2742824852466583,\n",
       "   -0.022634752094745636,\n",
       "   -0.12399712949991226,\n",
       "   0.014951542019844055,\n",
       "   -0.03199751675128937,\n",
       "   0.37550485134124756,\n",
       "   0.5077054500579834,\n",
       "   0.0516841784119606,\n",
       "   0.3939162492752075,\n",
       "   -0.017290394753217697,\n",
       "   -0.3436947166919708,\n",
       "   0.12561103701591492,\n",
       "   0.10647232085466385,\n",
       "   0.556349515914917,\n",
       "   0.16544219851493835,\n",
       "   -0.4380694031715393,\n",
       "   -0.29366329312324524,\n",
       "   0.13457034528255463,\n",
       "   0.41246673464775085,\n",
       "   -0.10533411055803299,\n",
       "   0.14395831525325775,\n",
       "   0.03666151314973831,\n",
       "   0.08479950577020645,\n",
       "   -0.36918801069259644,\n",
       "   0.012730087153613567,\n",
       "   -0.1631344109773636,\n",
       "   0.4313426613807678,\n",
       "   -0.15211862325668335,\n",
       "   0.02971781976521015,\n",
       "   -0.1978994756937027,\n",
       "   -0.11578582227230072,\n",
       "   -0.10289061069488525,\n",
       "   -0.11375577747821808,\n",
       "   0.16022810339927673,\n",
       "   0.3630654215812683,\n",
       "   -0.3239903748035431,\n",
       "   -0.06578429788351059,\n",
       "   0.00767158716917038,\n",
       "   0.001818247139453888,\n",
       "   0.1339440643787384,\n",
       "   -0.1764811873435974,\n",
       "   -0.2676340341567993,\n",
       "   0.15191711485385895,\n",
       "   -0.10827604681253433,\n",
       "   -1.3623851537704468,\n",
       "   -0.07092941552400589,\n",
       "   -0.18885043263435364,\n",
       "   -0.16689428687095642,\n",
       "   -0.1423303186893463,\n",
       "   0.14373184740543365,\n",
       "   0.20127630233764648,\n",
       "   -0.05581728741526604,\n",
       "   0.03404294699430466,\n",
       "   -0.12135753035545349,\n",
       "   0.3132980763912201,\n",
       "   0.32362428307533264,\n",
       "   0.30161428451538086,\n",
       "   0.07194533199071884,\n",
       "   -0.14758726954460144,\n",
       "   -0.35291188955307007,\n",
       "   -0.12904073297977448,\n",
       "   0.22641095519065857,\n",
       "   0.1218223124742508,\n",
       "   -0.22765128314495087,\n",
       "   -0.04301592335104942,\n",
       "   0.14558720588684082,\n",
       "   -0.58607417345047,\n",
       "   -0.17569570243358612,\n",
       "   -0.0108492411673069,\n",
       "   -0.28004276752471924,\n",
       "   -0.3287932276725769,\n",
       "   -0.1969223916530609,\n",
       "   0.04764207825064659,\n",
       "   -0.11026997119188309,\n",
       "   0.08103185147047043,\n",
       "   4.8253607749938965,\n",
       "   -0.3243459165096283,\n",
       "   0.10100069642066956,\n",
       "   -0.05974049121141434,\n",
       "   -0.29431289434432983,\n",
       "   -0.1284821778535843,\n",
       "   0.1781741827726364,\n",
       "   0.03799816966056824,\n",
       "   0.7144267559051514,\n",
       "   0.06037353724241257,\n",
       "   -0.24456241726875305,\n",
       "   -0.24537256360054016,\n",
       "   0.040366820991039276,\n",
       "   0.0532848984003067,\n",
       "   -0.4414789080619812,\n",
       "   0.4826921224594116,\n",
       "   0.028515692800283432,\n",
       "   -0.11976705491542816,\n",
       "   0.033535584807395935,\n",
       "   -0.12164302170276642,\n",
       "   -0.04954229295253754,\n",
       "   -0.019580483436584473,\n",
       "   0.20781518518924713,\n",
       "   -0.18230170011520386,\n",
       "   0.12229596078395844,\n",
       "   0.025663740932941437,\n",
       "   0.0880201905965805,\n",
       "   0.09810703992843628,\n",
       "   -0.17344912886619568,\n",
       "   -0.25087010860443115,\n",
       "   -0.302311509847641,\n",
       "   0.15177948772907257,\n",
       "   0.22892187535762787,\n",
       "   0.24345573782920837,\n",
       "   0.10894361138343811,\n",
       "   -0.11363250017166138,\n",
       "   0.005332394503057003,\n",
       "   0.016617203131318092,\n",
       "   0.26598060131073,\n",
       "   0.09045692533254623,\n",
       "   0.3567601442337036,\n",
       "   0.13098540902137756,\n",
       "   0.9233293533325195,\n",
       "   -0.06542514264583588,\n",
       "   0.24041679501533508,\n",
       "   0.3053031861782074,\n",
       "   -0.13723720610141754,\n",
       "   -0.0303206667304039,\n",
       "   0.36350587010383606,\n",
       "   0.1491258144378662,\n",
       "   -0.2227964550256729,\n",
       "   0.22395601868629456,\n",
       "   0.13512277603149414,\n",
       "   -0.15118105709552765,\n",
       "   0.05975586175918579,\n",
       "   0.06742770969867706,\n",
       "   0.02008090727031231,\n",
       "   -0.14436708390712738,\n",
       "   -0.13655780255794525,\n",
       "   0.057090599089860916,\n",
       "   -0.029614724218845367,\n",
       "   -0.07548576593399048,\n",
       "   -0.12393034994602203,\n",
       "   -0.17754486203193665,\n",
       "   0.18264056742191315,\n",
       "   -0.25133830308914185,\n",
       "   0.2817426025867462,\n",
       "   -0.16884610056877136,\n",
       "   -0.2702639102935791,\n",
       "   -0.17168933153152466,\n",
       "   -0.08322738856077194,\n",
       "   0.010901665315032005,\n",
       "   -0.5502949953079224,\n",
       "   0.26077181100845337,\n",
       "   0.2789182662963867,\n",
       "   -0.014032192528247833,\n",
       "   -0.38232144713401794,\n",
       "   0.16681969165802002,\n",
       "   -0.06155284121632576,\n",
       "   -0.22987550497055054,\n",
       "   -0.11973466724157333,\n",
       "   -0.15649089217185974,\n",
       "   -0.020268019288778305,\n",
       "   -0.3056457042694092,\n",
       "   0.0513010248541832,\n",
       "   0.19486740231513977,\n",
       "   -0.2882367670536041,\n",
       "   -0.31249064207077026,\n",
       "   -0.030300501734018326,\n",
       "   0.29838114976882935,\n",
       "   0.10992006957530975,\n",
       "   -0.2398691475391388,\n",
       "   -0.04719960689544678,\n",
       "   -0.021207259967923164,\n",
       "   0.020983591675758362,\n",
       "   0.34967556595802307,\n",
       "   -0.07224923372268677,\n",
       "   -0.11469587683677673,\n",
       "   -0.0013625547289848328,\n",
       "   -0.0758686512708664,\n",
       "   0.39446619153022766,\n",
       "   0.18480414152145386,\n",
       "   0.015379868447780609,\n",
       "   -0.0046890899538993835,\n",
       "   0.14852634072303772,\n",
       "   -0.19586923718452454,\n",
       "   -0.11002101749181747,\n",
       "   0.2183452993631363,\n",
       "   -0.032351743429899216,\n",
       "   0.11129415780305862,\n",
       "   0.2836683988571167,\n",
       "   0.10111215710639954,\n",
       "   0.20407013595104218,\n",
       "   -0.1991395801305771,\n",
       "   -0.04581526666879654,\n",
       "   0.12864406406879425,\n",
       "   -0.009162254631519318,\n",
       "   -0.3210267424583435,\n",
       "   -7.354915618896484,\n",
       "   0.28148818016052246,\n",
       "   0.007648970931768417,\n",
       "   0.14334070682525635,\n",
       "   -0.2796061336994171,\n",
       "   -0.09527626633644104,\n",
       "   0.29021400213241577,\n",
       "   0.09965111315250397,\n",
       "   0.17775675654411316,\n",
       "   0.06827165186405182,\n",
       "   -0.29400724172592163,\n",
       "   -0.1337352842092514,\n",
       "   -1.7726619243621826,\n",
       "   0.08600635826587677,\n",
       "   -0.07312272489070892,\n",
       "   0.1307162493467331,\n",
       "   -0.12630951404571533,\n",
       "   -0.8496419191360474,\n",
       "   -0.012155460193753242,\n",
       "   0.3473057746887207,\n",
       "   -0.24470120668411255,\n",
       "   0.16649064421653748,\n",
       "   0.17103536427021027,\n",
       "   0.2513573467731476,\n",
       "   0.23906753957271576,\n",
       "   0.055686987936496735,\n",
       "   -0.02849050611257553,\n",
       "   -0.04394463822245598,\n",
       "   0.07371437549591064,\n",
       "   -0.08893963694572449,\n",
       "   0.024265389889478683,\n",
       "   -0.04721666872501373,\n",
       "   0.3958306610584259,\n",
       "   -0.06935440748929977,\n",
       "   0.0019107051193714142,\n",
       "   -0.2702770531177521,\n",
       "   -0.010834509506821632,\n",
       "   -0.14161312580108643,\n",
       "   0.20045194029808044,\n",
       "   -0.3327438533306122,\n",
       "   -0.09886796027421951,\n",
       "   -0.03103594109416008,\n",
       "   0.16586539149284363,\n",
       "   0.2994542121887207,\n",
       "   -0.36826062202453613,\n",
       "   -0.18471315503120422,\n",
       "   -0.4640103578567505,\n",
       "   -2.159057378768921,\n",
       "   0.12460516393184662,\n",
       "   0.11814296245574951,\n",
       "   0.31934934854507446,\n",
       "   0.1323239952325821,\n",
       "   0.016761960461735725,\n",
       "   -0.06380059570074081,\n",
       "   -0.1537935435771942,\n",
       "   0.18390698730945587,\n",
       "   -0.009505858644843102,\n",
       "   0.09738513082265854,\n",
       "   0.2518868148326874,\n",
       "   -0.20578761398792267,\n",
       "   -0.21482306718826294,\n",
       "   -0.016515273600816727,\n",
       "   0.5897840261459351,\n",
       "   0.05243922024965286,\n",
       "   -0.22500231862068176,\n",
       "   0.301453173160553,\n",
       "   -0.30591705441474915,\n",
       "   -0.0562112033367157,\n",
       "   -0.19069895148277283,\n",
       "   -0.38098037242889404,\n",
       "   -0.12730616331100464,\n",
       "   -0.013144254684448242,\n",
       "   -0.11826541274785995,\n",
       "   0.2174551635980606,\n",
       "   0.24585264921188354,\n",
       "   0.20594339072704315,\n",
       "   -0.13156166672706604,\n",
       "   0.03846143186092377,\n",
       "   -0.21717897057533264,\n",
       "   -0.019957736134529114,\n",
       "   -0.3735863268375397,\n",
       "   -0.13257254660129547,\n",
       "   0.5215296745300293,\n",
       "   0.23171018064022064,\n",
       "   0.21484820544719696,\n",
       "   -0.20708854496479034,\n",
       "   -0.032680071890354156,\n",
       "   0.2988465428352356,\n",
       "   -0.0630921721458435,\n",
       "   -0.04420744255185127,\n",
       "   -0.11364057660102844,\n",
       "   0.005607351660728455,\n",
       "   0.07338641583919525,\n",
       "   -0.07835127413272858,\n",
       "   -0.18605510890483856,\n",
       "   0.1959027647972107,\n",
       "   -0.2328101396560669,\n",
       "   0.07054173946380615,\n",
       "   -0.24805191159248352,\n",
       "   -0.08754774183034897,\n",
       "   -0.1888904571533203,\n",
       "   -7.17751681804657e-05,\n",
       "   0.043953634798526764,\n",
       "   -0.37124672532081604,\n",
       "   0.052450813353061676,\n",
       "   -0.18718105554580688,\n",
       "   2.41194224357605,\n",
       "   0.041348837316036224,\n",
       "   -0.007798179984092712,\n",
       "   -0.05522521585226059,\n",
       "   -0.03203314170241356,\n",
       "   0.2589864134788513,\n",
       "   -0.02621312066912651,\n",
       "   0.2268058806657791,\n",
       "   1.4288947582244873,\n",
       "   0.09991723299026489,\n",
       "   -0.21881447732448578,\n",
       "   -0.03430645167827606,\n",
       "   -0.01179710403084755,\n",
       "   -0.06754478812217712,\n",
       "   -0.027797138318419456,\n",
       "   0.331086665391922,\n",
       "   -0.09177827090024948,\n",
       "   0.1320868879556656,\n",
       "   0.06792183220386505,\n",
       "   -0.030220234766602516,\n",
       "   0.25781431794166565,\n",
       "   -0.11922534555196762,\n",
       "   0.20231759548187256,\n",
       "   -0.040757033973932266,\n",
       "   -0.04223082959651947,\n",
       "   -0.06886104494333267,\n",
       "   -0.2922442853450775,\n",
       "   -0.23770855367183685,\n",
       "   0.29817646741867065,\n",
       "   -0.3711101710796356,\n",
       "   0.13910293579101562,\n",
       "   -0.000558905303478241,\n",
       "   -0.23229824006557465,\n",
       "   0.9053875803947449,\n",
       "   -0.13803398609161377,\n",
       "   0.1583109200000763,\n",
       "   0.27124202251434326,\n",
       "   -0.13938617706298828,\n",
       "   0.08495011925697327,\n",
       "   -0.4206404387950897,\n",
       "   -0.06801250576972961,\n",
       "   -0.030804309993982315,\n",
       "   -0.44098350405693054,\n",
       "   0.4754045903682709,\n",
       "   -0.07190141081809998,\n",
       "   0.039375290274620056,\n",
       "   -0.036771707236766815,\n",
       "   -0.20400375127792358,\n",
       "   0.15076124668121338,\n",
       "   -0.08466313034296036,\n",
       "   -0.0037454068660736084,\n",
       "   -0.23786525428295135,\n",
       "   -0.2641666829586029,\n",
       "   0.2623317837715149,\n",
       "   -0.2391541451215744,\n",
       "   0.07545838505029678,\n",
       "   -0.591170072555542,\n",
       "   -0.08598653972148895,\n",
       "   0.14061518013477325,\n",
       "   -0.08961527794599533,\n",
       "   -0.04917103052139282,\n",
       "   0.0114069152623415,\n",
       "   -0.011288337409496307,\n",
       "   0.051780134439468384,\n",
       "   -0.04352915659546852,\n",
       "   0.10861755162477493,\n",
       "   0.14760957658290863,\n",
       "   -0.1458081603050232,\n",
       "   -0.11427368968725204,\n",
       "   -0.2650631368160248,\n",
       "   -0.08526585251092911,\n",
       "   0.1680574119091034,\n",
       "   -2.128776788711548,\n",
       "   -0.12182040512561798,\n",
       "   0.13027776777744293,\n",
       "   -0.2915591597557068,\n",
       "   -0.2633644938468933,\n",
       "   -0.2972119152545929,\n",
       "   0.28921276330947876,\n",
       "   0.17075973749160767,\n",
       "   0.1703546643257141,\n",
       "   0.35742098093032837,\n",
       "   -0.08114869892597198,\n",
       "   1.8210911750793457,\n",
       "   -0.1042339876294136,\n",
       "   -0.03336142376065254,\n",
       "   -0.08374714106321335,\n",
       "   -0.01773170568048954,\n",
       "   0.18137237429618835,\n",
       "   0.16613127291202545,\n",
       "   -0.12657351791858673,\n",
       "   -0.4464450478553772,\n",
       "   -0.09487183392047882,\n",
       "   1.5146446228027344,\n",
       "   0.14150163531303406,\n",
       "   0.3442590832710266,\n",
       "   0.2715274393558502,\n",
       "   -0.20204031467437744,\n",
       "   -0.05952534079551697,\n",
       "   0.10047198086977005,\n",
       "   0.18689686059951782,\n",
       "   -0.009166646748781204,\n",
       "   -0.18844278156757355,\n",
       "   0.29771703481674194,\n",
       "   0.015029298141598701],\n",
       "  [-0.00015303120017051697,\n",
       "   0.0801270455121994,\n",
       "   -0.38847216963768005,\n",
       "   0.28976351022720337,\n",
       "   0.10035377740859985,\n",
       "   -0.11797253042459488,\n",
       "   0.26589876413345337,\n",
       "   -0.30863243341445923,\n",
       "   0.11265432089567184,\n",
       "   0.44062694907188416,\n",
       "   0.03702718764543533,\n",
       "   0.4588070511817932,\n",
       "   0.2659416198730469,\n",
       "   0.09318415820598602,\n",
       "   -0.31936782598495483,\n",
       "   -0.011046946980059147,\n",
       "   -0.11461184173822403,\n",
       "   0.06018763780593872,\n",
       "   0.03040596842765808,\n",
       "   0.031369730830192566,\n",
       "   -0.10687693953514099,\n",
       "   -0.07048780471086502,\n",
       "   0.017662283033132553,\n",
       "   0.15770223736763,\n",
       "   -0.3637043833732605,\n",
       "   -0.06969480961561203,\n",
       "   0.41403457522392273,\n",
       "   0.01698434352874756,\n",
       "   -0.02011209726333618,\n",
       "   0.25843337178230286,\n",
       "   -0.05424089729785919,\n",
       "   0.11880530416965485,\n",
       "   0.3490239679813385,\n",
       "   0.2729916572570801,\n",
       "   -0.12180683761835098,\n",
       "   0.24036921560764313,\n",
       "   0.023433707654476166,\n",
       "   0.649057149887085,\n",
       "   -0.1201815977692604,\n",
       "   0.05010783672332764,\n",
       "   0.07274441421031952,\n",
       "   0.09980972856283188,\n",
       "   0.02675240859389305,\n",
       "   -0.26077592372894287,\n",
       "   0.2671689987182617,\n",
       "   0.10708984732627869,\n",
       "   -0.15078571438789368,\n",
       "   0.15027467906475067,\n",
       "   0.16827842593193054,\n",
       "   -0.2969585359096527,\n",
       "   0.15839584171772003,\n",
       "   -0.18971025943756104,\n",
       "   -0.11853621155023575,\n",
       "   -0.11579308658838272,\n",
       "   -0.1717551201581955,\n",
       "   -0.15409882366657257,\n",
       "   0.15222027897834778,\n",
       "   0.026889141649007797,\n",
       "   -0.36224910616874695,\n",
       "   0.6280122995376587,\n",
       "   -0.5350469946861267,\n",
       "   0.08478540182113647,\n",
       "   0.1890597641468048,\n",
       "   -0.029970992356538773,\n",
       "   -0.1873568594455719,\n",
       "   0.22901201248168945,\n",
       "   -0.07461301237344742,\n",
       "   -0.011661369353532791,\n",
       "   0.1249813660979271,\n",
       "   -0.2056174874305725,\n",
       "   0.4230864942073822,\n",
       "   -0.44935929775238037,\n",
       "   -0.2351672351360321,\n",
       "   0.01431262120604515,\n",
       "   -0.23712781071662903,\n",
       "   -0.30790185928344727,\n",
       "   0.06486808508634567,\n",
       "   -0.1239602193236351,\n",
       "   0.12174259126186371,\n",
       "   -0.2739607095718384,\n",
       "   -0.0885036289691925,\n",
       "   0.37552323937416077,\n",
       "   0.014344200491905212,\n",
       "   -0.2604106664657593,\n",
       "   0.028975270688533783,\n",
       "   0.16294504702091217,\n",
       "   0.23932935297489166,\n",
       "   0.10503636300563812,\n",
       "   -0.27573901414871216,\n",
       "   0.2669031620025635,\n",
       "   -0.12328869104385376,\n",
       "   -0.22090266644954681,\n",
       "   -0.25630831718444824,\n",
       "   -0.16973033547401428,\n",
       "   -0.201066255569458,\n",
       "   0.10432000458240509,\n",
       "   -0.18833047151565552,\n",
       "   0.10760004073381424,\n",
       "   0.2190166562795639,\n",
       "   0.22199560701847076,\n",
       "   0.01820044405758381,\n",
       "   0.13149139285087585,\n",
       "   -0.5911523699760437,\n",
       "   -0.0385390967130661,\n",
       "   -0.1977068930864334,\n",
       "   0.16444148123264313,\n",
       "   0.20242057740688324,\n",
       "   -0.16990035772323608,\n",
       "   -0.05678623542189598,\n",
       "   0.6623991131782532,\n",
       "   0.2702387571334839,\n",
       "   -0.12408789992332458,\n",
       "   0.1522267460823059,\n",
       "   -0.25402772426605225,\n",
       "   -0.31624969840049744,\n",
       "   0.4545539617538452,\n",
       "   0.17869506776332855,\n",
       "   -0.0003685280680656433,\n",
       "   0.15513581037521362,\n",
       "   0.15764020383358002,\n",
       "   0.13469640910625458,\n",
       "   -0.06942051649093628,\n",
       "   0.16245593130588531,\n",
       "   -0.1443331092596054,\n",
       "   0.15981891751289368,\n",
       "   -0.07638797909021378,\n",
       "   0.13210713863372803,\n",
       "   -0.17590847611427307,\n",
       "   0.35162562131881714,\n",
       "   0.23922227323055267,\n",
       "   -0.3323217034339905,\n",
       "   0.29024335741996765,\n",
       "   0.22360791265964508,\n",
       "   -0.39458757638931274,\n",
       "   0.06739290058612823,\n",
       "   0.1641377955675125,\n",
       "   -0.10280169546604156,\n",
       "   0.4057442247867584,\n",
       "   -0.4385487139225006,\n",
       "   0.09518934786319733,\n",
       "   0.02076045423746109,\n",
       "   0.07459357380867004,\n",
       "   0.33550626039505005,\n",
       "   0.10091517865657806,\n",
       "   -0.10441379249095917,\n",
       "   0.1840467005968094,\n",
       "   -0.6618698239326477,\n",
       "   -0.3159269690513611,\n",
       "   -0.17223934829235077,\n",
       "   0.23729684948921204,\n",
       "   -0.25026220083236694,\n",
       "   -0.017625756561756134,\n",
       "   -0.0839921236038208,\n",
       "   0.1014840304851532,\n",
       "   -0.34183767437934875,\n",
       "   -0.10066722333431244,\n",
       "   -0.012531109154224396,\n",
       "   0.2818208634853363,\n",
       "   0.36611536145210266,\n",
       "   0.09728764742612839,\n",
       "   -0.2708801329135895,\n",
       "   -0.043160296976566315,\n",
       "   -0.5030595660209656,\n",
       "   0.07583259791135788,\n",
       "   -0.19294419884681702,\n",
       "   0.07756465673446655,\n",
       "   -0.37598928809165955,\n",
       "   0.19966362416744232,\n",
       "   -0.3178032636642456,\n",
       "   0.25058066844940186,\n",
       "   0.03841865435242653,\n",
       "   -0.04967179149389267,\n",
       "   -0.10398860275745392,\n",
       "   0.47289758920669556,\n",
       "   -0.3985273241996765,\n",
       "   -0.32598182559013367,\n",
       "   -0.11022534221410751,\n",
       "   -0.1136653795838356,\n",
       "   -0.33428364992141724,\n",
       "   -0.3177691102027893,\n",
       "   0.14207720756530762,\n",
       "   -0.1109209656715393,\n",
       "   0.13650624454021454,\n",
       "   -0.06089147925376892,\n",
       "   0.004765018820762634,\n",
       "   -0.17864704132080078,\n",
       "   0.17380595207214355,\n",
       "   -0.33048373460769653,\n",
       "   -0.16187554597854614,\n",
       "   0.13877421617507935,\n",
       "   0.2660761773586273,\n",
       "   -0.23538295924663544,\n",
       "   -0.5548015236854553,\n",
       "   0.3508426547050476,\n",
       "   -0.012734148651361465,\n",
       "   -0.0018338710069656372,\n",
       "   -0.007570108398795128,\n",
       "   0.20163071155548096,\n",
       "   0.08954043686389923,\n",
       "   0.15182499587535858,\n",
       "   -0.4681403934955597,\n",
       "   0.1518930196762085,\n",
       "   -0.10995202511548996,\n",
       "   0.08503083884716034,\n",
       "   -0.033146243542432785,\n",
       "   0.03849799931049347,\n",
       "   0.42788729071617126,\n",
       "   -0.09630013257265091,\n",
       "   -0.2791632115840912,\n",
       "   -0.12436126172542572,\n",
       "   -0.2936546504497528,\n",
       "   -0.343718022108078,\n",
       "   -0.1881519854068756,\n",
       "   0.24043631553649902,\n",
       "   -0.045665040612220764,\n",
       "   -0.20900607109069824,\n",
       "   -0.18979737162590027,\n",
       "   -0.2116726040840149,\n",
       "   -0.22652913630008698,\n",
       "   0.2818281650543213,\n",
       "   0.49571576714515686,\n",
       "   -0.14773212373256683,\n",
       "   0.12212107330560684,\n",
       "   0.0011846758425235748,\n",
       "   -0.00546303391456604,\n",
       "   -0.17044612765312195,\n",
       "   -0.051546696573495865,\n",
       "   0.23919659852981567,\n",
       "   0.1093883067369461,\n",
       "   -0.09988339990377426,\n",
       "   -0.16368813812732697,\n",
       "   0.3197430968284607,\n",
       "   0.35496020317077637,\n",
       "   -0.1381354182958603,\n",
       "   0.12288512289524078,\n",
       "   0.042640477418899536,\n",
       "   0.13763147592544556,\n",
       "   -0.14395958185195923,\n",
       "   -0.014802604913711548,\n",
       "   -0.21556246280670166,\n",
       "   -0.10527370125055313,\n",
       "   0.05362613499164581,\n",
       "   0.27002328634262085,\n",
       "   0.3578130602836609,\n",
       "   -0.16150979697704315,\n",
       "   -0.01887081190943718,\n",
       "   -0.08956141024827957,\n",
       "   0.21524286270141602,\n",
       "   0.28005802631378174,\n",
       "   -0.025290220975875854,\n",
       "   0.1204889565706253,\n",
       "   -0.1071687638759613,\n",
       "   -0.06003781408071518,\n",
       "   0.025192998349666595,\n",
       "   0.28607892990112305,\n",
       "   0.23131898045539856,\n",
       "   0.23700477182865143,\n",
       "   0.1806296408176422,\n",
       "   -0.0879603773355484,\n",
       "   -0.6042122840881348,\n",
       "   0.03309284895658493,\n",
       "   0.01919243484735489,\n",
       "   -0.0995342805981636,\n",
       "   0.13113057613372803,\n",
       "   -0.1855754256248474,\n",
       "   0.006712915375828743,\n",
       "   -0.11718913912773132,\n",
       "   -0.054636627435684204,\n",
       "   -0.4797152280807495,\n",
       "   -0.007000049576163292,\n",
       "   0.05780725181102753,\n",
       "   -0.1587860882282257,\n",
       "   -0.09185121208429337,\n",
       "   0.4564586281776428,\n",
       "   -0.3218238353729248,\n",
       "   0.24412408471107483,\n",
       "   -0.01257413625717163,\n",
       "   -0.0922946035861969,\n",
       "   0.10339882969856262,\n",
       "   0.0559966079890728,\n",
       "   0.06368451565504074,\n",
       "   0.14838476479053497,\n",
       "   0.0699344277381897,\n",
       "   0.5977004766464233,\n",
       "   0.04449949413537979,\n",
       "   0.256722629070282,\n",
       "   0.20496831834316254,\n",
       "   0.017163164913654327,\n",
       "   -0.06512810289859772,\n",
       "   0.029149174690246582,\n",
       "   -0.033197496086359024,\n",
       "   -0.20121806859970093,\n",
       "   -0.056524988263845444,\n",
       "   0.09818089008331299,\n",
       "   -0.295350581407547,\n",
       "   -0.11224684119224548,\n",
       "   -0.44622066617012024,\n",
       "   0.42616504430770874,\n",
       "   -0.41575661301612854,\n",
       "   -0.274789035320282,\n",
       "   0.03417012095451355,\n",
       "   0.05346103012561798,\n",
       "   0.0030582547187805176,\n",
       "   -0.08163157105445862,\n",
       "   -0.2761994004249573,\n",
       "   0.04011745750904083,\n",
       "   0.07726018130779266,\n",
       "   0.3432427942752838,\n",
       "   0.2671560049057007,\n",
       "   0.2691323161125183,\n",
       "   0.3970051407814026,\n",
       "   0.08045625686645508,\n",
       "   0.1281755268573761,\n",
       "   -0.2731298804283142,\n",
       "   -0.07787739485502243,\n",
       "   -0.4939488172531128,\n",
       "   -0.261795312166214,\n",
       "   -0.45987287163734436,\n",
       "   -0.37602823972702026,\n",
       "   -0.2862061560153961,\n",
       "   0.3783940374851227,\n",
       "   0.10633861273527145,\n",
       "   -0.1531219780445099,\n",
       "   0.17783978581428528,\n",
       "   -0.05095323175191879,\n",
       "   -0.044771842658519745,\n",
       "   -0.1803148090839386,\n",
       "   0.38773661851882935,\n",
       "   -0.04976796358823776,\n",
       "   0.029177909716963768,\n",
       "   0.05523168295621872,\n",
       "   -0.07215332984924316,\n",
       "   -0.25619369745254517,\n",
       "   0.05175478011369705,\n",
       "   -0.6389036178588867,\n",
       "   -0.12020554393529892,\n",
       "   -0.42154601216316223,\n",
       "   0.2870051860809326,\n",
       "   -0.016775846481323242,\n",
       "   0.4312609136104584,\n",
       "   0.12270773202180862,\n",
       "   -0.052671127021312714,\n",
       "   0.2948864698410034,\n",
       "   -0.13097336888313293,\n",
       "   0.0009659919887781143,\n",
       "   0.12286768853664398,\n",
       "   0.1494603157043457,\n",
       "   0.16690942645072937,\n",
       "   -0.05604266747832298,\n",
       "   0.32926514744758606,\n",
       "   0.016555912792682648,\n",
       "   0.20510393381118774,\n",
       "   -0.15346160531044006,\n",
       "   -0.13668027520179749,\n",
       "   -0.3815595805644989,\n",
       "   0.09475284069776535,\n",
       "   0.09593720734119415,\n",
       "   -0.09257322549819946,\n",
       "   -0.2515582740306854,\n",
       "   0.5767683982849121,\n",
       "   0.18879221379756927,\n",
       "   0.22871609032154083,\n",
       "   0.09541809558868408,\n",
       "   0.26925626397132874,\n",
       "   0.09543073177337646,\n",
       "   0.26321589946746826,\n",
       "   -0.13490749895572662,\n",
       "   0.4118644595146179,\n",
       "   0.16759943962097168,\n",
       "   -0.09127163887023926,\n",
       "   0.6363312602043152,\n",
       "   0.018297694623470306,\n",
       "   0.2899838089942932,\n",
       "   -0.44282692670822144,\n",
       "   -0.18466955423355103,\n",
       "   -0.17261792719364166,\n",
       "   -0.0045248353853821754,\n",
       "   0.0728541761636734,\n",
       "   0.1767006367444992,\n",
       "   -0.3789079189300537,\n",
       "   -0.08752322196960449,\n",
       "   0.07014001160860062,\n",
       "   0.6047354936599731,\n",
       "   -0.08484749495983124,\n",
       "   -0.11216239631175995,\n",
       "   -0.4263542890548706,\n",
       "   0.24071478843688965,\n",
       "   -0.21940714120864868,\n",
       "   0.13784611225128174,\n",
       "   0.10833136737346649,\n",
       "   -0.3378259241580963,\n",
       "   -0.0493781715631485,\n",
       "   0.26000547409057617,\n",
       "   -0.009205842390656471,\n",
       "   -0.09147574752569199,\n",
       "   -0.563521683216095,\n",
       "   0.1000271588563919,\n",
       "   0.39462053775787354,\n",
       "   -0.041023463010787964,\n",
       "   -0.4200049042701721,\n",
       "   -0.1276375651359558,\n",
       "   0.274468332529068,\n",
       "   0.078233502805233,\n",
       "   0.2829035818576813,\n",
       "   -0.272930383682251,\n",
       "   -0.5816152691841125,\n",
       "   0.6044448018074036,\n",
       "   0.09110800921916962,\n",
       "   0.410942405462265,\n",
       "   -0.34195590019226074,\n",
       "   0.10724348574876785,\n",
       "   -0.35463404655456543,\n",
       "   -0.09115094691514969,\n",
       "   -0.020411625504493713,\n",
       "   -0.05969979986548424,\n",
       "   -0.27264755964279175,\n",
       "   0.1954636573791504,\n",
       "   0.03151868283748627,\n",
       "   0.12798847258090973,\n",
       "   -0.003504786640405655,\n",
       "   0.18124251067638397,\n",
       "   0.20460215210914612,\n",
       "   -0.10051041096448898,\n",
       "   -0.4236728549003601,\n",
       "   0.1303880661725998,\n",
       "   -0.14669537544250488,\n",
       "   0.20964224636554718,\n",
       "   -0.06844843178987503,\n",
       "   -0.2637392282485962,\n",
       "   0.023391883820295334,\n",
       "   -0.32233625650405884,\n",
       "   0.025356128811836243,\n",
       "   0.0760735347867012,\n",
       "   0.05684947595000267,\n",
       "   -0.5167760848999023,\n",
       "   0.3545914590358734,\n",
       "   -0.051729217171669006,\n",
       "   0.06382808089256287,\n",
       "   0.1427939087152481,\n",
       "   -0.0033902153372764587,\n",
       "   0.124369777739048,\n",
       "   -0.15281574428081512,\n",
       "   -0.16003131866455078,\n",
       "   -0.2587355971336365,\n",
       "   0.07290741801261902,\n",
       "   -0.30474406480789185,\n",
       "   0.15506581962108612,\n",
       "   0.16690289974212646,\n",
       "   0.20128053426742554,\n",
       "   -0.07304376363754272,\n",
       "   0.09384970366954803,\n",
       "   -0.22091573476791382,\n",
       "   0.06544254720211029,\n",
       "   -0.40450674295425415,\n",
       "   0.32622677087783813,\n",
       "   0.2813291549682617,\n",
       "   -0.3125206232070923,\n",
       "   -0.13841789960861206,\n",
       "   -0.1394723653793335,\n",
       "   -0.3387356102466583,\n",
       "   -0.07362586259841919,\n",
       "   0.16972580552101135,\n",
       "   -0.06862639635801315,\n",
       "   -0.17628581821918488,\n",
       "   -0.21169507503509521,\n",
       "   0.1876138150691986,\n",
       "   0.2377990186214447,\n",
       "   0.3058033287525177,\n",
       "   -0.5417349338531494,\n",
       "   -0.19614122807979584,\n",
       "   0.08289582282304764,\n",
       "   0.1368594914674759,\n",
       "   -0.09357719123363495,\n",
       "   0.4364911615848541,\n",
       "   -0.12487800419330597,\n",
       "   0.2895357608795166,\n",
       "   -0.02241101861000061,\n",
       "   0.008828173391520977,\n",
       "   -0.4945322573184967,\n",
       "   0.1552850753068924,\n",
       "   -0.057118985801935196,\n",
       "   0.14097507297992706,\n",
       "   -0.05202460289001465,\n",
       "   0.09517751634120941,\n",
       "   0.550431489944458,\n",
       "   -0.41379639506340027,\n",
       "   0.3002834916114807,\n",
       "   0.12164189666509628,\n",
       "   0.5905409455299377,\n",
       "   0.019207146018743515,\n",
       "   0.136081263422966,\n",
       "   -0.3568136990070343,\n",
       "   0.04468976706266403,\n",
       "   -0.20468345284461975,\n",
       "   0.0206389669328928,\n",
       "   -0.13248148560523987,\n",
       "   -0.38487982749938965,\n",
       "   -0.3477114737033844,\n",
       "   0.017020203173160553,\n",
       "   0.07902881503105164,\n",
       "   -0.07676653563976288,\n",
       "   -0.030579164624214172,\n",
       "   -0.16025793552398682,\n",
       "   0.04591894522309303,\n",
       "   -0.4216354489326477,\n",
       "   0.41087573766708374,\n",
       "   0.08581790328025818,\n",
       "   -0.06589177250862122,\n",
       "   0.04895954951643944,\n",
       "   -0.061272308230400085,\n",
       "   0.10846813768148422,\n",
       "   -0.2658384144306183,\n",
       "   0.3738951086997986,\n",
       "   -0.054181795567274094,\n",
       "   0.23127979040145874,\n",
       "   -0.5158988237380981,\n",
       "   0.18100130558013916,\n",
       "   -0.05507638677954674,\n",
       "   -0.0487695038318634,\n",
       "   -0.1360994577407837,\n",
       "   0.20625191926956177,\n",
       "   0.2940974831581116,\n",
       "   -0.2825430929660797,\n",
       "   -0.08931496739387512,\n",
       "   0.12599167227745056,\n",
       "   0.037133459001779556,\n",
       "   -0.004584666341543198,\n",
       "   0.19247153401374817,\n",
       "   0.21793194115161896,\n",
       "   0.13368791341781616,\n",
       "   -0.2948600947856903,\n",
       "   0.021680857986211777,\n",
       "   -0.3488118648529053,\n",
       "   -0.16656376421451569,\n",
       "   0.753606915473938,\n",
       "   -0.04590819031000137,\n",
       "   -0.10698394477367401,\n",
       "   0.044153742492198944,\n",
       "   -0.14341232180595398,\n",
       "   0.3505244553089142,\n",
       "   0.30773475766181946,\n",
       "   0.07333317399024963,\n",
       "   -0.2863321006298065,\n",
       "   0.27741408348083496,\n",
       "   -0.1514962911605835,\n",
       "   0.011389344930648804,\n",
       "   0.19610393047332764,\n",
       "   0.02038375660777092,\n",
       "   0.15385377407073975,\n",
       "   0.24062399566173553,\n",
       "   0.32627028226852417,\n",
       "   0.1983087956905365,\n",
       "   -0.5969808101654053,\n",
       "   0.15447549521923065,\n",
       "   0.24537725746631622,\n",
       "   -0.056402385234832764,\n",
       "   -0.1017841100692749,\n",
       "   -9.280998229980469,\n",
       "   -0.00821223296225071,\n",
       "   -0.22246047854423523,\n",
       "   0.2825571596622467,\n",
       "   -0.1357842981815338,\n",
       "   0.02255374938249588,\n",
       "   -0.05895960330963135,\n",
       "   0.08270826935768127,\n",
       "   -0.3490270972251892,\n",
       "   0.141569584608078,\n",
       "   0.05112476274371147,\n",
       "   0.037003569304943085,\n",
       "   0.5797572731971741,\n",
       "   -0.4449830949306488,\n",
       "   -0.07033449411392212,\n",
       "   -0.29665976762771606,\n",
       "   0.09307102113962173,\n",
       "   -0.25031667947769165,\n",
       "   0.04351339489221573,\n",
       "   0.05314555764198303,\n",
       "   0.04293996840715408,\n",
       "   0.10169795900583267,\n",
       "   -0.005132004618644714,\n",
       "   0.2425527423620224,\n",
       "   0.4041634798049927,\n",
       "   0.28174304962158203,\n",
       "   -0.36682549118995667,\n",
       "   -0.30723628401756287,\n",
       "   0.3987463414669037,\n",
       "   -0.26636144518852234,\n",
       "   -0.14362144470214844,\n",
       "   0.32195043563842773,\n",
       "   -0.21467718482017517,\n",
       "   -0.012919915840029716,\n",
       "   0.5197741985321045,\n",
       "   -0.05032996088266373,\n",
       "   0.1904941350221634,\n",
       "   -0.013730961829423904,\n",
       "   0.02195516601204872,\n",
       "   -0.38185006380081177,\n",
       "   -0.4925754964351654,\n",
       "   -0.08942121267318726,\n",
       "   -0.34651678800582886,\n",
       "   0.25916245579719543,\n",
       "   -0.06432119011878967,\n",
       "   0.15702354907989502,\n",
       "   0.10262428224086761,\n",
       "   0.26395556330680847,\n",
       "   -0.10086312890052795,\n",
       "   -0.05177575722336769,\n",
       "   0.12811249494552612,\n",
       "   0.1597449779510498,\n",
       "   0.36122170090675354,\n",
       "   -0.045796945691108704,\n",
       "   0.15572424232959747,\n",
       "   0.14501717686653137,\n",
       "   0.06949779391288757,\n",
       "   0.42532145977020264,\n",
       "   -0.19818885624408722,\n",
       "   -0.07091161608695984,\n",
       "   0.08120536804199219,\n",
       "   -0.30696481466293335,\n",
       "   0.5813222527503967,\n",
       "   -0.3947487473487854,\n",
       "   -0.26802462339401245,\n",
       "   0.13773828744888306,\n",
       "   0.2123442441225052,\n",
       "   0.07204271852970123,\n",
       "   0.17776153981685638,\n",
       "   -0.31464526057243347,\n",
       "   -0.22207893431186676,\n",
       "   0.3809051215648651,\n",
       "   -0.1651836335659027,\n",
       "   -0.1040218397974968,\n",
       "   -0.14159266650676727,\n",
       "   0.0006809830665588379,\n",
       "   -0.03485634922981262,\n",
       "   0.03225497156381607,\n",
       "   0.12432137131690979,\n",
       "   0.03602937236428261,\n",
       "   -0.09786778688430786,\n",
       "   0.13107219338417053,\n",
       "   0.7350969314575195,\n",
       "   -0.12630510330200195,\n",
       "   -0.04537195712327957,\n",
       "   0.07449889183044434,\n",
       "   0.028340429067611694,\n",
       "   0.44093137979507446,\n",
       "   0.04460552707314491,\n",
       "   0.14423201978206635,\n",
       "   0.007860681042075157,\n",
       "   -0.06530711054801941,\n",
       "   0.08805728703737259,\n",
       "   0.1522849202156067,\n",
       "   0.14748650789260864,\n",
       "   0.33132749795913696,\n",
       "   -0.21360798180103302,\n",
       "   0.3747905492782593,\n",
       "   0.030666597187519073,\n",
       "   0.349523663520813,\n",
       "   -0.00016949325799942017,\n",
       "   0.052258916199207306,\n",
       "   0.167293518781662,\n",
       "   -0.3643808662891388,\n",
       "   0.2641407549381256,\n",
       "   -0.16352207958698273,\n",
       "   0.20189130306243896,\n",
       "   -0.21488457918167114,\n",
       "   0.29292672872543335,\n",
       "   -0.020521122962236404,\n",
       "   0.0013361675664782524,\n",
       "   0.3619029223918915,\n",
       "   0.2978453040122986,\n",
       "   -0.08858578652143478,\n",
       "   0.2985840439796448,\n",
       "   0.15602803230285645,\n",
       "   -0.055181801319122314,\n",
       "   -0.10142093896865845,\n",
       "   0.45412081480026245,\n",
       "   -0.12852635979652405,\n",
       "   -0.01293284259736538,\n",
       "   -0.49733054637908936,\n",
       "   -0.15247982740402222,\n",
       "   -0.20644332468509674,\n",
       "   0.3394472002983093,\n",
       "   0.13662831485271454,\n",
       "   -0.3632943630218506,\n",
       "   0.013815504498779774,\n",
       "   0.06824734061956406,\n",
       "   -0.15402863919734955,\n",
       "   0.3162427246570587,\n",
       "   0.4128345251083374,\n",
       "   -0.4770159423351288,\n",
       "   0.45475512742996216,\n",
       "   0.19208867847919464,\n",
       "   -0.19714723527431488,\n",
       "   -0.1819487065076828,\n",
       "   0.3014407455921173,\n",
       "   -0.6145211458206177,\n",
       "   0.20278556644916534,\n",
       "   0.017937354743480682,\n",
       "   -0.1407044678926468,\n",
       "   0.18681973218917847,\n",
       "   -0.39693060517311096,\n",
       "   0.536551296710968,\n",
       "   -0.20806676149368286,\n",
       "   -0.3524590730667114,\n",
       "   -0.2564416825771332,\n",
       "   -0.06417785584926605,\n",
       "   -0.03768645226955414,\n",
       "   -0.09187235683202744,\n",
       "   0.4709033966064453,\n",
       "   0.4521094262599945,\n",
       "   -0.011662334203720093,\n",
       "   0.30871933698654175,\n",
       "   -0.05044431984424591,\n",
       "   0.14938965439796448,\n",
       "   -0.08899816870689392,\n",
       "   -0.087129607796669,\n",
       "   0.30498185753822327,\n",
       "   -0.2803284227848053,\n",
       "   0.13587385416030884,\n",
       "   0.22019073367118835,\n",
       "   -0.2669580578804016,\n",
       "   -0.037611961364746094,\n",
       "   -0.3200797438621521,\n",
       "   -0.08830084651708603,\n",
       "   -0.10875900834798813,\n",
       "   0.16118434071540833,\n",
       "   -0.24312560260295868,\n",
       "   -0.2122786045074463,\n",
       "   0.1421068012714386,\n",
       "   -0.05903184413909912,\n",
       "   0.22269608080387115,\n",
       "   -0.11649788916110992,\n",
       "   0.395491361618042,\n",
       "   -0.36930665373802185,\n",
       "   -0.20106154680252075,\n",
       "   -0.06846897304058075,\n",
       "   0.28492432832717896,\n",
       "   0.25130119919776917,\n",
       "   0.10105599462985992,\n",
       "   -0.26584017276763916,\n",
       "   -0.42318323254585266,\n",
       "   0.1640443503856659,\n",
       "   0.26750463247299194,\n",
       "   -0.10294127464294434,\n",
       "   0.22136901319026947,\n",
       "   -0.002261711284518242,\n",
       "   0.1812385618686676,\n",
       "   -0.11931973695755005,\n",
       "   0.0009458065032958984,\n",
       "   0.1542034149169922,\n",
       "   0.2142222672700882,\n",
       "   0.4453597664833069,\n",
       "   0.05038197338581085,\n",
       "   -0.021729137748479843,\n",
       "   -0.07024797052145004,\n",
       "   -0.05593535676598549,\n",
       "   -0.12208319455385208,\n",
       "   0.042336635291576385,\n",
       "   0.16644376516342163,\n",
       "   -0.020931651815772057,\n",
       "   -0.019061703234910965,\n",
       "   0.038001611828804016,\n",
       "   0.11045018583536148,\n",
       "   -0.039673227816820145,\n",
       "   0.6590478420257568,\n",
       "   0.19384026527404785,\n",
       "   0.1713157296180725,\n",
       "   -0.008038848638534546],\n",
       "  [-0.05362832918763161,\n",
       "   -0.205461785197258,\n",
       "   -0.08306892216205597,\n",
       "   0.05380932614207268,\n",
       "   -0.007553172297775745,\n",
       "   -0.5140759944915771,\n",
       "   0.21959692239761353,\n",
       "   -0.020551301538944244,\n",
       "   0.0942186638712883,\n",
       "   0.11344600468873978,\n",
       "   0.2718883752822876,\n",
       "   0.14522041380405426,\n",
       "   -0.4942181408405304,\n",
       "   0.3581475615501404,\n",
       "   -0.5362510681152344,\n",
       "   -0.14989088475704193,\n",
       "   -0.3637552559375763,\n",
       "   -0.2375938892364502,\n",
       "   -0.01682303473353386,\n",
       "   -0.018630869686603546,\n",
       "   0.08365584164857864,\n",
       "   0.047601208090782166,\n",
       "   -0.16935239732265472,\n",
       "   0.2833702564239502,\n",
       "   0.12189820408821106,\n",
       "   -0.4917048215866089,\n",
       "   0.5224189758300781,\n",
       "   0.08158328384160995,\n",
       "   0.07785224914550781,\n",
       "   0.8906053900718689,\n",
       "   0.10127472877502441,\n",
       "   -0.4020179212093353,\n",
       "   0.17184656858444214,\n",
       "   0.30420657992362976,\n",
       "   -0.3309769034385681,\n",
       "   0.4277556538581848,\n",
       "   -0.2868654727935791,\n",
       "   0.5523383021354675,\n",
       "   -0.28912046551704407,\n",
       "   0.37290990352630615,\n",
       "   -0.04067879542708397,\n",
       "   -0.05551303178071976,\n",
       "   -0.13687601685523987,\n",
       "   -0.29994168877601624,\n",
       "   0.24825890362262726,\n",
       "   0.1874229460954666,\n",
       "   0.10128293931484222,\n",
       "   -0.2612585723400116,\n",
       "   -0.3203577697277069,\n",
       "   -0.1790173202753067,\n",
       "   0.06644225120544434,\n",
       "   0.23667430877685547,\n",
       "   -0.07292050123214722,\n",
       "   0.0371282622218132,\n",
       "   0.004287751391530037,\n",
       "   -0.13084179162979126,\n",
       "   0.2671530544757843,\n",
       "   -0.2050998955965042,\n",
       "   -0.3359947204589844,\n",
       "   0.663327157497406,\n",
       "   -0.28976020216941833,\n",
       "   -0.16201448440551758,\n",
       "   -8.538155816495419e-05,\n",
       "   0.044055141508579254,\n",
       "   0.03685906529426575,\n",
       "   -0.01735539175570011,\n",
       "   0.037494610995054245,\n",
       "   -0.2503463625907898,\n",
       "   0.23044905066490173,\n",
       "   -0.19164255261421204,\n",
       "   0.2821591794490814,\n",
       "   0.1202182024717331,\n",
       "   -0.33890989422798157,\n",
       "   -0.154722198843956,\n",
       "   -0.2756355106830597,\n",
       "   -0.07882492244243622,\n",
       "   0.18077704310417175,\n",
       "   0.061676282435655594,\n",
       "   -0.043881550431251526,\n",
       "   -0.4297831952571869,\n",
       "   -0.12242630124092102,\n",
       "   0.4867512881755829,\n",
       "   -0.0017870552837848663,\n",
       "   0.532903254032135,\n",
       "   -0.2011435478925705,\n",
       "   0.1962241679430008,\n",
       "   0.17177839577198029,\n",
       "   0.6112411618232727,\n",
       "   -0.23867352306842804,\n",
       "   0.4171759784221649,\n",
       "   -0.3697170615196228,\n",
       "   0.03788112848997116,\n",
       "   0.23435916006565094,\n",
       "   -0.003829485969617963,\n",
       "   0.0934201255440712,\n",
       "   0.2549617290496826,\n",
       "   -0.34543129801750183,\n",
       "   -0.1366923302412033,\n",
       "   -0.07532772421836853,\n",
       "   0.31328344345092773,\n",
       "   0.17452262341976166,\n",
       "   0.2923644185066223,\n",
       "   -0.45042163133621216,\n",
       "   -0.031812842935323715,\n",
       "   -0.10080819576978683,\n",
       "   0.053152550011873245,\n",
       "   0.28282955288887024,\n",
       "   -0.7349063158035278,\n",
       "   -0.10437417030334473,\n",
       "   0.5179675817489624,\n",
       "   0.2839478552341461,\n",
       "   0.044260501861572266,\n",
       "   0.06968250870704651,\n",
       "   -0.2543763518333435,\n",
       "   -0.25282958149909973,\n",
       "   -0.32133591175079346,\n",
       "   0.23128587007522583,\n",
       "   -0.2702183723449707,\n",
       "   -0.1119004338979721,\n",
       "   0.5673360824584961,\n",
       "   0.21817149221897125,\n",
       "   -0.6245009303092957,\n",
       "   -0.14326860010623932,\n",
       "   -0.2814622223377228,\n",
       "   0.019654832780361176,\n",
       "   0.08600947260856628,\n",
       "   -0.012269863858819008,\n",
       "   -0.3146880865097046,\n",
       "   0.08279627561569214,\n",
       "   0.4800523519515991,\n",
       "   -0.23965221643447876,\n",
       "   0.3956538736820221,\n",
       "   -0.00891514215618372,\n",
       "   -0.15879033505916595,\n",
       "   0.1531285047531128,\n",
       "   0.20323213934898376,\n",
       "   -0.3276326060295105,\n",
       "   0.7224744558334351,\n",
       "   -0.5931824445724487,\n",
       "   0.21834903955459595,\n",
       "   -0.08925065398216248,\n",
       "   0.12112458795309067,\n",
       "   0.4626753330230713,\n",
       "   0.11335764825344086,\n",
       "   0.4537334740161896,\n",
       "   -0.27412763237953186,\n",
       "   -0.17266815900802612,\n",
       "   -0.369682252407074,\n",
       "   -0.3810664415359497,\n",
       "   0.014234685339033604,\n",
       "   -0.5636182427406311,\n",
       "   -0.040171168744564056,\n",
       "   -0.23603138327598572,\n",
       "   0.36006587743759155,\n",
       "   -0.5233231782913208,\n",
       "   -0.21323120594024658,\n",
       "   0.028268802911043167,\n",
       "   0.5501100420951843,\n",
       "   0.6047948002815247,\n",
       "   0.28056085109710693,\n",
       "   0.1500527560710907,\n",
       "   -0.21077562868595123,\n",
       "   -0.48765796422958374,\n",
       "   0.287969708442688,\n",
       "   -0.09444652497768402,\n",
       "   0.15870538353919983,\n",
       "   -0.35030192136764526,\n",
       "   0.17914964258670807,\n",
       "   -0.17136511206626892,\n",
       "   0.3119801878929138,\n",
       "   0.4843761622905731,\n",
       "   0.12355902045965195,\n",
       "   0.19923950731754303,\n",
       "   0.10623839497566223,\n",
       "   -0.15028801560401917,\n",
       "   -0.49865955114364624,\n",
       "   0.2123171091079712,\n",
       "   -0.45602017641067505,\n",
       "   -0.05247596278786659,\n",
       "   0.002665877342224121,\n",
       "   0.24121496081352234,\n",
       "   -0.38238725066185,\n",
       "   -0.3125019967556,\n",
       "   -0.5896741151809692,\n",
       "   -0.21716341376304626,\n",
       "   -0.11549344658851624,\n",
       "   0.21698251366615295,\n",
       "   -0.26323401927948,\n",
       "   -0.0730137899518013,\n",
       "   -0.06658968329429626,\n",
       "   -0.0639987587928772,\n",
       "   -0.09268256276845932,\n",
       "   -0.5809713006019592,\n",
       "   0.16412894427776337,\n",
       "   -0.08313950151205063,\n",
       "   0.03454296290874481,\n",
       "   -0.12032520771026611,\n",
       "   0.20796075463294983,\n",
       "   -0.02845989167690277,\n",
       "   -0.14473548531532288,\n",
       "   -0.09370655566453934,\n",
       "   -0.30572509765625,\n",
       "   0.2168678194284439,\n",
       "   0.22809283435344696,\n",
       "   0.15786424279212952,\n",
       "   0.3060404658317566,\n",
       "   -0.2894030809402466,\n",
       "   0.26351478695869446,\n",
       "   -0.3504876494407654,\n",
       "   -0.046353574842214584,\n",
       "   0.18372498452663422,\n",
       "   -0.13899146020412445,\n",
       "   -0.3103474974632263,\n",
       "   0.22996287047863007,\n",
       "   -0.03741815686225891,\n",
       "   -0.19104546308517456,\n",
       "   -0.16201306879520416,\n",
       "   -0.08577939867973328,\n",
       "   -0.16318318247795105,\n",
       "   -0.08298179507255554,\n",
       "   -0.1488473117351532,\n",
       "   0.07608769088983536,\n",
       "   0.46172934770584106,\n",
       "   -0.2646411657333374,\n",
       "   -0.5016830563545227,\n",
       "   -0.3622983396053314,\n",
       "   0.25896990299224854,\n",
       "   0.2869430482387543,\n",
       "   0.5618934035301208,\n",
       "   -0.0656551793217659,\n",
       "   -0.182443305850029,\n",
       "   -0.22580893337726593,\n",
       "   0.36543962359428406,\n",
       "   -0.00600975938141346,\n",
       "   0.13818851113319397,\n",
       "   0.4065086245536804,\n",
       "   0.023546874523162842,\n",
       "   -0.02660617046058178,\n",
       "   -0.14492575824260712,\n",
       "   -0.8568020462989807,\n",
       "   0.15001335740089417,\n",
       "   0.17898738384246826,\n",
       "   -0.13954424858093262,\n",
       "   0.4933999180793762,\n",
       "   -0.09181022644042969,\n",
       "   -0.1466590166091919,\n",
       "   -0.2337236851453781,\n",
       "   0.3207126259803772,\n",
       "   0.3891821503639221,\n",
       "   -0.31812459230422974,\n",
       "   -0.17354290187358856,\n",
       "   -0.3614796996116638,\n",
       "   0.0012782961130142212,\n",
       "   -0.4159018099308014,\n",
       "   0.5858232378959656,\n",
       "   -0.1412869095802307,\n",
       "   0.5271643400192261,\n",
       "   0.016722433269023895,\n",
       "   0.21780255436897278,\n",
       "   -0.6339845061302185,\n",
       "   0.20317137241363525,\n",
       "   -0.07165427505970001,\n",
       "   0.005372179672122002,\n",
       "   -0.04978416860103607,\n",
       "   -0.39731040596961975,\n",
       "   -0.2703627049922943,\n",
       "   0.09139512479305267,\n",
       "   0.28777042031288147,\n",
       "   -0.46449100971221924,\n",
       "   0.34780821204185486,\n",
       "   -0.05130702257156372,\n",
       "   -0.2255183756351471,\n",
       "   -0.0696653202176094,\n",
       "   0.15498092770576477,\n",
       "   -0.19721783697605133,\n",
       "   0.07669822871685028,\n",
       "   -0.06048191338777542,\n",
       "   0.00046484172344207764,\n",
       "   -0.4507623314857483,\n",
       "   0.06659846007823944,\n",
       "   0.4352515935897827,\n",
       "   0.4204883575439453,\n",
       "   0.36917299032211304,\n",
       "   0.2501051127910614,\n",
       "   -0.5123940706253052,\n",
       "   0.34246399998664856,\n",
       "   0.3373982310295105,\n",
       "   0.3479726016521454,\n",
       "   -0.2168128788471222,\n",
       "   -0.2651115953922272,\n",
       "   0.16333404183387756,\n",
       "   0.2580566704273224,\n",
       "   -0.7078707814216614,\n",
       "   0.1640683114528656,\n",
       "   -0.1114533394575119,\n",
       "   -0.010198179632425308,\n",
       "   -0.05493532121181488,\n",
       "   0.36400675773620605,\n",
       "   -0.3444760739803314,\n",
       "   -0.25349685549736023,\n",
       "   -0.3767421543598175,\n",
       "   0.08505526930093765,\n",
       "   0.2562457323074341,\n",
       "   0.13639159500598907,\n",
       "   -0.18084177374839783,\n",
       "   -0.859517514705658,\n",
       "   0.17695660889148712,\n",
       "   -0.12898653745651245,\n",
       "   -0.06778304278850555,\n",
       "   0.5088977217674255,\n",
       "   0.41431134939193726,\n",
       "   0.2264896035194397,\n",
       "   0.1978958547115326,\n",
       "   -0.35914725065231323,\n",
       "   0.14089635014533997,\n",
       "   0.068503737449646,\n",
       "   -0.3873603940010071,\n",
       "   -0.5178375840187073,\n",
       "   -0.2330508977174759,\n",
       "   -0.14532151818275452,\n",
       "   0.4971678555011749,\n",
       "   0.5020577907562256,\n",
       "   -0.588623046875,\n",
       "   0.19678516685962677,\n",
       "   -0.06515857577323914,\n",
       "   -0.3492591977119446,\n",
       "   0.013919670134782791,\n",
       "   0.3298570513725281,\n",
       "   0.14004001021385193,\n",
       "   -0.1939309537410736,\n",
       "   0.01717306300997734,\n",
       "   0.18248069286346436,\n",
       "   -0.06340678781270981,\n",
       "   -0.5034672021865845,\n",
       "   -0.39561474323272705,\n",
       "   -0.1290588229894638,\n",
       "   -0.27686193585395813,\n",
       "   0.24908895790576935,\n",
       "   -0.11404185742139816,\n",
       "   0.322529137134552,\n",
       "   0.18218430876731873,\n",
       "   -0.008789591491222382,\n",
       "   0.24241070449352264,\n",
       "   -0.2529588043689728,\n",
       "   0.4048524498939514,\n",
       "   -0.1789894849061966,\n",
       "   -0.05180549621582031,\n",
       "   -0.22442397475242615,\n",
       "   -0.06441392004489899,\n",
       "   0.43866488337516785,\n",
       "   0.008010908961296082,\n",
       "   0.08875030279159546,\n",
       "   -0.023502029478549957,\n",
       "   0.07542258501052856,\n",
       "   0.2902790904045105,\n",
       "   0.2501511871814728,\n",
       "   0.020081857219338417,\n",
       "   0.05902739241719246,\n",
       "   0.09603913128376007,\n",
       "   0.2134655863046646,\n",
       "   0.14963872730731964,\n",
       "   -0.14202700555324554,\n",
       "   -0.5195451378822327,\n",
       "   -0.10715498775243759,\n",
       "   0.038912974298000336,\n",
       "   0.04277011379599571,\n",
       "   0.17188262939453125,\n",
       "   -0.0015731379389762878,\n",
       "   0.3909703493118286,\n",
       "   -0.23781226575374603,\n",
       "   0.7424627542495728,\n",
       "   -0.33552905917167664,\n",
       "   0.25690051913261414,\n",
       "   -0.29824990034103394,\n",
       "   -0.06990744173526764,\n",
       "   -0.07204827666282654,\n",
       "   -0.29939883947372437,\n",
       "   0.00611584447324276,\n",
       "   0.027818461880087852,\n",
       "   -0.455874502658844,\n",
       "   -0.0973542332649231,\n",
       "   0.21095143258571625,\n",
       "   0.6865812540054321,\n",
       "   -0.18912483751773834,\n",
       "   0.23749591410160065,\n",
       "   -0.4773625135421753,\n",
       "   0.1705971211194992,\n",
       "   -0.08132623881101608,\n",
       "   0.36546704173088074,\n",
       "   0.1337437778711319,\n",
       "   0.42046284675598145,\n",
       "   -0.02323191612958908,\n",
       "   0.3555743098258972,\n",
       "   0.07926074415445328,\n",
       "   -0.2681882381439209,\n",
       "   -0.15400388836860657,\n",
       "   -0.3109884262084961,\n",
       "   0.1597648411989212,\n",
       "   0.07967302203178406,\n",
       "   -0.19272729754447937,\n",
       "   -0.3793773055076599,\n",
       "   0.19909781217575073,\n",
       "   -0.06992683559656143,\n",
       "   -0.19074346125125885,\n",
       "   -0.12638047337532043,\n",
       "   -0.5467631220817566,\n",
       "   0.013998470269143581,\n",
       "   0.18523302674293518,\n",
       "   0.18227814137935638,\n",
       "   0.03600471839308739,\n",
       "   -0.08682483434677124,\n",
       "   -0.3155366778373718,\n",
       "   -0.18816609680652618,\n",
       "   0.5235685706138611,\n",
       "   0.32038941979408264,\n",
       "   -0.39599916338920593,\n",
       "   0.23779165744781494,\n",
       "   -0.12753784656524658,\n",
       "   0.10524643212556839,\n",
       "   0.03633101284503937,\n",
       "   0.0981212705373764,\n",
       "   -0.13765305280685425,\n",
       "   -0.17168398201465607,\n",
       "   -0.153740793466568,\n",
       "   0.25866395235061646,\n",
       "   -0.1683775782585144,\n",
       "   0.08551928400993347,\n",
       "   -0.3115885257720947,\n",
       "   -0.42880409955978394,\n",
       "   0.5075844526290894,\n",
       "   0.0017557218670845032,\n",
       "   -0.30108776688575745,\n",
       "   0.00902608036994934,\n",
       "   0.2797882556915283,\n",
       "   -0.4239885210990906,\n",
       "   0.5470949411392212,\n",
       "   0.3156229853630066,\n",
       "   0.4432592988014221,\n",
       "   -0.0958164632320404,\n",
       "   0.16891705989837646,\n",
       "   0.17567531764507294,\n",
       "   -0.2544829845428467,\n",
       "   -0.04954424127936363,\n",
       "   -0.46157345175743103,\n",
       "   -0.23219461739063263,\n",
       "   -0.30620667338371277,\n",
       "   0.27848097681999207,\n",
       "   -0.3672805428504944,\n",
       "   0.28286993503570557,\n",
       "   -0.009174976497888565,\n",
       "   0.27188992500305176,\n",
       "   -0.1991022527217865,\n",
       "   -0.05488860607147217,\n",
       "   -0.39756929874420166,\n",
       "   0.3716476261615753,\n",
       "   -0.12217600643634796,\n",
       "   -0.2717757225036621,\n",
       "   -0.24256262183189392,\n",
       "   -0.14728528261184692,\n",
       "   -0.20084896683692932,\n",
       "   0.07986022531986237,\n",
       "   0.24943825602531433,\n",
       "   0.3369887173175812,\n",
       "   0.47297903895378113,\n",
       "   -0.03890034928917885,\n",
       "   0.000379316508769989,\n",
       "   0.2303621768951416,\n",
       "   0.37665215134620667,\n",
       "   0.08140763640403748,\n",
       "   -0.0607578381896019,\n",
       "   0.538267195224762,\n",
       "   0.0642184317111969,\n",
       "   0.20213311910629272,\n",
       "   0.4138662815093994,\n",
       "   -0.1930159479379654,\n",
       "   -0.21939030289649963,\n",
       "   -0.16473518311977386,\n",
       "   0.4899192154407501,\n",
       "   -0.5034286975860596,\n",
       "   -0.0725044384598732,\n",
       "   -0.12600871920585632,\n",
       "   -0.1042628139257431,\n",
       "   -0.13484933972358704,\n",
       "   -0.17648689448833466,\n",
       "   0.5261836647987366,\n",
       "   -0.29050108790397644,\n",
       "   0.6408199667930603,\n",
       "   -0.05856499820947647,\n",
       "   0.06075044348835945,\n",
       "   0.30638545751571655,\n",
       "   -0.2671169936656952,\n",
       "   -0.2788453698158264,\n",
       "   0.2427617609500885,\n",
       "   0.11250492185354233,\n",
       "   -0.03693297877907753,\n",
       "   0.02109324000775814,\n",
       "   -0.15776224434375763,\n",
       "   -0.14244335889816284,\n",
       "   -0.3042551875114441,\n",
       "   0.12864699959754944,\n",
       "   -0.5330509543418884,\n",
       "   -0.18862199783325195,\n",
       "   0.1428264081478119,\n",
       "   0.03108673356473446,\n",
       "   -0.2284584641456604,\n",
       "   0.876287579536438,\n",
       "   0.579356849193573,\n",
       "   -0.43257781863212585,\n",
       "   0.1880376636981964,\n",
       "   -0.2160225659608841,\n",
       "   -0.20937331020832062,\n",
       "   -0.6137093305587769,\n",
       "   -0.023942753672599792,\n",
       "   0.0376744419336319,\n",
       "   -0.1254623383283615,\n",
       "   -0.2098856270313263,\n",
       "   0.3158891797065735,\n",
       "   -0.12539799511432648,\n",
       "   -0.009359501302242279,\n",
       "   0.20354555547237396,\n",
       "   0.6587958335876465,\n",
       "   0.24253296852111816,\n",
       "   -0.2114257663488388,\n",
       "   -0.25033828616142273,\n",
       "   -0.044923730194568634,\n",
       "   0.2737962603569031,\n",
       "   0.7606791257858276,\n",
       "   0.09243728220462799,\n",
       "   0.08066040277481079,\n",
       "   0.3366348147392273,\n",
       "   -0.3980686068534851,\n",
       "   0.29193973541259766,\n",
       "   -0.004101462196558714,\n",
       "   0.0762399435043335,\n",
       "   0.8717882037162781,\n",
       "   -0.04986729472875595,\n",
       "   -0.17173385620117188,\n",
       "   0.09651833027601242,\n",
       "   -0.4136779010295868,\n",
       "   0.18127217888832092,\n",
       "   0.050770893692970276,\n",
       "   -0.09530943632125854,\n",
       "   -0.045974280685186386,\n",
       "   0.12423916161060333,\n",
       "   -0.16614381968975067,\n",
       "   0.38909098505973816,\n",
       "   0.25796183943748474,\n",
       "   0.28161191940307617,\n",
       "   0.11072946339845657,\n",
       "   0.26184287667274475,\n",
       "   0.0008685742504894733,\n",
       "   -0.06953422725200653,\n",
       "   -0.7481268048286438,\n",
       "   0.1528906226158142,\n",
       "   0.22463266551494598,\n",
       "   -0.0036383606493473053,\n",
       "   -0.12603424489498138,\n",
       "   -8.999353408813477,\n",
       "   0.3217071294784546,\n",
       "   -0.04464658349752426,\n",
       "   0.16836148500442505,\n",
       "   0.3016795217990875,\n",
       "   -0.03146398812532425,\n",
       "   0.029475141316652298,\n",
       "   -0.630878210067749,\n",
       "   -0.7517759799957275,\n",
       "   0.18763820827007294,\n",
       "   0.010063320398330688,\n",
       "   0.09255540370941162,\n",
       "   0.41599899530410767,\n",
       "   -0.28053519129753113,\n",
       "   0.16165581345558167,\n",
       "   -0.2639472186565399,\n",
       "   0.2221774458885193,\n",
       "   -0.5653944611549377,\n",
       "   0.3649219870567322,\n",
       "   -0.08855637907981873,\n",
       "   0.061309196054935455,\n",
       "   0.441487580537796,\n",
       "   -0.22061170637607574,\n",
       "   0.4304497539997101,\n",
       "   0.12455371767282486,\n",
       "   0.3414780795574188,\n",
       "   -0.012087799608707428,\n",
       "   -0.06078856810927391,\n",
       "   0.27137142419815063,\n",
       "   -0.10276015102863312,\n",
       "   -0.1361442655324936,\n",
       "   0.3643707036972046,\n",
       "   0.0029640905559062958,\n",
       "   -0.23015038669109344,\n",
       "   0.4713820815086365,\n",
       "   -0.2655623257160187,\n",
       "   0.2382424771785736,\n",
       "   0.09812332689762115,\n",
       "   0.1551549732685089,\n",
       "   -0.5550961494445801,\n",
       "   -0.20755454897880554,\n",
       "   -0.041183676570653915,\n",
       "   -0.17978762090206146,\n",
       "   0.2723586857318878,\n",
       "   0.13414333760738373,\n",
       "   0.019539907574653625,\n",
       "   0.330207496881485,\n",
       "   0.4179411828517914,\n",
       "   -0.14288084208965302,\n",
       "   -0.21366271376609802,\n",
       "   0.2619004547595978,\n",
       "   0.4650755524635315,\n",
       "   0.08701498806476593,\n",
       "   0.18946795165538788,\n",
       "   0.3481075167655945,\n",
       "   0.23744040727615356,\n",
       "   -0.048567257821559906,\n",
       "   0.6342607736587524,\n",
       "   -0.27459752559661865,\n",
       "   -0.3747442960739136,\n",
       "   0.39568519592285156,\n",
       "   -0.19444787502288818,\n",
       "   0.8582135438919067,\n",
       "   0.016868703067302704,\n",
       "   0.0021727681159973145,\n",
       "   0.2970178425312042,\n",
       "   0.07636323571205139,\n",
       "   -0.08508989959955215,\n",
       "   0.20625106990337372,\n",
       "   -0.418404757976532,\n",
       "   -0.4455188512802124,\n",
       "   -0.2013373076915741,\n",
       "   0.10678388178348541,\n",
       "   -0.5432322025299072,\n",
       "   -0.2860344350337982,\n",
       "   -0.22817851603031158,\n",
       "   -0.19513866305351257,\n",
       "   -0.007889710366725922,\n",
       "   -0.020244352519512177,\n",
       "   0.017201699316501617,\n",
       "   -0.440136581659317,\n",
       "   0.6403215527534485,\n",
       "   0.7450941205024719,\n",
       "   -0.3521512746810913,\n",
       "   -0.13234253227710724,\n",
       "   -0.22960549592971802,\n",
       "   -0.06280320137739182,\n",
       "   0.5212648510932922,\n",
       "   0.23034259676933289,\n",
       "   -0.14702108502388,\n",
       "   -0.08800298720598221,\n",
       "   0.24004009366035461,\n",
       "   0.2587001919746399,\n",
       "   0.24559557437896729,\n",
       "   -0.213115856051445,\n",
       "   0.4161129295825958,\n",
       "   -0.032949965447187424,\n",
       "   0.016953010112047195,\n",
       "   -0.07967526465654373,\n",
       "   0.007562445476651192,\n",
       "   -0.06319012492895126,\n",
       "   -0.14362962543964386,\n",
       "   -0.6314523220062256,\n",
       "   -0.23479825258255005,\n",
       "   -0.030016999691724777,\n",
       "   -0.1388438194990158,\n",
       "   -0.06113153323531151,\n",
       "   -0.3778402805328369,\n",
       "   0.5381001830101013,\n",
       "   0.3161104917526245,\n",
       "   -0.19397972524166107,\n",
       "   0.30043137073516846,\n",
       "   0.5870150923728943,\n",
       "   -0.015773775056004524,\n",
       "   0.2721717357635498,\n",
       "   0.39699387550354004,\n",
       "   -0.20656093955039978,\n",
       "   -0.35150614380836487,\n",
       "   -0.024800917133688927,\n",
       "   -0.07735414803028107,\n",
       "   -0.13374090194702148,\n",
       "   -0.2900844216346741,\n",
       "   -0.32052144408226013,\n",
       "   -0.2635993957519531,\n",
       "   0.24110805988311768,\n",
       "   0.10319669544696808,\n",
       "   -0.22554488480091095,\n",
       "   -0.5072115659713745,\n",
       "   0.017501771450042725,\n",
       "   -0.028519507497549057,\n",
       "   0.6414295434951782,\n",
       "   0.3313780426979065,\n",
       "   -0.3362939655780792,\n",
       "   0.3946354389190674,\n",
       "   0.007589690387248993,\n",
       "   -0.05294450372457504,\n",
       "   -0.2747686207294464,\n",
       "   0.16706393659114838,\n",
       "   -0.45583024621009827,\n",
       "   0.011317433789372444,\n",
       "   -0.2533473074436188,\n",
       "   -0.1674576848745346,\n",
       "   0.832347571849823,\n",
       "   0.06274242699146271,\n",
       "   0.6852891445159912,\n",
       "   0.03830668330192566,\n",
       "   -0.030538469552993774,\n",
       "   -0.21385151147842407,\n",
       "   -0.05297791585326195,\n",
       "   -0.12857580184936523,\n",
       "   -0.07907041907310486,\n",
       "   0.5193734765052795,\n",
       "   0.3684435188770294,\n",
       "   0.08162911981344223,\n",
       "   0.20541240274906158,\n",
       "   -0.4686743915081024,\n",
       "   3.209710121154785e-05,\n",
       "   0.12128942459821701,\n",
       "   -0.6480848789215088,\n",
       "   0.03944752737879753,\n",
       "   0.17222589254379272,\n",
       "   -0.012855060398578644,\n",
       "   0.03631968796253204,\n",
       "   0.011874450370669365,\n",
       "   -0.33758237957954407,\n",
       "   -0.12635086476802826,\n",
       "   -0.11699521541595459,\n",
       "   0.09132567048072815,\n",
       "   0.03185693919658661,\n",
       "   -0.18540233373641968,\n",
       "   -0.17000257968902588,\n",
       "   -0.03367264196276665,\n",
       "   0.11955338716506958,\n",
       "   0.19259725511074066,\n",
       "   0.06709835678339005,\n",
       "   -0.048445720225572586,\n",
       "   0.25349897146224976,\n",
       "   -0.22531750798225403,\n",
       "   0.36905431747436523,\n",
       "   0.2651670277118683,\n",
       "   0.5691763162612915,\n",
       "   0.2886292040348053,\n",
       "   -0.1527673304080963,\n",
       "   -0.7804216146469116,\n",
       "   -0.2748805284500122,\n",
       "   0.4905607998371124,\n",
       "   0.454522967338562,\n",
       "   0.05469171702861786,\n",
       "   -0.13031704723834991,\n",
       "   0.3847528100013733,\n",
       "   0.01947116106748581,\n",
       "   0.05145920813083649,\n",
       "   -0.03250489383935928,\n",
       "   0.22893407940864563,\n",
       "   -0.04318094998598099,\n",
       "   0.43716976046562195,\n",
       "   0.08685074746608734,\n",
       "   0.3104165196418762,\n",
       "   -0.3181529641151428,\n",
       "   -0.11609218269586563,\n",
       "   0.19027069211006165,\n",
       "   0.22529110312461853,\n",
       "   -0.0229530930519104,\n",
       "   -0.4304540753364563,\n",
       "   0.25192224979400635,\n",
       "   -0.03491279482841492,\n",
       "   -0.08118055760860443,\n",
       "   0.23829394578933716,\n",
       "   0.38286858797073364,\n",
       "   -0.08544863015413284,\n",
       "   0.21378913521766663],\n",
       "  [0.28046566247940063,\n",
       "   -0.1907060146331787,\n",
       "   -0.07161261141300201,\n",
       "   0.13770951330661774,\n",
       "   0.046841636300086975,\n",
       "   -0.18801838159561157,\n",
       "   0.24970127642154694,\n",
       "   -0.01801086589694023,\n",
       "   -0.417228102684021,\n",
       "   0.21666130423545837,\n",
       "   0.2436591386795044,\n",
       "   0.19053500890731812,\n",
       "   -0.5529879927635193,\n",
       "   0.41170942783355713,\n",
       "   -0.3722992241382599,\n",
       "   0.2776958644390106,\n",
       "   0.2829170525074005,\n",
       "   -0.20407482981681824,\n",
       "   0.040250517427921295,\n",
       "   0.11470455676317215,\n",
       "   0.08038927614688873,\n",
       "   0.0507783368229866,\n",
       "   -0.30934053659439087,\n",
       "   -0.004048982635140419,\n",
       "   -0.082188680768013,\n",
       "   -0.502585768699646,\n",
       "   0.2120140790939331,\n",
       "   0.15799197554588318,\n",
       "   0.5911694765090942,\n",
       "   0.43270084261894226,\n",
       "   0.15369099378585815,\n",
       "   -0.27464959025382996,\n",
       "   0.47410887479782104,\n",
       "   0.04739478975534439,\n",
       "   -0.2523052990436554,\n",
       "   0.3999541997909546,\n",
       "   -0.08065534383058548,\n",
       "   0.41212379932403564,\n",
       "   -0.1391267031431198,\n",
       "   0.14276130497455597,\n",
       "   0.2154366672039032,\n",
       "   -0.21752890944480896,\n",
       "   -0.15143753588199615,\n",
       "   -0.518211305141449,\n",
       "   0.28600484132766724,\n",
       "   -0.4104137420654297,\n",
       "   0.14714473485946655,\n",
       "   -0.31390711665153503,\n",
       "   -0.2017967402935028,\n",
       "   0.03571142628788948,\n",
       "   -0.00596959562972188,\n",
       "   0.37444978952407837,\n",
       "   -0.6001505255699158,\n",
       "   -0.16806048154830933,\n",
       "   -0.09315285831689835,\n",
       "   -0.3247120976448059,\n",
       "   0.15698379278182983,\n",
       "   -0.0006217584013938904,\n",
       "   -0.42996662855148315,\n",
       "   0.6649532318115234,\n",
       "   -0.38317084312438965,\n",
       "   0.4111851751804352,\n",
       "   -0.018294017761945724,\n",
       "   0.2554061710834503,\n",
       "   0.14531387388706207,\n",
       "   0.34802791476249695,\n",
       "   -0.0992635041475296,\n",
       "   0.005301326513290405,\n",
       "   0.2456410527229309,\n",
       "   -0.4722099304199219,\n",
       "   0.12551620602607727,\n",
       "   0.21108736097812653,\n",
       "   -0.3383449614048004,\n",
       "   -0.045976608991622925,\n",
       "   0.22009724378585815,\n",
       "   -0.40419328212738037,\n",
       "   0.08835428953170776,\n",
       "   -0.1378772258758545,\n",
       "   0.3614386022090912,\n",
       "   -0.2579596936702728,\n",
       "   0.19070197641849518,\n",
       "   0.6091405749320984,\n",
       "   -0.046463266015052795,\n",
       "   0.5479344725608826,\n",
       "   0.08290408551692963,\n",
       "   0.03386375308036804,\n",
       "   -0.05032047629356384,\n",
       "   0.38689011335372925,\n",
       "   0.1379460245370865,\n",
       "   0.36167001724243164,\n",
       "   -0.24461863934993744,\n",
       "   -0.15714028477668762,\n",
       "   0.23653200268745422,\n",
       "   -0.21548347175121307,\n",
       "   -0.21891914308071136,\n",
       "   0.30615848302841187,\n",
       "   -0.2593446373939514,\n",
       "   -0.38854238390922546,\n",
       "   -0.46611741185188293,\n",
       "   0.23841401934623718,\n",
       "   -0.05474341660737991,\n",
       "   0.08159220218658447,\n",
       "   -0.30595794320106506,\n",
       "   -0.13717257976531982,\n",
       "   -0.03643728047609329,\n",
       "   0.14796000719070435,\n",
       "   -0.20264042913913727,\n",
       "   -0.5799483060836792,\n",
       "   -0.2555874288082123,\n",
       "   0.5630893111228943,\n",
       "   0.025393780320882797,\n",
       "   0.11531226336956024,\n",
       "   -0.16514141857624054,\n",
       "   0.07766444981098175,\n",
       "   -0.022659242153167725,\n",
       "   -0.05984171852469444,\n",
       "   -0.15539290010929108,\n",
       "   -0.42498183250427246,\n",
       "   -0.35624662041664124,\n",
       "   0.20757409930229187,\n",
       "   0.1738976687192917,\n",
       "   0.14418761432170868,\n",
       "   -0.07639700174331665,\n",
       "   -0.22204676270484924,\n",
       "   0.10295402258634567,\n",
       "   -0.09547226130962372,\n",
       "   -0.1919882893562317,\n",
       "   -0.34288540482521057,\n",
       "   -0.0868791788816452,\n",
       "   0.45756465196609497,\n",
       "   -0.21950063109397888,\n",
       "   -0.18788953125476837,\n",
       "   0.06836912035942078,\n",
       "   -0.1343105286359787,\n",
       "   0.13328932225704193,\n",
       "   -0.07013221830129623,\n",
       "   -0.31547340750694275,\n",
       "   0.26853281259536743,\n",
       "   -0.9699925184249878,\n",
       "   0.11136531829833984,\n",
       "   -0.2754775285720825,\n",
       "   -0.37144672870635986,\n",
       "   0.4972290098667145,\n",
       "   0.16314572095870972,\n",
       "   0.2403424084186554,\n",
       "   -0.18157848715782166,\n",
       "   -0.2793082296848297,\n",
       "   -0.18836426734924316,\n",
       "   -0.05748417600989342,\n",
       "   0.299461305141449,\n",
       "   -0.671981155872345,\n",
       "   -0.044774238020181656,\n",
       "   0.07126785814762115,\n",
       "   0.5851690173149109,\n",
       "   -0.2497045397758484,\n",
       "   -0.12436899542808533,\n",
       "   -0.06188753619790077,\n",
       "   0.44257181882858276,\n",
       "   0.6064181327819824,\n",
       "   0.3930947780609131,\n",
       "   0.22473157942295074,\n",
       "   -0.13751594722270966,\n",
       "   -0.30458903312683105,\n",
       "   0.007434658706188202,\n",
       "   0.01976519078016281,\n",
       "   0.04383612051606178,\n",
       "   -0.36319833993911743,\n",
       "   0.20249827206134796,\n",
       "   0.10180529952049255,\n",
       "   0.06803611665964127,\n",
       "   0.3316957354545593,\n",
       "   0.005490189418196678,\n",
       "   -0.029876045882701874,\n",
       "   0.22760573029518127,\n",
       "   -0.05782100185751915,\n",
       "   -0.07238645106554031,\n",
       "   0.03959353268146515,\n",
       "   -0.1148080974817276,\n",
       "   0.13026128709316254,\n",
       "   0.11777308583259583,\n",
       "   0.11285483837127686,\n",
       "   -0.07290905714035034,\n",
       "   -0.2236161082983017,\n",
       "   -0.41470852494239807,\n",
       "   0.17711994051933289,\n",
       "   -0.0029919659718871117,\n",
       "   0.41527482867240906,\n",
       "   -0.2980608344078064,\n",
       "   0.6583303809165955,\n",
       "   -0.2813657820224762,\n",
       "   -0.06928694248199463,\n",
       "   -0.13537299633026123,\n",
       "   -0.6296666264533997,\n",
       "   0.2090333104133606,\n",
       "   -0.19579994678497314,\n",
       "   -0.2244706153869629,\n",
       "   0.25079864263534546,\n",
       "   0.04155122488737106,\n",
       "   0.20382007956504822,\n",
       "   0.020759329199790955,\n",
       "   -0.3248455226421356,\n",
       "   0.12609732151031494,\n",
       "   0.05276285111904144,\n",
       "   -0.05727829784154892,\n",
       "   -0.02086416631937027,\n",
       "   0.20032286643981934,\n",
       "   -0.3126007616519928,\n",
       "   0.2171267867088318,\n",
       "   -0.1652587503194809,\n",
       "   -0.29021820425987244,\n",
       "   0.37143102288246155,\n",
       "   -0.48044145107269287,\n",
       "   -0.3374648690223694,\n",
       "   -0.183543398976326,\n",
       "   -0.24972519278526306,\n",
       "   -0.2881585657596588,\n",
       "   0.34243646264076233,\n",
       "   -0.059604257345199585,\n",
       "   -0.08653917908668518,\n",
       "   -0.021655671298503876,\n",
       "   0.028078284114599228,\n",
       "   -0.07577262818813324,\n",
       "   0.39770054817199707,\n",
       "   -0.4505549669265747,\n",
       "   -0.13066688179969788,\n",
       "   -0.22321335971355438,\n",
       "   0.15359950065612793,\n",
       "   0.2481004297733307,\n",
       "   0.2492152750492096,\n",
       "   -0.1708306521177292,\n",
       "   -0.19797533750534058,\n",
       "   -0.11478942632675171,\n",
       "   0.1790669560432434,\n",
       "   -0.2998792231082916,\n",
       "   -0.012273834086954594,\n",
       "   0.21376857161521912,\n",
       "   0.12966451048851013,\n",
       "   0.12815681099891663,\n",
       "   -0.1688053160905838,\n",
       "   -0.8358518481254578,\n",
       "   0.03342067450284958,\n",
       "   0.12065461277961731,\n",
       "   0.07747851312160492,\n",
       "   0.07218116521835327,\n",
       "   -0.06992574036121368,\n",
       "   0.06738431751728058,\n",
       "   -0.26335740089416504,\n",
       "   0.37408703565597534,\n",
       "   0.33649975061416626,\n",
       "   -0.08286446332931519,\n",
       "   0.18065781891345978,\n",
       "   -0.16770988702774048,\n",
       "   0.10610955208539963,\n",
       "   -0.27909204363822937,\n",
       "   0.5945280194282532,\n",
       "   -0.5345039367675781,\n",
       "   0.002979673445224762,\n",
       "   0.2213110327720642,\n",
       "   -0.005304649472236633,\n",
       "   -0.4740186333656311,\n",
       "   0.10756248980760574,\n",
       "   0.13850955665111542,\n",
       "   0.1655110865831375,\n",
       "   0.10023956000804901,\n",
       "   -0.22550196945667267,\n",
       "   -0.20865754783153534,\n",
       "   0.09439337253570557,\n",
       "   0.35153600573539734,\n",
       "   -0.3605830669403076,\n",
       "   0.6094027757644653,\n",
       "   0.20584893226623535,\n",
       "   -0.39897385239601135,\n",
       "   -0.26624783873558044,\n",
       "   0.4169728457927704,\n",
       "   -0.12624722719192505,\n",
       "   0.17250412702560425,\n",
       "   -0.04548436403274536,\n",
       "   -0.3575599491596222,\n",
       "   -0.2762342393398285,\n",
       "   -0.0451497882604599,\n",
       "   0.6578978300094604,\n",
       "   0.20727437734603882,\n",
       "   -0.021165311336517334,\n",
       "   0.1530093103647232,\n",
       "   -0.49907350540161133,\n",
       "   -0.224941223859787,\n",
       "   -0.14443056285381317,\n",
       "   0.3264307677745819,\n",
       "   -0.4026346802711487,\n",
       "   -0.31889092922210693,\n",
       "   -0.18746072053909302,\n",
       "   0.23551800847053528,\n",
       "   -0.23759408295154572,\n",
       "   -0.086464062333107,\n",
       "   -0.148118257522583,\n",
       "   -0.04754452407360077,\n",
       "   -0.3354332745075226,\n",
       "   0.377895712852478,\n",
       "   -0.3946053683757782,\n",
       "   -0.12479192018508911,\n",
       "   -0.10522522032260895,\n",
       "   0.18605607748031616,\n",
       "   0.2801075279712677,\n",
       "   0.3042789399623871,\n",
       "   -0.19937416911125183,\n",
       "   -0.47311294078826904,\n",
       "   0.28181350231170654,\n",
       "   0.42185908555984497,\n",
       "   0.5846206545829773,\n",
       "   0.3771345913410187,\n",
       "   0.5932139754295349,\n",
       "   0.19461527466773987,\n",
       "   -0.3596399128437042,\n",
       "   -0.3920895457267761,\n",
       "   0.15044721961021423,\n",
       "   0.06536264717578888,\n",
       "   -0.07896704971790314,\n",
       "   -0.4464740753173828,\n",
       "   -0.333562433719635,\n",
       "   0.370476096868515,\n",
       "   0.4445869028568268,\n",
       "   0.23213046789169312,\n",
       "   -0.1687229722738266,\n",
       "   0.21553008258342743,\n",
       "   0.12799294292926788,\n",
       "   -0.09816664457321167,\n",
       "   0.06661953032016754,\n",
       "   0.10992521047592163,\n",
       "   -0.18555158376693726,\n",
       "   -0.040740661323070526,\n",
       "   0.046661145985126495,\n",
       "   -0.2762303352355957,\n",
       "   0.06632214784622192,\n",
       "   -0.4623810648918152,\n",
       "   -0.1945383995771408,\n",
       "   0.1255946010351181,\n",
       "   -0.3792429268360138,\n",
       "   0.5554714798927307,\n",
       "   -0.36829662322998047,\n",
       "   0.35641002655029297,\n",
       "   0.12620580196380615,\n",
       "   0.1349298655986786,\n",
       "   0.24288196861743927,\n",
       "   -0.3725599944591522,\n",
       "   0.2883260250091553,\n",
       "   0.04158497601747513,\n",
       "   -0.12451440840959549,\n",
       "   0.06824344396591187,\n",
       "   0.0007359124720096588,\n",
       "   -0.03811458498239517,\n",
       "   -0.23182779550552368,\n",
       "   -0.11581242084503174,\n",
       "   -0.20385107398033142,\n",
       "   -0.1381775140762329,\n",
       "   0.2427220344543457,\n",
       "   0.3383963406085968,\n",
       "   0.2618468403816223,\n",
       "   0.05679131671786308,\n",
       "   0.26532629132270813,\n",
       "   0.4210490882396698,\n",
       "   0.251071035861969,\n",
       "   0.18245728313922882,\n",
       "   -0.24122293293476105,\n",
       "   -0.18292666971683502,\n",
       "   0.11523288488388062,\n",
       "   -0.11466916650533676,\n",
       "   0.0939355418086052,\n",
       "   0.3563159108161926,\n",
       "   -0.1719617247581482,\n",
       "   0.05633055046200752,\n",
       "   0.7249979972839355,\n",
       "   -0.389008492231369,\n",
       "   0.5116390585899353,\n",
       "   0.006958644837141037,\n",
       "   0.07264629006385803,\n",
       "   0.11919616907835007,\n",
       "   -0.19112737476825714,\n",
       "   0.015644002705812454,\n",
       "   0.1742994636297226,\n",
       "   -0.6224802732467651,\n",
       "   0.213344007730484,\n",
       "   0.1548260599374771,\n",
       "   0.7324671745300293,\n",
       "   -0.3307558000087738,\n",
       "   0.09260238707065582,\n",
       "   -0.06254199892282486,\n",
       "   -0.25699418783187866,\n",
       "   -0.09689122438430786,\n",
       "   0.0671636089682579,\n",
       "   0.0551040880382061,\n",
       "   0.5388460159301758,\n",
       "   -0.15824821591377258,\n",
       "   0.2227768898010254,\n",
       "   -0.3230810761451721,\n",
       "   -0.16120529174804688,\n",
       "   -0.26929858326911926,\n",
       "   -0.4310672879219055,\n",
       "   0.0934714823961258,\n",
       "   0.01527431607246399,\n",
       "   -0.3619496524333954,\n",
       "   -0.1724274605512619,\n",
       "   -0.09507370740175247,\n",
       "   -0.16932204365730286,\n",
       "   -0.13111335039138794,\n",
       "   -0.39338287711143494,\n",
       "   -0.552977442741394,\n",
       "   -0.025292985141277313,\n",
       "   0.40649503469467163,\n",
       "   0.09032641351222992,\n",
       "   0.17174050211906433,\n",
       "   0.13950756192207336,\n",
       "   -0.34078145027160645,\n",
       "   -0.08669846504926682,\n",
       "   0.13900066912174225,\n",
       "   0.4264952540397644,\n",
       "   -0.6357011198997498,\n",
       "   0.2565879821777344,\n",
       "   -0.23800040781497955,\n",
       "   0.16183218359947205,\n",
       "   0.24388524889945984,\n",
       "   -0.039872244000434875,\n",
       "   0.5732668042182922,\n",
       "   -0.17869316041469574,\n",
       "   -0.30883878469467163,\n",
       "   0.16188128292560577,\n",
       "   0.2614705264568329,\n",
       "   0.08322931826114655,\n",
       "   -0.36345091462135315,\n",
       "   -0.44759124517440796,\n",
       "   -0.0030071921646595,\n",
       "   -0.22247005999088287,\n",
       "   0.043032243847846985,\n",
       "   0.18310987949371338,\n",
       "   0.5187605023384094,\n",
       "   -0.6175909042358398,\n",
       "   0.35946542024612427,\n",
       "   0.2451017200946808,\n",
       "   0.23091015219688416,\n",
       "   0.08362837135791779,\n",
       "   0.06908148527145386,\n",
       "   -0.1306104212999344,\n",
       "   -0.24879029393196106,\n",
       "   0.2996731698513031,\n",
       "   -0.41831716895103455,\n",
       "   -0.19879525899887085,\n",
       "   -0.07655653357505798,\n",
       "   0.31150394678115845,\n",
       "   0.05329880863428116,\n",
       "   -0.1432035118341446,\n",
       "   -0.4544605612754822,\n",
       "   0.44997337460517883,\n",
       "   -0.12363806366920471,\n",
       "   -0.11600089818239212,\n",
       "   -0.49027758836746216,\n",
       "   0.4913441836833954,\n",
       "   -0.48790866136550903,\n",
       "   -0.1719866693019867,\n",
       "   -0.02073819562792778,\n",
       "   -0.09735159575939178,\n",
       "   -0.060100942850112915,\n",
       "   -0.20680338144302368,\n",
       "   -0.1615496575832367,\n",
       "   0.3298477530479431,\n",
       "   0.37927764654159546,\n",
       "   -0.026785466820001602,\n",
       "   -0.02874581515789032,\n",
       "   0.5901309251785278,\n",
       "   0.11200928688049316,\n",
       "   -0.20354245603084564,\n",
       "   0.018518825992941856,\n",
       "   0.45638442039489746,\n",
       "   0.3196355700492859,\n",
       "   0.2732473313808441,\n",
       "   0.7404160499572754,\n",
       "   0.22738976776599884,\n",
       "   -0.22645625472068787,\n",
       "   0.11364754289388657,\n",
       "   0.1966157704591751,\n",
       "   -0.51454097032547,\n",
       "   -0.02569919265806675,\n",
       "   -0.2851206064224243,\n",
       "   -0.330100953578949,\n",
       "   -0.32183346152305603,\n",
       "   -0.3202173411846161,\n",
       "   0.35393795371055603,\n",
       "   0.06106824800372124,\n",
       "   0.8587263822555542,\n",
       "   -0.07265770435333252,\n",
       "   -0.2960420846939087,\n",
       "   0.2978822886943817,\n",
       "   0.07465620338916779,\n",
       "   0.13059557974338531,\n",
       "   0.09960167109966278,\n",
       "   0.25301653146743774,\n",
       "   -0.3091275990009308,\n",
       "   -0.07268326729536057,\n",
       "   -0.013109331019222736,\n",
       "   0.026924386620521545,\n",
       "   -0.4313250184059143,\n",
       "   -0.09348895400762558,\n",
       "   -0.08733375370502472,\n",
       "   -0.13201627135276794,\n",
       "   -0.171132892370224,\n",
       "   0.22280257940292358,\n",
       "   -0.13218146562576294,\n",
       "   0.6625165343284607,\n",
       "   0.4187995195388794,\n",
       "   0.05763890594244003,\n",
       "   -0.09143870323896408,\n",
       "   -0.04850049316883087,\n",
       "   -0.048836324363946915,\n",
       "   -0.3522735834121704,\n",
       "   0.011073309928178787,\n",
       "   0.37982994318008423,\n",
       "   -0.2128392457962036,\n",
       "   -0.01624857634305954,\n",
       "   0.32269176840782166,\n",
       "   0.13656800985336304,\n",
       "   0.029070649296045303,\n",
       "   0.18946842849254608,\n",
       "   0.17845794558525085,\n",
       "   0.17018678784370422,\n",
       "   -0.6106257438659668,\n",
       "   -0.5298742055892944,\n",
       "   -0.04239760339260101,\n",
       "   -0.1407000869512558,\n",
       "   0.8901748657226562,\n",
       "   0.10680057108402252,\n",
       "   -0.20574583113193512,\n",
       "   0.4974721372127533,\n",
       "   -0.08822893351316452,\n",
       "   0.046275898814201355,\n",
       "   0.3583196997642517,\n",
       "   -0.16294530034065247,\n",
       "   0.6583921313285828,\n",
       "   -0.1615627408027649,\n",
       "   -0.011413995176553726,\n",
       "   -0.16684773564338684,\n",
       "   -0.3905298411846161,\n",
       "   0.15429764986038208,\n",
       "   0.004722781479358673,\n",
       "   0.14956754446029663,\n",
       "   0.1385544240474701,\n",
       "   0.17286668717861176,\n",
       "   -0.29761645197868347,\n",
       "   0.3166879117488861,\n",
       "   0.020758595317602158,\n",
       "   0.39757609367370605,\n",
       "   0.2575247883796692,\n",
       "   0.43607816100120544,\n",
       "   0.18686212599277496,\n",
       "   -0.18917429447174072,\n",
       "   -0.9979379773139954,\n",
       "   0.27248263359069824,\n",
       "   -0.23106597363948822,\n",
       "   -0.13643845915794373,\n",
       "   -0.11473631113767624,\n",
       "   -9.012744903564453,\n",
       "   0.5318707227706909,\n",
       "   -0.07489799708127975,\n",
       "   0.12643791735172272,\n",
       "   -0.31927117705345154,\n",
       "   0.1577761024236679,\n",
       "   -0.36587733030319214,\n",
       "   -0.28250062465667725,\n",
       "   -0.4359094500541687,\n",
       "   0.23797693848609924,\n",
       "   -0.09316699206829071,\n",
       "   -0.015111546963453293,\n",
       "   0.18104493618011475,\n",
       "   -0.24381493031978607,\n",
       "   0.1939016580581665,\n",
       "   -0.2170322686433792,\n",
       "   0.1524011790752411,\n",
       "   -0.22333309054374695,\n",
       "   0.30028992891311646,\n",
       "   -0.2028491497039795,\n",
       "   0.32404041290283203,\n",
       "   0.4477396607398987,\n",
       "   0.1705523133277893,\n",
       "   0.3797563314437866,\n",
       "   -0.17646679282188416,\n",
       "   0.3713650107383728,\n",
       "   -0.1774771809577942,\n",
       "   0.0499463826417923,\n",
       "   0.10881531238555908,\n",
       "   -0.3875492811203003,\n",
       "   -0.3850943446159363,\n",
       "   0.23167461156845093,\n",
       "   -0.09221173822879791,\n",
       "   0.0979943722486496,\n",
       "   0.22706225514411926,\n",
       "   -0.1323329657316208,\n",
       "   0.057732198387384415,\n",
       "   -0.4753904342651367,\n",
       "   0.06873980164527893,\n",
       "   -0.5873453617095947,\n",
       "   -0.4858867824077606,\n",
       "   -0.3095933794975281,\n",
       "   -0.12415565550327301,\n",
       "   0.24487273395061493,\n",
       "   0.4538544714450836,\n",
       "   0.17694687843322754,\n",
       "   -0.04016922786831856,\n",
       "   0.45367780327796936,\n",
       "   0.04983224719762802,\n",
       "   0.2608381509780884,\n",
       "   0.3254074454307556,\n",
       "   0.24420563876628876,\n",
       "   0.3181005120277405,\n",
       "   -0.17165257036685944,\n",
       "   0.07670088112354279,\n",
       "   0.06019159406423569,\n",
       "   0.07876366376876831,\n",
       "   0.35108473896980286,\n",
       "   -0.0628802701830864,\n",
       "   -0.17710934579372406,\n",
       "   0.08651471883058548,\n",
       "   0.20341090857982635,\n",
       "   0.8572871088981628,\n",
       "   0.10273762792348862,\n",
       "   -0.27229297161102295,\n",
       "   0.5857035517692566,\n",
       "   0.05783194303512573,\n",
       "   0.23818239569664001,\n",
       "   0.1272163689136505,\n",
       "   -0.3882474899291992,\n",
       "   -0.3170219659805298,\n",
       "   -0.25692009925842285,\n",
       "   0.1275334358215332,\n",
       "   -0.45262566208839417,\n",
       "   0.3498847186565399,\n",
       "   -0.6254685521125793,\n",
       "   0.08023431897163391,\n",
       "   0.08813554793596268,\n",
       "   -0.17659872770309448,\n",
       "   0.08983950316905975,\n",
       "   -0.4586469829082489,\n",
       "   0.6088521480560303,\n",
       "   0.5694794058799744,\n",
       "   -0.03240639343857765,\n",
       "   0.2102731168270111,\n",
       "   -0.19903351366519928,\n",
       "   -0.21990320086479187,\n",
       "   0.3382953703403473,\n",
       "   0.32159939408302307,\n",
       "   -0.21131375432014465,\n",
       "   -0.3494698107242584,\n",
       "   0.3773777186870575,\n",
       "   -0.05590292811393738,\n",
       "   0.26431453227996826,\n",
       "   0.1250302940607071,\n",
       "   0.5956053733825684,\n",
       "   -0.1550147831439972,\n",
       "   0.11176411807537079,\n",
       "   0.0015483908355236053,\n",
       "   0.008603451773524284,\n",
       "   -0.09948684275150299,\n",
       "   0.0022574253380298615,\n",
       "   -0.38198551535606384,\n",
       "   -0.2612495422363281,\n",
       "   -0.2898063659667969,\n",
       "   -0.06655316054821014,\n",
       "   -0.0792742520570755,\n",
       "   -0.2403060793876648,\n",
       "   0.24676446616649628,\n",
       "   -0.09016139060258865,\n",
       "   -0.26377469301223755,\n",
       "   0.26548904180526733,\n",
       "   0.17124930024147034,\n",
       "   -0.4030730128288269,\n",
       "   0.014071449637413025,\n",
       "   0.17717084288597107,\n",
       "   -0.1104394942522049,\n",
       "   -0.11119477450847626,\n",
       "   0.2531805634498596,\n",
       "   0.02209468185901642,\n",
       "   -0.17217788100242615,\n",
       "   -0.23512467741966248,\n",
       "   -0.01298116147518158,\n",
       "   -0.26364824175834656,\n",
       "   0.17397870123386383,\n",
       "   0.22605352103710175,\n",
       "   0.16334916651248932,\n",
       "   -0.17419730126857758,\n",
       "   -0.022375158965587616,\n",
       "   0.014007216319441795,\n",
       "   0.21922658383846283,\n",
       "   0.542341411113739,\n",
       "   -0.4357748031616211,\n",
       "   0.4426119029521942,\n",
       "   -0.12476159632205963,\n",
       "   -0.10771071910858154,\n",
       "   0.052580736577510834,\n",
       "   -0.13532206416130066,\n",
       "   -0.3215433359146118,\n",
       "   0.39030081033706665,\n",
       "   -0.9367333054542542,\n",
       "   0.02565188705921173,\n",
       "   0.39900732040405273,\n",
       "   0.29873013496398926,\n",
       "   0.41469088196754456,\n",
       "   0.13804122805595398,\n",
       "   -0.27229607105255127,\n",
       "   0.20985597372055054,\n",
       "   0.12815575301647186,\n",
       "   0.1209612488746643,\n",
       "   -0.40189915895462036,\n",
       "   0.3790358901023865,\n",
       "   0.17520998418331146,\n",
       "   0.1611005663871765,\n",
       "   -0.1368800848722458,\n",
       "   -0.4379034638404846,\n",
       "   -0.00880257785320282,\n",
       "   0.16840718686580658,\n",
       "   -0.28789272904396057,\n",
       "   0.07405602931976318,\n",
       "   0.0023680943995714188,\n",
       "   0.11198638379573822,\n",
       "   0.17422376573085785,\n",
       "   -0.20434679090976715,\n",
       "   -0.27330854535102844,\n",
       "   -0.25948506593704224,\n",
       "   -0.12906938791275024,\n",
       "   -0.18747234344482422,\n",
       "   0.17341367900371552,\n",
       "   -0.054815828800201416,\n",
       "   -0.23183166980743408,\n",
       "   -0.0014099306426942348,\n",
       "   -0.18889813125133514,\n",
       "   0.30897796154022217,\n",
       "   0.07258935272693634,\n",
       "   -0.14138102531433105,\n",
       "   -0.2812405228614807,\n",
       "   -0.01857040822505951,\n",
       "   1.08963942527771,\n",
       "   -0.10443127900362015,\n",
       "   0.21196818351745605,\n",
       "   0.23415710031986237,\n",
       "   0.12423442304134369,\n",
       "   -0.4747680425643921,\n",
       "   -0.15677449107170105,\n",
       "   0.25994014739990234,\n",
       "   0.607613205909729,\n",
       "   0.3590202033519745,\n",
       "   -0.38546833395957947,\n",
       "   0.19434194266796112,\n",
       "   0.516679048538208,\n",
       "   0.02785993367433548,\n",
       "   -0.26105666160583496,\n",
       "   0.4319879412651062,\n",
       "   -0.21414577960968018,\n",
       "   0.4434346854686737,\n",
       "   0.08585691452026367,\n",
       "   0.3925277292728424,\n",
       "   -0.21910826861858368,\n",
       "   -0.14944583177566528,\n",
       "   -0.06552644073963165,\n",
       "   0.27253296971321106,\n",
       "   0.21361558139324188,\n",
       "   -0.16300348937511444,\n",
       "   -0.036653175950050354,\n",
       "   0.16648587584495544,\n",
       "   -0.1487254947423935,\n",
       "   0.2167394459247589,\n",
       "   -0.035547953099012375,\n",
       "   0.3411962389945984,\n",
       "   -0.03323618695139885],\n",
       "  [0.20413924753665924,\n",
       "   -0.3568560779094696,\n",
       "   0.12449602782726288,\n",
       "   0.054010991007089615,\n",
       "   0.06376805901527405,\n",
       "   -0.24773801863193512,\n",
       "   0.5897759795188904,\n",
       "   -0.4193081259727478,\n",
       "   -0.34902095794677734,\n",
       "   0.502951979637146,\n",
       "   -0.16014328598976135,\n",
       "   -0.0693819522857666,\n",
       "   -0.24661850929260254,\n",
       "   0.38921618461608887,\n",
       "   -0.5427654385566711,\n",
       "   0.12053613364696503,\n",
       "   -0.06470140814781189,\n",
       "   0.2772042453289032,\n",
       "   0.16415256261825562,\n",
       "   0.15801772475242615,\n",
       "   -0.5916618704795837,\n",
       "   -0.08048633486032486,\n",
       "   -0.13380809128284454,\n",
       "   0.08695024251937866,\n",
       "   0.07232543081045151,\n",
       "   0.09370220452547073,\n",
       "   -0.04333219304680824,\n",
       "   0.1970205008983612,\n",
       "   0.32434841990470886,\n",
       "   0.8598072528839111,\n",
       "   0.17782792448997498,\n",
       "   -0.013364880345761776,\n",
       "   -0.1386842280626297,\n",
       "   0.1317128986120224,\n",
       "   -0.12296147644519806,\n",
       "   0.3470030426979065,\n",
       "   -0.10825292021036148,\n",
       "   0.24402478337287903,\n",
       "   -0.25672659277915955,\n",
       "   0.3196113705635071,\n",
       "   -0.08875980228185654,\n",
       "   -0.18075823783874512,\n",
       "   0.21915344893932343,\n",
       "   -0.48362845182418823,\n",
       "   0.10042299330234528,\n",
       "   -0.2928459942340851,\n",
       "   0.4135139584541321,\n",
       "   -0.006402002647519112,\n",
       "   -0.12705886363983154,\n",
       "   0.37664300203323364,\n",
       "   -0.1964421272277832,\n",
       "   0.3314369320869446,\n",
       "   -0.2768298089504242,\n",
       "   -0.22581464052200317,\n",
       "   0.1539279967546463,\n",
       "   -0.14203989505767822,\n",
       "   0.13676121830940247,\n",
       "   -0.13211321830749512,\n",
       "   -0.2241363227367401,\n",
       "   0.31887608766555786,\n",
       "   -0.5182226896286011,\n",
       "   -0.05533129349350929,\n",
       "   0.2297421246767044,\n",
       "   0.09438195824623108,\n",
       "   -0.029836304485797882,\n",
       "   0.08631891757249832,\n",
       "   -0.12409508228302002,\n",
       "   -0.03973877429962158,\n",
       "   -0.08609969913959503,\n",
       "   0.03943774104118347,\n",
       "   -0.04575200378894806,\n",
       "   0.02687574177980423,\n",
       "   -0.447841614484787,\n",
       "   -0.3651161789894104,\n",
       "   -0.34233981370925903,\n",
       "   -0.28576305508613586,\n",
       "   -0.4527853727340698,\n",
       "   0.16042760014533997,\n",
       "   -0.03481636196374893,\n",
       "   -0.15802136063575745,\n",
       "   -0.003913521766662598,\n",
       "   0.06728540360927582,\n",
       "   0.2044999748468399,\n",
       "   0.14259475469589233,\n",
       "   -0.024848412722349167,\n",
       "   0.156051367521286,\n",
       "   0.020930472761392593,\n",
       "   0.30117926001548767,\n",
       "   0.09652318805456161,\n",
       "   0.42519450187683105,\n",
       "   0.08147688210010529,\n",
       "   -0.1423415094614029,\n",
       "   0.1765812635421753,\n",
       "   0.18009144067764282,\n",
       "   -0.38452598452568054,\n",
       "   -0.02515365183353424,\n",
       "   -0.11571862548589706,\n",
       "   0.030664145946502686,\n",
       "   -0.8927040100097656,\n",
       "   0.13132081925868988,\n",
       "   -0.2145007699728012,\n",
       "   0.4136577546596527,\n",
       "   -0.10435210168361664,\n",
       "   0.40742558240890503,\n",
       "   -0.21160130202770233,\n",
       "   0.3413996696472168,\n",
       "   0.21361543238162994,\n",
       "   -0.20169560611248016,\n",
       "   -0.19992493093013763,\n",
       "   0.5722945332527161,\n",
       "   -0.008428871631622314,\n",
       "   0.31510645151138306,\n",
       "   0.3865639567375183,\n",
       "   -0.07625576853752136,\n",
       "   -0.24153466522693634,\n",
       "   -0.011609118431806564,\n",
       "   -0.24754662811756134,\n",
       "   -0.16888129711151123,\n",
       "   -0.13505122065544128,\n",
       "   0.5064759850502014,\n",
       "   0.2766219675540924,\n",
       "   0.07928931713104248,\n",
       "   0.22615905106067657,\n",
       "   -0.4253874123096466,\n",
       "   0.2295917123556137,\n",
       "   0.3486521244049072,\n",
       "   -0.18475130200386047,\n",
       "   -0.02507733926177025,\n",
       "   -0.23050783574581146,\n",
       "   0.1653260737657547,\n",
       "   -0.20433518290519714,\n",
       "   0.25575944781303406,\n",
       "   0.5871421694755554,\n",
       "   0.10397358983755112,\n",
       "   0.03240273892879486,\n",
       "   0.24689766764640808,\n",
       "   -0.41678041219711304,\n",
       "   0.7314234375953674,\n",
       "   -0.2583545446395874,\n",
       "   -0.36937153339385986,\n",
       "   -0.30445513129234314,\n",
       "   0.1684081256389618,\n",
       "   0.3246764540672302,\n",
       "   0.1035403311252594,\n",
       "   0.11477281153202057,\n",
       "   -0.2844043970108032,\n",
       "   -0.3931368589401245,\n",
       "   0.011565335094928741,\n",
       "   -0.04105657711625099,\n",
       "   0.16180841624736786,\n",
       "   -0.08739468455314636,\n",
       "   0.18896229565143585,\n",
       "   -0.34952589869499207,\n",
       "   0.05521804839372635,\n",
       "   -0.1761164665222168,\n",
       "   -0.326985239982605,\n",
       "   -0.16051174700260162,\n",
       "   0.37652355432510376,\n",
       "   0.548005998134613,\n",
       "   0.28314825892448425,\n",
       "   0.1798436939716339,\n",
       "   -0.16100984811782837,\n",
       "   -0.5059978365898132,\n",
       "   -0.23875518143177032,\n",
       "   0.25182196497917175,\n",
       "   -0.03414882346987724,\n",
       "   -0.2586565613746643,\n",
       "   0.4247260093688965,\n",
       "   -0.0710577592253685,\n",
       "   0.49106040596961975,\n",
       "   0.05745590850710869,\n",
       "   0.058250218629837036,\n",
       "   0.020327866077423096,\n",
       "   0.12551017105579376,\n",
       "   -0.2921035587787628,\n",
       "   0.35960596799850464,\n",
       "   0.13779929280281067,\n",
       "   -0.19401288032531738,\n",
       "   0.045493416488170624,\n",
       "   0.1334555745124817,\n",
       "   -0.00953623466193676,\n",
       "   0.2117251753807068,\n",
       "   -0.34051230549812317,\n",
       "   -0.6508037447929382,\n",
       "   -0.24383190274238586,\n",
       "   0.11267951130867004,\n",
       "   0.10437868535518646,\n",
       "   -0.3260403871536255,\n",
       "   0.04888761788606644,\n",
       "   -0.42781874537467957,\n",
       "   -0.0832991749048233,\n",
       "   -0.02332499623298645,\n",
       "   -0.03078487701714039,\n",
       "   0.04294043406844139,\n",
       "   -0.475334107875824,\n",
       "   0.21344757080078125,\n",
       "   -0.12406432628631592,\n",
       "   0.31512218713760376,\n",
       "   0.1710931956768036,\n",
       "   -0.0406494066119194,\n",
       "   0.16870874166488647,\n",
       "   -0.19998519122600555,\n",
       "   0.20386137068271637,\n",
       "   -0.007402661256492138,\n",
       "   0.06613164395093918,\n",
       "   0.6220704913139343,\n",
       "   -0.22548864781856537,\n",
       "   0.010341022163629532,\n",
       "   -0.027643883600831032,\n",
       "   -0.3119482398033142,\n",
       "   -0.04112834110856056,\n",
       "   -0.41988563537597656,\n",
       "   -0.3629026412963867,\n",
       "   -0.026894809678196907,\n",
       "   0.0473615899682045,\n",
       "   -0.3396607041358948,\n",
       "   -0.034982357174158096,\n",
       "   0.05400906875729561,\n",
       "   -0.46790221333503723,\n",
       "   0.0389196053147316,\n",
       "   0.16825591027736664,\n",
       "   0.06522747129201889,\n",
       "   0.03315419703722,\n",
       "   -0.023890910670161247,\n",
       "   -0.8163592219352722,\n",
       "   0.1024085283279419,\n",
       "   0.05948982387781143,\n",
       "   0.1423839032649994,\n",
       "   0.2367101013660431,\n",
       "   0.1043342649936676,\n",
       "   -0.3486049473285675,\n",
       "   0.1058100014925003,\n",
       "   0.03212355822324753,\n",
       "   -0.03559409826993942,\n",
       "   0.022767728194594383,\n",
       "   -0.2773352265357971,\n",
       "   0.03033795952796936,\n",
       "   -0.08208523690700531,\n",
       "   -0.4185677170753479,\n",
       "   -0.3435911536216736,\n",
       "   0.46280404925346375,\n",
       "   0.48315221071243286,\n",
       "   0.11337746679782867,\n",
       "   0.028936538845300674,\n",
       "   0.14542414247989655,\n",
       "   0.004311498254537582,\n",
       "   -0.3877109885215759,\n",
       "   0.29990094900131226,\n",
       "   0.35847973823547363,\n",
       "   -0.28415966033935547,\n",
       "   0.015583287924528122,\n",
       "   -0.25904858112335205,\n",
       "   -0.004991680383682251,\n",
       "   -0.2889529764652252,\n",
       "   -0.04581134766340256,\n",
       "   -0.06114637851715088,\n",
       "   0.15516090393066406,\n",
       "   0.1408044397830963,\n",
       "   0.2753959894180298,\n",
       "   -0.5157514810562134,\n",
       "   0.4342174232006073,\n",
       "   0.37702521681785583,\n",
       "   0.01559390313923359,\n",
       "   -0.1938229203224182,\n",
       "   -0.06328605115413666,\n",
       "   -0.15428481996059418,\n",
       "   -0.2398671656847,\n",
       "   0.42759549617767334,\n",
       "   -0.4105149507522583,\n",
       "   0.20167846977710724,\n",
       "   -0.058990005403757095,\n",
       "   0.08890827745199203,\n",
       "   -0.07808137685060501,\n",
       "   0.2528347074985504,\n",
       "   0.27163073420524597,\n",
       "   0.5206976532936096,\n",
       "   0.18808747828006744,\n",
       "   -0.45106470584869385,\n",
       "   0.06647670269012451,\n",
       "   -0.14869730174541473,\n",
       "   0.037368856370449066,\n",
       "   0.34710395336151123,\n",
       "   0.09061770141124725,\n",
       "   0.6289491653442383,\n",
       "   -0.5643739700317383,\n",
       "   -0.19217714667320251,\n",
       "   0.11113698035478592,\n",
       "   0.5242507457733154,\n",
       "   -0.1074259877204895,\n",
       "   -0.2080475091934204,\n",
       "   -0.14202062785625458,\n",
       "   0.051321543753147125,\n",
       "   -0.20669201016426086,\n",
       "   0.24216559529304504,\n",
       "   0.5033590197563171,\n",
       "   0.23848509788513184,\n",
       "   -0.08631044626235962,\n",
       "   0.3812201917171478,\n",
       "   -0.46800944209098816,\n",
       "   -0.0876067578792572,\n",
       "   0.11819534748792648,\n",
       "   0.3141050934791565,\n",
       "   -0.015903331339359283,\n",
       "   -0.09388629347085953,\n",
       "   -0.22886976599693298,\n",
       "   0.14701955020427704,\n",
       "   0.20584946870803833,\n",
       "   0.2829587161540985,\n",
       "   -0.24028347432613373,\n",
       "   -0.08887571096420288,\n",
       "   0.31375086307525635,\n",
       "   0.02649727463722229,\n",
       "   -0.2225055694580078,\n",
       "   0.12060508131980896,\n",
       "   0.19975945353507996,\n",
       "   -0.18518716096878052,\n",
       "   -0.16018961369991302,\n",
       "   -0.41504788398742676,\n",
       "   0.26232197880744934,\n",
       "   0.05513101816177368,\n",
       "   0.1706998497247696,\n",
       "   0.12081301212310791,\n",
       "   -0.40144211053848267,\n",
       "   0.546747088432312,\n",
       "   0.1870344877243042,\n",
       "   0.35632529854774475,\n",
       "   -0.023669125512242317,\n",
       "   0.10427086055278778,\n",
       "   -0.17535869777202606,\n",
       "   0.08704525232315063,\n",
       "   0.1980896294116974,\n",
       "   0.2247779369354248,\n",
       "   0.16756229102611542,\n",
       "   -0.5818102359771729,\n",
       "   -0.4519621431827545,\n",
       "   -0.3038359582424164,\n",
       "   0.08357913792133331,\n",
       "   0.6449502110481262,\n",
       "   -0.8334811925888062,\n",
       "   -0.18829864263534546,\n",
       "   0.03740174323320389,\n",
       "   0.12920203804969788,\n",
       "   -0.00519464910030365,\n",
       "   -0.42626455426216125,\n",
       "   0.12813638150691986,\n",
       "   0.21312609314918518,\n",
       "   0.11222296953201294,\n",
       "   0.07903315126895905,\n",
       "   0.18491624295711517,\n",
       "   0.2481619119644165,\n",
       "   -0.3195268511772156,\n",
       "   0.0850493311882019,\n",
       "   0.09663710743188858,\n",
       "   -0.1155182272195816,\n",
       "   0.6483938694000244,\n",
       "   0.21044212579727173,\n",
       "   -0.2650132477283478,\n",
       "   0.05628182366490364,\n",
       "   0.37427279353141785,\n",
       "   -0.09004160016775131,\n",
       "   -0.1311427503824234,\n",
       "   0.0340186208486557,\n",
       "   -0.0014101378619670868,\n",
       "   -0.645471453666687,\n",
       "   -0.49064481258392334,\n",
       "   0.03945160657167435,\n",
       "   -0.26639649271965027,\n",
       "   0.16466118395328522,\n",
       "   -0.021851617842912674,\n",
       "   0.047284964472055435,\n",
       "   0.7948010563850403,\n",
       "   -0.17563843727111816,\n",
       "   -0.13631972670555115,\n",
       "   -0.4299536347389221,\n",
       "   -0.16805975139141083,\n",
       "   -0.0992419570684433,\n",
       "   -0.19902759790420532,\n",
       "   -0.23036926984786987,\n",
       "   0.22935184836387634,\n",
       "   -0.2725226581096649,\n",
       "   0.39323678612709045,\n",
       "   0.23371800780296326,\n",
       "   0.34290581941604614,\n",
       "   -0.32414722442626953,\n",
       "   0.198301300406456,\n",
       "   -0.1941903531551361,\n",
       "   0.1526925265789032,\n",
       "   0.14078180491924286,\n",
       "   0.46011167764663696,\n",
       "   -0.009904373437166214,\n",
       "   0.5689187049865723,\n",
       "   0.013101916760206223,\n",
       "   0.3602549135684967,\n",
       "   0.3600277602672577,\n",
       "   0.00925857201218605,\n",
       "   0.027540579438209534,\n",
       "   -0.419475257396698,\n",
       "   0.4250412583351135,\n",
       "   0.24219761788845062,\n",
       "   -0.3798332214355469,\n",
       "   -0.14216278493404388,\n",
       "   -0.17126908898353577,\n",
       "   0.308023065328598,\n",
       "   -0.39227592945098877,\n",
       "   -0.45599451661109924,\n",
       "   -0.021177485585212708,\n",
       "   -0.3390555679798126,\n",
       "   0.22782836854457855,\n",
       "   0.17317533493041992,\n",
       "   -0.05403808876872063,\n",
       "   0.011919811367988586,\n",
       "   -0.27222761511802673,\n",
       "   -0.16716395318508148,\n",
       "   -0.10849061608314514,\n",
       "   0.17502045631408691,\n",
       "   -0.2726097106933594,\n",
       "   -0.12625928223133087,\n",
       "   -0.28978604078292847,\n",
       "   -0.16771328449249268,\n",
       "   0.3339798152446747,\n",
       "   0.24075156450271606,\n",
       "   -0.22731715440750122,\n",
       "   -0.32676172256469727,\n",
       "   0.06564649939537048,\n",
       "   -0.0975348949432373,\n",
       "   0.35585033893585205,\n",
       "   -0.005295686423778534,\n",
       "   -0.20506499707698822,\n",
       "   -0.3314487934112549,\n",
       "   0.4007318615913391,\n",
       "   0.40134280920028687,\n",
       "   -0.19581493735313416,\n",
       "   0.05052998661994934,\n",
       "   0.08564890921115875,\n",
       "   -0.19177646934986115,\n",
       "   0.04129641130566597,\n",
       "   -0.017527390271425247,\n",
       "   0.22203415632247925,\n",
       "   0.24723511934280396,\n",
       "   -0.14993694424629211,\n",
       "   0.122634656727314,\n",
       "   -0.35518190264701843,\n",
       "   -0.19171187281608582,\n",
       "   -0.4368213415145874,\n",
       "   -0.2659626305103302,\n",
       "   0.20161378383636475,\n",
       "   0.30641335248947144,\n",
       "   -0.25068697333335876,\n",
       "   -0.23162658512592316,\n",
       "   0.07353168725967407,\n",
       "   0.38968032598495483,\n",
       "   -0.24919965863227844,\n",
       "   0.12911094725131989,\n",
       "   -0.15384945273399353,\n",
       "   0.1810169517993927,\n",
       "   0.42398208379745483,\n",
       "   -0.30571699142456055,\n",
       "   -0.31334400177001953,\n",
       "   -0.037433747202157974,\n",
       "   -0.20662203431129456,\n",
       "   -0.3660804331302643,\n",
       "   0.36366504430770874,\n",
       "   0.20085939764976501,\n",
       "   0.11704522371292114,\n",
       "   0.0052789486944675446,\n",
       "   -0.3777421712875366,\n",
       "   0.26421603560447693,\n",
       "   0.5350065231323242,\n",
       "   -0.05167780816555023,\n",
       "   0.07734833657741547,\n",
       "   0.40770474076271057,\n",
       "   0.17531856894493103,\n",
       "   0.5437824130058289,\n",
       "   0.46091213822364807,\n",
       "   0.11989142000675201,\n",
       "   -0.03599374741315842,\n",
       "   -0.21057897806167603,\n",
       "   0.37365198135375977,\n",
       "   -0.6677942276000977,\n",
       "   0.1632332056760788,\n",
       "   0.07735440135002136,\n",
       "   -0.18887239694595337,\n",
       "   -0.11793286353349686,\n",
       "   -0.27148446440696716,\n",
       "   0.19119548797607422,\n",
       "   0.12069325149059296,\n",
       "   0.25791820883750916,\n",
       "   -0.0018990756943821907,\n",
       "   0.09523728489875793,\n",
       "   0.10531581938266754,\n",
       "   -0.28823527693748474,\n",
       "   -0.12673939764499664,\n",
       "   0.21780328452587128,\n",
       "   0.30078816413879395,\n",
       "   -0.1473252773284912,\n",
       "   -0.09554002434015274,\n",
       "   0.08822394907474518,\n",
       "   0.23567664623260498,\n",
       "   -0.26523521542549133,\n",
       "   -0.04557753726840019,\n",
       "   -0.2973255217075348,\n",
       "   0.02609589695930481,\n",
       "   -0.007501829415559769,\n",
       "   -0.10835948586463928,\n",
       "   -0.29483604431152344,\n",
       "   0.6132369041442871,\n",
       "   0.07140575349330902,\n",
       "   -0.25085708498954773,\n",
       "   0.07564729452133179,\n",
       "   -0.1265709102153778,\n",
       "   -0.046422913670539856,\n",
       "   -0.3950924873352051,\n",
       "   0.031644873321056366,\n",
       "   -0.46707800030708313,\n",
       "   0.10601808875799179,\n",
       "   -0.21391147375106812,\n",
       "   -0.24836382269859314,\n",
       "   0.09532560408115387,\n",
       "   -0.04805499315261841,\n",
       "   0.5193487405776978,\n",
       "   0.508939802646637,\n",
       "   -0.054517313838005066,\n",
       "   0.06415535509586334,\n",
       "   -0.1605851650238037,\n",
       "   0.6895292401313782,\n",
       "   -0.30648669600486755,\n",
       "   0.20508459210395813,\n",
       "   0.15800775587558746,\n",
       "   -0.0715644359588623,\n",
       "   -0.0359477698802948,\n",
       "   -0.0648898184299469,\n",
       "   0.1277032494544983,\n",
       "   0.14497193694114685,\n",
       "   -0.035923369228839874,\n",
       "   0.5531874299049377,\n",
       "   0.006970588117837906,\n",
       "   -0.189924418926239,\n",
       "   -0.05781823769211769,\n",
       "   -0.7433050274848938,\n",
       "   -0.07452140748500824,\n",
       "   0.1647225022315979,\n",
       "   0.008282218128442764,\n",
       "   0.050423745065927505,\n",
       "   0.2214844971895218,\n",
       "   -0.012744372710585594,\n",
       "   -0.03967224061489105,\n",
       "   0.0131960678845644,\n",
       "   0.16274124383926392,\n",
       "   0.37912026047706604,\n",
       "   0.06910285353660583,\n",
       "   0.1602601408958435,\n",
       "   -0.007719293236732483,\n",
       "   -0.17578227818012238,\n",
       "   -0.014500908553600311,\n",
       "   -0.0765952318906784,\n",
       "   0.20302985608577728,\n",
       "   -0.5195914506912231,\n",
       "   -9.118025779724121,\n",
       "   0.28347891569137573,\n",
       "   -0.3070468306541443,\n",
       "   0.4747934341430664,\n",
       "   -0.3556436598300934,\n",
       "   0.13476493954658508,\n",
       "   0.30293160676956177,\n",
       "   -0.04679163172841072,\n",
       "   -0.1664494127035141,\n",
       "   -0.0947629064321518,\n",
       "   0.1279289573431015,\n",
       "   -0.07252516597509384,\n",
       "   -0.025974420830607414,\n",
       "   -0.006741061806678772,\n",
       "   0.17865537106990814,\n",
       "   -0.09961655735969543,\n",
       "   0.19130954146385193,\n",
       "   -0.13887126743793488,\n",
       "   -0.27167460322380066,\n",
       "   -0.009677369147539139,\n",
       "   -0.04578521475195885,\n",
       "   -0.335038423538208,\n",
       "   -0.20709840953350067,\n",
       "   0.8308441042900085,\n",
       "   0.030986635014414787,\n",
       "   0.2872360348701477,\n",
       "   0.07659133523702621,\n",
       "   -0.22239431738853455,\n",
       "   -0.11557964235544205,\n",
       "   -0.07059018313884735,\n",
       "   -0.0523584820330143,\n",
       "   0.15311299264431,\n",
       "   -0.14616072177886963,\n",
       "   0.2315066158771515,\n",
       "   0.0697508156299591,\n",
       "   -0.16529937088489532,\n",
       "   -0.09772959351539612,\n",
       "   0.17975687980651855,\n",
       "   -0.019806165248155594,\n",
       "   -0.3082614243030548,\n",
       "   0.20657841861248016,\n",
       "   0.031823188066482544,\n",
       "   -0.07340377569198608,\n",
       "   0.44814831018447876,\n",
       "   -0.06679181754589081,\n",
       "   0.05887158215045929,\n",
       "   0.0989336147904396,\n",
       "   -0.10483048111200333,\n",
       "   0.014865130186080933,\n",
       "   0.03466976806521416,\n",
       "   0.5009469985961914,\n",
       "   0.22170542180538177,\n",
       "   0.6296206116676331,\n",
       "   0.35476410388946533,\n",
       "   0.10546614229679108,\n",
       "   0.20796851813793182,\n",
       "   0.16451579332351685,\n",
       "   0.43887385725975037,\n",
       "   -0.2757897675037384,\n",
       "   -0.6501741409301758,\n",
       "   0.03113829344511032,\n",
       "   -0.010247919708490372,\n",
       "   0.3951868712902069,\n",
       "   0.05085241049528122,\n",
       "   -0.2547387480735779,\n",
       "   0.22206757962703705,\n",
       "   -0.36587095260620117,\n",
       "   -0.07988753914833069,\n",
       "   -0.007169944234192371,\n",
       "   -0.2556035816669464,\n",
       "   -0.5023268461227417,\n",
       "   -0.3941129446029663,\n",
       "   0.26623281836509705,\n",
       "   -0.24248795211315155,\n",
       "   0.07840697467327118,\n",
       "   -0.11499191075563431,\n",
       "   -0.045509107410907745,\n",
       "   0.2730454206466675,\n",
       "   0.11502476036548615,\n",
       "   0.33978793025016785,\n",
       "   -0.6479333639144897,\n",
       "   0.22305575013160706,\n",
       "   0.4331168830394745,\n",
       "   0.03172297403216362,\n",
       "   0.3302573561668396,\n",
       "   -0.20069454610347748,\n",
       "   -0.2277628779411316,\n",
       "   0.030992619693279266,\n",
       "   -0.0555151142179966,\n",
       "   0.3387666344642639,\n",
       "   -0.34400808811187744,\n",
       "   0.18899208307266235,\n",
       "   -0.056187327951192856,\n",
       "   0.22342577576637268,\n",
       "   -0.30415332317352295,\n",
       "   0.3847416937351227,\n",
       "   -0.09401193261146545,\n",
       "   -0.08708983659744263,\n",
       "   -0.012768032029271126,\n",
       "   0.13414403796195984,\n",
       "   0.25615790486335754,\n",
       "   -0.07115158438682556,\n",
       "   0.09551407396793365,\n",
       "   -0.5203419327735901,\n",
       "   -0.4847779870033264,\n",
       "   -0.03024482913315296,\n",
       "   0.022203754633665085,\n",
       "   -0.05235445499420166,\n",
       "   0.24888373911380768,\n",
       "   0.35359013080596924,\n",
       "   -0.18211571872234344,\n",
       "   0.06022220104932785,\n",
       "   0.05252022296190262,\n",
       "   0.002144111320376396,\n",
       "   0.1897173374891281,\n",
       "   0.2827225923538208,\n",
       "   -0.008366318419575691,\n",
       "   -0.42145878076553345,\n",
       "   -0.006511302664875984,\n",
       "   -0.2378809154033661,\n",
       "   0.09491822868585587,\n",
       "   -0.7180010080337524,\n",
       "   0.11016766726970673,\n",
       "   -0.3432789444923401,\n",
       "   -0.05652065947651863,\n",
       "   0.04703931882977486,\n",
       "   0.0064996033906936646,\n",
       "   0.08746618032455444,\n",
       "   0.10795749723911285,\n",
       "   0.036016449332237244,\n",
       "   0.31577378511428833,\n",
       "   0.34196385741233826,\n",
       "   0.5567550659179688,\n",
       "   -0.15163275599479675,\n",
       "   0.12989984452724457,\n",
       "   -0.07308980822563171,\n",
       "   0.15937179327011108,\n",
       "   -0.0859527736902237,\n",
       "   -0.19974114000797272,\n",
       "   0.1664518415927887,\n",
       "   -0.5543859601020813,\n",
       "   -0.04041256010532379,\n",
       "   0.16396914422512054,\n",
       "   0.19336730241775513,\n",
       "   0.357740581035614,\n",
       "   -0.171189546585083,\n",
       "   -0.34950441122055054,\n",
       "   0.35263147950172424,\n",
       "   0.03847857192158699,\n",
       "   -0.16705182194709778,\n",
       "   -0.2523927688598633,\n",
       "   0.11041735857725143,\n",
       "   0.26704809069633484,\n",
       "   6.810575723648071e-05,\n",
       "   0.2974712550640106,\n",
       "   -0.346294641494751,\n",
       "   -0.17280811071395874,\n",
       "   0.348603755235672,\n",
       "   -0.044969938695430756,\n",
       "   0.0630953311920166,\n",
       "   -0.39871084690093994,\n",
       "   -0.08995185792446136,\n",
       "   0.055362701416015625,\n",
       "   -0.5251460075378418,\n",
       "   -0.21295467019081116,\n",
       "   -0.07076636701822281,\n",
       "   -0.2876065969467163,\n",
       "   -0.42501890659332275,\n",
       "   0.1323309689760208,\n",
       "   0.1797022968530655,\n",
       "   0.3321036696434021,\n",
       "   0.01939324289560318,\n",
       "   0.3076454997062683,\n",
       "   -0.049907930195331573,\n",
       "   0.18875804543495178,\n",
       "   -0.059084221720695496,\n",
       "   -0.06327172368764877,\n",
       "   -0.3218719959259033,\n",
       "   0.31719183921813965,\n",
       "   -0.06633516401052475,\n",
       "   0.09714193642139435,\n",
       "   0.6615357398986816,\n",
       "   0.07924569398164749,\n",
       "   -0.4919227957725525,\n",
       "   -0.330605149269104,\n",
       "   0.25160664319992065,\n",
       "   0.5288069248199463,\n",
       "   0.19661390781402588,\n",
       "   -0.10171951353549957,\n",
       "   0.005397185683250427,\n",
       "   0.3191368579864502,\n",
       "   0.2212260514497757,\n",
       "   -0.08569322526454926,\n",
       "   -0.05345262214541435,\n",
       "   0.17569640278816223,\n",
       "   0.16806448996067047,\n",
       "   0.11552317440509796,\n",
       "   0.4791201949119568,\n",
       "   -0.27732837200164795,\n",
       "   -0.20398294925689697,\n",
       "   -0.24475008249282837,\n",
       "   0.4472591280937195,\n",
       "   0.05991986766457558,\n",
       "   0.14667093753814697,\n",
       "   -0.02295646071434021,\n",
       "   -0.015726886689662933,\n",
       "   -0.11855394393205643,\n",
       "   0.05611760914325714,\n",
       "   -0.025354892015457153,\n",
       "   0.16087913513183594,\n",
       "   0.23543061316013336],\n",
       "  [0.4380304515361786,\n",
       "   0.21090039610862732,\n",
       "   -0.052670568227767944,\n",
       "   -0.12622539699077606,\n",
       "   -0.0587451234459877,\n",
       "   0.05492723733186722,\n",
       "   0.3202916383743286,\n",
       "   0.19914421439170837,\n",
       "   -0.17336885631084442,\n",
       "   -0.0004860237240791321,\n",
       "   -0.161742702126503,\n",
       "   -0.09440384805202484,\n",
       "   -0.1904532015323639,\n",
       "   0.22495292127132416,\n",
       "   -0.36981427669525146,\n",
       "   -0.023018307983875275,\n",
       "   0.31124621629714966,\n",
       "   -0.048478864133358,\n",
       "   0.05093821883201599,\n",
       "   -0.03317873552441597,\n",
       "   -0.24663212895393372,\n",
       "   -0.18018198013305664,\n",
       "   -0.3680281937122345,\n",
       "   0.1983022391796112,\n",
       "   -0.2498982697725296,\n",
       "   -0.03530954569578171,\n",
       "   0.2543594241142273,\n",
       "   0.24461962282657623,\n",
       "   -0.0007478445768356323,\n",
       "   0.5119880437850952,\n",
       "   -0.12161281704902649,\n",
       "   -0.34501519799232483,\n",
       "   0.23305511474609375,\n",
       "   0.14886094629764557,\n",
       "   0.2391655147075653,\n",
       "   -0.04996977746486664,\n",
       "   -0.07122334092855453,\n",
       "   0.07648073136806488,\n",
       "   -0.20264548063278198,\n",
       "   0.23693010210990906,\n",
       "   0.012702006846666336,\n",
       "   -0.021763939410448074,\n",
       "   0.10153056681156158,\n",
       "   0.06759737432003021,\n",
       "   0.6030600666999817,\n",
       "   -0.29588231444358826,\n",
       "   -0.10587034374475479,\n",
       "   -0.007551843300461769,\n",
       "   -0.1861165463924408,\n",
       "   0.1991730034351349,\n",
       "   -0.25504016876220703,\n",
       "   0.07514617592096329,\n",
       "   -0.36273831129074097,\n",
       "   -0.00914494227617979,\n",
       "   0.17082448303699493,\n",
       "   -0.17435839772224426,\n",
       "   0.14619259536266327,\n",
       "   -0.04109565168619156,\n",
       "   -0.21357128024101257,\n",
       "   0.054613351821899414,\n",
       "   -0.27208834886550903,\n",
       "   0.13409675657749176,\n",
       "   0.22321298718452454,\n",
       "   0.03171560913324356,\n",
       "   -0.279173344373703,\n",
       "   -0.20939378440380096,\n",
       "   -0.0077777523547410965,\n",
       "   0.20247095823287964,\n",
       "   -0.19757118821144104,\n",
       "   -0.04267413541674614,\n",
       "   -0.11777575314044952,\n",
       "   -0.1332433819770813,\n",
       "   -0.03235364705324173,\n",
       "   0.018398825079202652,\n",
       "   0.2175949513912201,\n",
       "   -0.30084359645843506,\n",
       "   0.1371454894542694,\n",
       "   -0.19882237911224365,\n",
       "   0.050969481468200684,\n",
       "   0.09462478011846542,\n",
       "   0.3047657310962677,\n",
       "   0.33934926986694336,\n",
       "   -0.01948031410574913,\n",
       "   -0.07364854961633682,\n",
       "   -0.05286882072687149,\n",
       "   -0.12930609285831451,\n",
       "   -0.004092101007699966,\n",
       "   0.1394147425889969,\n",
       "   0.2145254760980606,\n",
       "   0.1787693351507187,\n",
       "   0.20739424228668213,\n",
       "   -0.27849841117858887,\n",
       "   0.017806706950068474,\n",
       "   -0.15293093025684357,\n",
       "   -0.41025322675704956,\n",
       "   -0.38746726512908936,\n",
       "   -0.04002240672707558,\n",
       "   -0.10330231487751007,\n",
       "   -0.08789043128490448,\n",
       "   0.21928671002388,\n",
       "   0.16402201354503632,\n",
       "   -0.06744542717933655,\n",
       "   -0.03665168583393097,\n",
       "   -0.03238905966281891,\n",
       "   0.06484410911798477,\n",
       "   0.37366095185279846,\n",
       "   0.019249960780143738,\n",
       "   -0.4348294734954834,\n",
       "   -0.2717999219894409,\n",
       "   0.48058050870895386,\n",
       "   0.2945427894592285,\n",
       "   -0.15215347707271576,\n",
       "   0.25192147493362427,\n",
       "   0.07214255630970001,\n",
       "   -0.06667058169841766,\n",
       "   -0.10644858330488205,\n",
       "   -0.22674481570720673,\n",
       "   -0.04689236730337143,\n",
       "   -0.16510480642318726,\n",
       "   0.0065456535667181015,\n",
       "   0.42086783051490784,\n",
       "   0.14052611589431763,\n",
       "   0.38085126876831055,\n",
       "   0.04547519236803055,\n",
       "   0.28689050674438477,\n",
       "   0.19731321930885315,\n",
       "   0.004574582912027836,\n",
       "   -0.15157592296600342,\n",
       "   -0.13122953474521637,\n",
       "   0.16861294209957123,\n",
       "   -0.3945450782775879,\n",
       "   -0.08477743715047836,\n",
       "   0.2322731465101242,\n",
       "   0.020449034869670868,\n",
       "   0.03927692770957947,\n",
       "   0.06817284226417542,\n",
       "   -0.5179182887077332,\n",
       "   0.33212822675704956,\n",
       "   -0.8057745099067688,\n",
       "   -0.015311665832996368,\n",
       "   -0.05798112601041794,\n",
       "   0.15777933597564697,\n",
       "   0.22208179533481598,\n",
       "   -0.008002668619155884,\n",
       "   -0.008231004700064659,\n",
       "   -0.1637251079082489,\n",
       "   -0.45709115266799927,\n",
       "   -0.2643117308616638,\n",
       "   0.3087526559829712,\n",
       "   0.18788652122020721,\n",
       "   -0.40488168597221375,\n",
       "   -0.06350306421518326,\n",
       "   -0.039999425411224365,\n",
       "   0.19185864925384521,\n",
       "   -0.039502013474702835,\n",
       "   0.03195404261350632,\n",
       "   -0.08923648297786713,\n",
       "   0.16681404411792755,\n",
       "   0.3545745313167572,\n",
       "   0.18125852942466736,\n",
       "   -0.14237523078918457,\n",
       "   -0.4386007785797119,\n",
       "   0.027754060924053192,\n",
       "   -0.06387440860271454,\n",
       "   0.05645030736923218,\n",
       "   0.36928629875183105,\n",
       "   -0.2795342803001404,\n",
       "   0.06912174820899963,\n",
       "   0.14247485995292664,\n",
       "   0.1947825849056244,\n",
       "   0.3089935779571533,\n",
       "   -0.05117259919643402,\n",
       "   -0.1679862141609192,\n",
       "   0.18936964869499207,\n",
       "   -0.10407848656177521,\n",
       "   0.2087307870388031,\n",
       "   0.024733133614063263,\n",
       "   -0.38827353715896606,\n",
       "   -0.035601697862148285,\n",
       "   0.15831777453422546,\n",
       "   0.16325171291828156,\n",
       "   -0.011504270136356354,\n",
       "   -0.055208370089530945,\n",
       "   -0.3699217736721039,\n",
       "   -0.10951157659292221,\n",
       "   -0.3616715967655182,\n",
       "   0.2034863829612732,\n",
       "   -0.399466335773468,\n",
       "   -0.002219792455434799,\n",
       "   -0.20102640986442566,\n",
       "   0.0327509380877018,\n",
       "   -0.02984604239463806,\n",
       "   -0.112245112657547,\n",
       "   0.033744677901268005,\n",
       "   -0.35441166162490845,\n",
       "   -0.03676115348935127,\n",
       "   0.06937791407108307,\n",
       "   0.34447306394577026,\n",
       "   0.25859349966049194,\n",
       "   0.15039129555225372,\n",
       "   -0.3442443907260895,\n",
       "   -0.08351227641105652,\n",
       "   0.17500148713588715,\n",
       "   -0.0077534993179142475,\n",
       "   0.11199815571308136,\n",
       "   0.20388562977313995,\n",
       "   0.01010415330529213,\n",
       "   -0.05796469748020172,\n",
       "   -0.08703495562076569,\n",
       "   -0.11421166360378265,\n",
       "   0.29567834734916687,\n",
       "   -0.10118894279003143,\n",
       "   0.09944601356983185,\n",
       "   0.2533281147480011,\n",
       "   -0.11213904619216919,\n",
       "   -0.42625677585601807,\n",
       "   0.19721688330173492,\n",
       "   -0.251058429479599,\n",
       "   -0.38164523243904114,\n",
       "   -0.20430414378643036,\n",
       "   0.2432873398065567,\n",
       "   -0.11683918535709381,\n",
       "   0.4173729419708252,\n",
       "   -0.15499408543109894,\n",
       "   -0.3758143484592438,\n",
       "   0.21921251714229584,\n",
       "   0.3188774287700653,\n",
       "   0.40734097361564636,\n",
       "   0.17026227712631226,\n",
       "   -0.06188393384218216,\n",
       "   0.18801596760749817,\n",
       "   0.13301625847816467,\n",
       "   0.08371838927268982,\n",
       "   -0.3115854263305664,\n",
       "   0.15769754350185394,\n",
       "   -0.39442744851112366,\n",
       "   -0.1828216314315796,\n",
       "   0.10773356258869171,\n",
       "   -0.30379223823547363,\n",
       "   -0.1774568408727646,\n",
       "   0.25191453099250793,\n",
       "   0.20117561519145966,\n",
       "   0.061101820319890976,\n",
       "   -0.022105108946561813,\n",
       "   -0.18423785269260406,\n",
       "   -0.01032939925789833,\n",
       "   -0.21459828317165375,\n",
       "   0.45014628767967224,\n",
       "   0.7791002988815308,\n",
       "   0.04754434525966644,\n",
       "   -0.037919919937849045,\n",
       "   -0.4102312922477722,\n",
       "   0.28360632061958313,\n",
       "   -0.30398595333099365,\n",
       "   0.12932907044887543,\n",
       "   -0.08981727808713913,\n",
       "   0.29589220881462097,\n",
       "   -0.05062790960073471,\n",
       "   0.10319333523511887,\n",
       "   -0.12104997038841248,\n",
       "   0.2321396768093109,\n",
       "   0.16863080859184265,\n",
       "   0.21620671451091766,\n",
       "   0.2343050241470337,\n",
       "   -0.14859209954738617,\n",
       "   -0.34676653146743774,\n",
       "   -0.0045609548687934875,\n",
       "   0.08379527926445007,\n",
       "   -0.35075628757476807,\n",
       "   -0.19479317963123322,\n",
       "   -0.06932725012302399,\n",
       "   -0.11023740470409393,\n",
       "   0.18135812878608704,\n",
       "   0.44031333923339844,\n",
       "   0.3217200040817261,\n",
       "   0.00839812308549881,\n",
       "   0.3061652183532715,\n",
       "   -0.4940994679927826,\n",
       "   0.03551322966814041,\n",
       "   -0.31654539704322815,\n",
       "   0.11276303231716156,\n",
       "   0.03652319684624672,\n",
       "   0.18995344638824463,\n",
       "   0.43970319628715515,\n",
       "   -0.23830074071884155,\n",
       "   -0.20870976150035858,\n",
       "   0.15968337655067444,\n",
       "   0.11831549555063248,\n",
       "   0.04039257764816284,\n",
       "   -0.21522068977355957,\n",
       "   0.028479760512709618,\n",
       "   0.18114078044891357,\n",
       "   -0.2429761290550232,\n",
       "   0.03136279061436653,\n",
       "   0.29554370045661926,\n",
       "   0.16462908685207367,\n",
       "   -0.1199546530842781,\n",
       "   0.09964807331562042,\n",
       "   -0.025733299553394318,\n",
       "   -0.16909322142601013,\n",
       "   0.037822119891643524,\n",
       "   0.005222201347351074,\n",
       "   0.026718027889728546,\n",
       "   -0.05419605225324631,\n",
       "   -0.2698011100292206,\n",
       "   -0.06399297714233398,\n",
       "   0.006289191544055939,\n",
       "   0.35960912704467773,\n",
       "   0.18837006390094757,\n",
       "   0.16775909066200256,\n",
       "   0.048630792647600174,\n",
       "   -0.09324589371681213,\n",
       "   -0.07806337624788284,\n",
       "   -0.05535028502345085,\n",
       "   -0.2879191040992737,\n",
       "   -0.33686792850494385,\n",
       "   -0.42951852083206177,\n",
       "   -0.14738108217716217,\n",
       "   -0.14600461721420288,\n",
       "   -0.018825475126504898,\n",
       "   0.2905735373497009,\n",
       "   0.1037764623761177,\n",
       "   -0.4138586223125458,\n",
       "   0.45998767018318176,\n",
       "   0.0158083438873291,\n",
       "   0.4257696568965912,\n",
       "   -0.22173738479614258,\n",
       "   0.25761616230010986,\n",
       "   0.14323246479034424,\n",
       "   -0.16359521448612213,\n",
       "   -0.31659263372421265,\n",
       "   -0.16561713814735413,\n",
       "   0.1661924123764038,\n",
       "   0.04196120798587799,\n",
       "   -0.4118790030479431,\n",
       "   -0.27600276470184326,\n",
       "   0.17035046219825745,\n",
       "   0.6534945964813232,\n",
       "   -0.6824845671653748,\n",
       "   -0.13542461395263672,\n",
       "   0.08721188455820084,\n",
       "   0.27125638723373413,\n",
       "   0.20926232635974884,\n",
       "   -0.12083925306797028,\n",
       "   -0.15765970945358276,\n",
       "   0.03232947364449501,\n",
       "   0.2601531147956848,\n",
       "   0.05075002461671829,\n",
       "   0.08390246331691742,\n",
       "   0.014329099096357822,\n",
       "   -0.21115636825561523,\n",
       "   -0.13296367228031158,\n",
       "   0.029654324054718018,\n",
       "   -0.3067528307437897,\n",
       "   0.3503211438655853,\n",
       "   -0.13680562376976013,\n",
       "   -0.003611339256167412,\n",
       "   -0.12180429697036743,\n",
       "   0.1505786031484604,\n",
       "   0.11526431143283844,\n",
       "   0.29670464992523193,\n",
       "   -0.15221059322357178,\n",
       "   -0.20553641021251678,\n",
       "   -0.06789255887269974,\n",
       "   -0.11760514974594116,\n",
       "   0.0012065693736076355,\n",
       "   0.026814686134457588,\n",
       "   0.6970221400260925,\n",
       "   -0.19296197593212128,\n",
       "   -0.20109018683433533,\n",
       "   0.5711535811424255,\n",
       "   0.3741532862186432,\n",
       "   -0.035538528114557266,\n",
       "   -0.2838599681854248,\n",
       "   0.13142026960849762,\n",
       "   -0.22025088965892792,\n",
       "   -0.06158144399523735,\n",
       "   0.007960706949234009,\n",
       "   -0.27946799993515015,\n",
       "   -0.2727736234664917,\n",
       "   -0.001176491379737854,\n",
       "   0.24786250293254852,\n",
       "   0.21426545083522797,\n",
       "   -0.1894182711839676,\n",
       "   0.05922742187976837,\n",
       "   0.10297946631908417,\n",
       "   -0.07828813046216965,\n",
       "   -0.06953828036785126,\n",
       "   0.07679551094770432,\n",
       "   -0.0024041123688220978,\n",
       "   -0.4432339370250702,\n",
       "   0.08208046853542328,\n",
       "   0.3659500181674957,\n",
       "   -0.05342324450612068,\n",
       "   -0.13028541207313538,\n",
       "   -0.05336273834109306,\n",
       "   -0.40387290716171265,\n",
       "   -0.05483833700418472,\n",
       "   0.18836136162281036,\n",
       "   -0.39290398359298706,\n",
       "   0.0501931793987751,\n",
       "   0.05341622233390808,\n",
       "   0.02955448627471924,\n",
       "   -0.1148831769824028,\n",
       "   -0.24069184064865112,\n",
       "   -0.42196303606033325,\n",
       "   0.041309576481580734,\n",
       "   -0.04805581644177437,\n",
       "   0.07495458424091339,\n",
       "   0.24770759046077728,\n",
       "   -0.18263307213783264,\n",
       "   -0.24363744258880615,\n",
       "   -0.13797688484191895,\n",
       "   -0.5324183702468872,\n",
       "   -0.0017106607556343079,\n",
       "   -0.47825098037719727,\n",
       "   0.07046850025653839,\n",
       "   -0.17923952639102936,\n",
       "   0.13301271200180054,\n",
       "   0.14417552947998047,\n",
       "   0.19596049189567566,\n",
       "   0.2858143746852875,\n",
       "   0.2589791417121887,\n",
       "   -0.40215998888015747,\n",
       "   -0.12183120846748352,\n",
       "   0.33623355627059937,\n",
       "   0.17743755877017975,\n",
       "   -0.10306409746408463,\n",
       "   -0.04938945174217224,\n",
       "   0.1297295242547989,\n",
       "   -0.07298316061496735,\n",
       "   0.018082281574606895,\n",
       "   -0.12282034754753113,\n",
       "   0.05954312905669212,\n",
       "   -0.27247869968414307,\n",
       "   0.15533828735351562,\n",
       "   0.13148349523544312,\n",
       "   0.07626058161258698,\n",
       "   0.06830254197120667,\n",
       "   0.14919278025627136,\n",
       "   -0.08339181542396545,\n",
       "   -0.3364963233470917,\n",
       "   0.03880247473716736,\n",
       "   -0.15943820774555206,\n",
       "   0.15880027413368225,\n",
       "   -0.046206794679164886,\n",
       "   0.2467665821313858,\n",
       "   0.07561299949884415,\n",
       "   0.18306389451026917,\n",
       "   -0.03845822811126709,\n",
       "   0.0752282440662384,\n",
       "   -0.4264800250530243,\n",
       "   0.5815994143486023,\n",
       "   -0.2399565875530243,\n",
       "   0.4808430075645447,\n",
       "   -0.29133719205856323,\n",
       "   -0.20187908411026,\n",
       "   -0.12091860920190811,\n",
       "   -0.2218814343214035,\n",
       "   -0.0337996631860733,\n",
       "   -0.3299068808555603,\n",
       "   0.11025433242321014,\n",
       "   0.11386315524578094,\n",
       "   0.0398026704788208,\n",
       "   0.005319550633430481,\n",
       "   0.13600942492485046,\n",
       "   0.6356155872344971,\n",
       "   0.0975881889462471,\n",
       "   0.00984938070178032,\n",
       "   0.0550069622695446,\n",
       "   0.052209071815013885,\n",
       "   0.24203141033649445,\n",
       "   0.30488133430480957,\n",
       "   0.48102501034736633,\n",
       "   0.3098185360431671,\n",
       "   -0.3901372253894806,\n",
       "   -0.1261458694934845,\n",
       "   0.1372225433588028,\n",
       "   -0.326910138130188,\n",
       "   0.1829024851322174,\n",
       "   0.17920435965061188,\n",
       "   0.08139146119356155,\n",
       "   -0.3141728341579437,\n",
       "   0.09026987105607986,\n",
       "   0.31507912278175354,\n",
       "   0.1326594054698944,\n",
       "   0.1770327240228653,\n",
       "   0.07696729153394699,\n",
       "   0.08245822787284851,\n",
       "   -0.023447100073099136,\n",
       "   -0.1906282603740692,\n",
       "   0.23024235665798187,\n",
       "   0.3259516954421997,\n",
       "   0.13202348351478577,\n",
       "   0.25641068816185,\n",
       "   -0.01114480011165142,\n",
       "   -0.08377447724342346,\n",
       "   -0.05975186452269554,\n",
       "   0.04745220020413399,\n",
       "   0.2101917564868927,\n",
       "   0.0762118399143219,\n",
       "   0.1798326075077057,\n",
       "   0.1168338879942894,\n",
       "   0.26468297839164734,\n",
       "   -0.08653197437524796,\n",
       "   0.4500013291835785,\n",
       "   0.2586759030818939,\n",
       "   0.0017352160066366196,\n",
       "   0.07030817866325378,\n",
       "   -0.08616583049297333,\n",
       "   0.08902295678853989,\n",
       "   -0.4497078061103821,\n",
       "   -0.0396035835146904,\n",
       "   -0.29388701915740967,\n",
       "   -0.05764046311378479,\n",
       "   -0.278936505317688,\n",
       "   0.17882320284843445,\n",
       "   0.11600080132484436,\n",
       "   -0.15016406774520874,\n",
       "   0.3212971091270447,\n",
       "   0.030591586604714394,\n",
       "   0.3201100826263428,\n",
       "   -0.2542576193809509,\n",
       "   -0.32755720615386963,\n",
       "   0.1803496927022934,\n",
       "   -0.338163286447525,\n",
       "   0.2838674783706665,\n",
       "   -0.2318229377269745,\n",
       "   -0.1901935636997223,\n",
       "   0.24319855868816376,\n",
       "   0.06920728832483292,\n",
       "   0.2447897344827652,\n",
       "   0.44464588165283203,\n",
       "   -0.2517566680908203,\n",
       "   0.2790299952030182,\n",
       "   -0.07325579226016998,\n",
       "   -0.02892381325364113,\n",
       "   0.09787844866514206,\n",
       "   -0.10459628701210022,\n",
       "   -0.19441714882850647,\n",
       "   0.25143277645111084,\n",
       "   -0.12311789393424988,\n",
       "   0.08852344751358032,\n",
       "   -0.030952483415603638,\n",
       "   -0.39934107661247253,\n",
       "   0.40472936630249023,\n",
       "   0.12288497388362885,\n",
       "   0.45952683687210083,\n",
       "   0.4476197063922882,\n",
       "   0.20812706649303436,\n",
       "   0.046648476272821426,\n",
       "   -0.19744455814361572,\n",
       "   -0.4105338156223297,\n",
       "   0.12212584167718887,\n",
       "   0.16474954783916473,\n",
       "   0.14503319561481476,\n",
       "   -0.34454262256622314,\n",
       "   -9.322747230529785,\n",
       "   0.24078281223773956,\n",
       "   -0.44471275806427,\n",
       "   0.39564138650894165,\n",
       "   -0.3840837776660919,\n",
       "   0.036498285830020905,\n",
       "   -0.05595600605010986,\n",
       "   -0.03218662366271019,\n",
       "   -0.03099474497139454,\n",
       "   0.10308723896741867,\n",
       "   0.09320282936096191,\n",
       "   0.1578977257013321,\n",
       "   0.20670777559280396,\n",
       "   0.016843508929014206,\n",
       "   -0.12958037853240967,\n",
       "   -0.10823740065097809,\n",
       "   0.29996100068092346,\n",
       "   -0.48483312129974365,\n",
       "   0.11482155323028564,\n",
       "   0.021416133269667625,\n",
       "   0.029623016715049744,\n",
       "   -0.28744199872016907,\n",
       "   -0.09994493424892426,\n",
       "   0.28398245573043823,\n",
       "   -0.1782420426607132,\n",
       "   -0.019584007561206818,\n",
       "   0.14008811116218567,\n",
       "   -0.01783730462193489,\n",
       "   0.44972556829452515,\n",
       "   0.016170701012015343,\n",
       "   0.1466299593448639,\n",
       "   -0.13026858866214752,\n",
       "   -0.33585965633392334,\n",
       "   0.1471477448940277,\n",
       "   0.1185544952750206,\n",
       "   -0.1868990808725357,\n",
       "   0.1187785267829895,\n",
       "   -0.6414977312088013,\n",
       "   0.6867643594741821,\n",
       "   -0.39368394017219543,\n",
       "   0.046809740364551544,\n",
       "   -0.17705664038658142,\n",
       "   0.0055230408906936646,\n",
       "   0.05874690040946007,\n",
       "   0.20094257593154907,\n",
       "   0.06209634244441986,\n",
       "   -0.02483529970049858,\n",
       "   0.05178941413760185,\n",
       "   -0.04807141423225403,\n",
       "   0.43067386746406555,\n",
       "   0.13396836817264557,\n",
       "   -0.15484291315078735,\n",
       "   0.014185985550284386,\n",
       "   -0.1323527842760086,\n",
       "   -0.24366723001003265,\n",
       "   0.0006667058914899826,\n",
       "   -0.0028250589966773987,\n",
       "   0.09867925196886063,\n",
       "   0.25335171818733215,\n",
       "   -0.3747820556163788,\n",
       "   -0.009525638073682785,\n",
       "   0.5556349754333496,\n",
       "   0.6756356954574585,\n",
       "   -0.0055144429206848145,\n",
       "   -0.16983279585838318,\n",
       "   0.5373855233192444,\n",
       "   -0.23973245918750763,\n",
       "   0.04505375772714615,\n",
       "   0.007625310681760311,\n",
       "   0.28085607290267944,\n",
       "   -0.21621116995811462,\n",
       "   -0.15523937344551086,\n",
       "   0.16535785794258118,\n",
       "   -0.187611922621727,\n",
       "   -0.07248160988092422,\n",
       "   -0.3388785123825073,\n",
       "   0.037574395537376404,\n",
       "   0.11687029898166656,\n",
       "   -0.025732003152370453,\n",
       "   0.35235458612442017,\n",
       "   -0.11610656976699829,\n",
       "   0.13035614788532257,\n",
       "   0.47674688696861267,\n",
       "   -0.544294536113739,\n",
       "   0.018883079290390015,\n",
       "   0.20235416293144226,\n",
       "   -0.15825772285461426,\n",
       "   0.04982096701860428,\n",
       "   -0.3555745780467987,\n",
       "   0.208181694149971,\n",
       "   -0.4992436468601227,\n",
       "   0.1208297535777092,\n",
       "   0.11671970039606094,\n",
       "   -0.047998975962400436,\n",
       "   -0.008167129009962082,\n",
       "   0.413565456867218,\n",
       "   0.004664316773414612,\n",
       "   -0.08934324979782104,\n",
       "   0.1742316484451294,\n",
       "   0.14946308732032776,\n",
       "   -0.16346964240074158,\n",
       "   0.20827560126781464,\n",
       "   0.05947350338101387,\n",
       "   -0.40804323554039,\n",
       "   0.27301010489463806,\n",
       "   0.28856804966926575,\n",
       "   0.2190588414669037,\n",
       "   -0.02791528031229973,\n",
       "   0.03552819788455963,\n",
       "   -0.34404483437538147,\n",
       "   -0.15635693073272705,\n",
       "   0.03787929564714432,\n",
       "   0.2769809067249298,\n",
       "   0.12806951999664307,\n",
       "   -0.07923868298530579,\n",
       "   0.22679230570793152,\n",
       "   0.15942803025245667,\n",
       "   0.2387419193983078,\n",
       "   0.18281617760658264,\n",
       "   -0.17555192112922668,\n",
       "   0.2552703320980072,\n",
       "   -0.49288421869277954,\n",
       "   0.2391616702079773,\n",
       "   -0.015249333344399929,\n",
       "   -0.048016924411058426,\n",
       "   0.2303663045167923,\n",
       "   -0.02786581963300705,\n",
       "   0.2692216634750366,\n",
       "   0.08893634378910065,\n",
       "   -0.13743874430656433,\n",
       "   0.09915430843830109,\n",
       "   0.43327245116233826,\n",
       "   -0.2160198837518692,\n",
       "   0.10483354330062866,\n",
       "   -0.04306716471910477,\n",
       "   0.10380138456821442,\n",
       "   -0.4468037784099579,\n",
       "   0.01953260228037834,\n",
       "   0.14524556696414948,\n",
       "   0.4038349986076355,\n",
       "   -0.2994629442691803,\n",
       "   0.09826608002185822,\n",
       "   0.07292855530977249,\n",
       "   -0.10378304868936539,\n",
       "   0.3837405741214752,\n",
       "   -0.167281374335289,\n",
       "   -0.16628295183181763,\n",
       "   0.10470833629369736,\n",
       "   0.35058870911598206,\n",
       "   0.02915998175740242,\n",
       "   -0.13187578320503235,\n",
       "   0.31319671869277954,\n",
       "   0.41711047291755676,\n",
       "   -0.11177972704172134,\n",
       "   0.13979648053646088,\n",
       "   -0.3176712393760681,\n",
       "   0.07381892204284668,\n",
       "   -0.195560485124588,\n",
       "   0.015735521912574768,\n",
       "   0.19369162619113922,\n",
       "   -0.22244425117969513,\n",
       "   0.04331902787089348,\n",
       "   -0.24917800724506378,\n",
       "   0.047787293791770935,\n",
       "   0.018834102898836136,\n",
       "   -0.08522088825702667,\n",
       "   0.12228278815746307,\n",
       "   -0.28362971544265747,\n",
       "   0.040887124836444855,\n",
       "   -0.32678526639938354,\n",
       "   -0.166325181722641,\n",
       "   0.20252454280853271,\n",
       "   -0.038067568093538284,\n",
       "   -0.07733113318681717,\n",
       "   -0.14201408624649048,\n",
       "   -0.16003642976284027,\n",
       "   -0.15139302611351013,\n",
       "   -0.18066734075546265,\n",
       "   0.3577120900154114,\n",
       "   -0.06347157061100006,\n",
       "   0.18099789321422577,\n",
       "   0.3026465177536011,\n",
       "   -0.05101539194583893,\n",
       "   -0.4416082203388214,\n",
       "   -0.2733086347579956,\n",
       "   0.06614183634519577,\n",
       "   0.10400614142417908,\n",
       "   0.0020873285830020905,\n",
       "   0.1986571103334427,\n",
       "   0.07123616337776184,\n",
       "   0.10624600201845169,\n",
       "   -0.4558655023574829,\n",
       "   -0.13405241072177887,\n",
       "   0.056323859840631485,\n",
       "   0.05211488902568817,\n",
       "   0.41609203815460205,\n",
       "   0.11906857788562775,\n",
       "   0.37769970297813416,\n",
       "   -0.03881402686238289,\n",
       "   0.08613448590040207,\n",
       "   -0.0031885728240013123,\n",
       "   0.1740264892578125,\n",
       "   0.00782101135700941,\n",
       "   0.07826609164476395,\n",
       "   -0.10115710645914078,\n",
       "   0.03784390538930893,\n",
       "   -0.15570521354675293,\n",
       "   -0.4175284504890442,\n",
       "   0.07057565450668335,\n",
       "   0.0355537086725235,\n",
       "   0.11988192796707153],\n",
       "  [-0.16118097305297852,\n",
       "   0.2548772692680359,\n",
       "   -0.33907386660575867,\n",
       "   -0.12996605038642883,\n",
       "   0.524167537689209,\n",
       "   -0.23995766043663025,\n",
       "   0.3960501551628113,\n",
       "   -0.08184156566858292,\n",
       "   0.09046124666929245,\n",
       "   0.47681108117103577,\n",
       "   0.3645859360694885,\n",
       "   0.08496443927288055,\n",
       "   -0.21909379959106445,\n",
       "   0.2074047476053238,\n",
       "   -0.6709026098251343,\n",
       "   -0.502063512802124,\n",
       "   0.0706387311220169,\n",
       "   -0.4079698324203491,\n",
       "   -0.3414824604988098,\n",
       "   -0.04170173779129982,\n",
       "   0.2854067087173462,\n",
       "   0.3054153621196747,\n",
       "   -0.09589685499668121,\n",
       "   -0.02722599357366562,\n",
       "   0.1627006232738495,\n",
       "   -0.07065781950950623,\n",
       "   0.32896313071250916,\n",
       "   0.7368720769882202,\n",
       "   0.10515297949314117,\n",
       "   0.3984681963920593,\n",
       "   0.3337146043777466,\n",
       "   0.050209421664476395,\n",
       "   0.01961497589945793,\n",
       "   0.03776194155216217,\n",
       "   0.526965320110321,\n",
       "   0.5777069330215454,\n",
       "   -0.618889331817627,\n",
       "   0.41022247076034546,\n",
       "   0.022481946274638176,\n",
       "   0.5657495856285095,\n",
       "   -0.015380015596747398,\n",
       "   -0.10566258430480957,\n",
       "   -0.00917331874370575,\n",
       "   -0.03448772430419922,\n",
       "   0.36081749200820923,\n",
       "   0.03025764971971512,\n",
       "   -0.03941452503204346,\n",
       "   -0.12640231847763062,\n",
       "   -0.38163888454437256,\n",
       "   -0.016927272081375122,\n",
       "   0.017097216099500656,\n",
       "   -0.3749006390571594,\n",
       "   -0.2622561454772949,\n",
       "   -0.2754163146018982,\n",
       "   0.05243949592113495,\n",
       "   -0.18997031450271606,\n",
       "   0.1809079498052597,\n",
       "   -0.0811045914888382,\n",
       "   -0.20976001024246216,\n",
       "   0.8003365397453308,\n",
       "   -0.1325104832649231,\n",
       "   0.2743006944656372,\n",
       "   -0.05620782449841499,\n",
       "   -0.4608655869960785,\n",
       "   0.2837352752685547,\n",
       "   0.20740863680839539,\n",
       "   0.0765499472618103,\n",
       "   -0.13816767930984497,\n",
       "   0.4950920641422272,\n",
       "   0.07816614955663681,\n",
       "   -0.23057374358177185,\n",
       "   0.2954872250556946,\n",
       "   -0.16200436651706696,\n",
       "   -0.25953710079193115,\n",
       "   -0.268892765045166,\n",
       "   -0.38496291637420654,\n",
       "   0.24702449142932892,\n",
       "   0.05513212829828262,\n",
       "   0.024578168988227844,\n",
       "   -0.03938745707273483,\n",
       "   0.14451783895492554,\n",
       "   0.6435465216636658,\n",
       "   -0.10290679335594177,\n",
       "   0.3885154128074646,\n",
       "   -0.5587704181671143,\n",
       "   0.016720784828066826,\n",
       "   -0.5611447095870972,\n",
       "   0.03553320840001106,\n",
       "   0.43896278738975525,\n",
       "   0.7349929809570312,\n",
       "   0.1931522637605667,\n",
       "   -0.5062920451164246,\n",
       "   0.19690994918346405,\n",
       "   -0.31515631079673767,\n",
       "   -0.22122816741466522,\n",
       "   -0.0012900978326797485,\n",
       "   -0.37908539175987244,\n",
       "   0.08574910461902618,\n",
       "   -0.5695248246192932,\n",
       "   -0.09390757977962494,\n",
       "   -0.33695724606513977,\n",
       "   -0.019939929246902466,\n",
       "   -0.06315156817436218,\n",
       "   -0.17143502831459045,\n",
       "   -0.1548851728439331,\n",
       "   0.12075063586235046,\n",
       "   -0.12998254597187042,\n",
       "   -0.7910169363021851,\n",
       "   -0.3719598054885864,\n",
       "   0.37155258655548096,\n",
       "   0.19428673386573792,\n",
       "   0.1816430687904358,\n",
       "   0.31810641288757324,\n",
       "   0.31228917837142944,\n",
       "   -0.3338969647884369,\n",
       "   -0.29936593770980835,\n",
       "   -0.1441805213689804,\n",
       "   0.30526480078697205,\n",
       "   0.2649693489074707,\n",
       "   -0.27686306834220886,\n",
       "   0.18579037487506866,\n",
       "   0.1535530984401703,\n",
       "   -0.23716120421886444,\n",
       "   -0.17795707285404205,\n",
       "   0.10366486757993698,\n",
       "   -0.3558696210384369,\n",
       "   -0.06384380906820297,\n",
       "   -0.3636842668056488,\n",
       "   0.2096613347530365,\n",
       "   -0.17850035429000854,\n",
       "   -0.26084521412849426,\n",
       "   -0.11780278384685516,\n",
       "   -0.1839798241853714,\n",
       "   0.264168381690979,\n",
       "   0.18772976100444794,\n",
       "   0.27084997296333313,\n",
       "   -0.8909934759140015,\n",
       "   0.505607545375824,\n",
       "   -0.9116325974464417,\n",
       "   -0.2875383198261261,\n",
       "   0.034109942615032196,\n",
       "   0.46750208735466003,\n",
       "   0.5237370729446411,\n",
       "   -0.32237550616264343,\n",
       "   -0.047804370522499084,\n",
       "   -0.5050094127655029,\n",
       "   -0.3783014416694641,\n",
       "   -0.06776939332485199,\n",
       "   0.35862988233566284,\n",
       "   0.15622824430465698,\n",
       "   -0.4706546366214752,\n",
       "   0.11395478248596191,\n",
       "   0.18747811019420624,\n",
       "   -0.6924285888671875,\n",
       "   -0.1241801381111145,\n",
       "   -0.13461720943450928,\n",
       "   -0.38879072666168213,\n",
       "   0.0317913256585598,\n",
       "   0.631635844707489,\n",
       "   0.5841692090034485,\n",
       "   -0.04956034570932388,\n",
       "   -0.5320764183998108,\n",
       "   -0.38924503326416016,\n",
       "   0.29332953691482544,\n",
       "   0.16334912180900574,\n",
       "   0.09418612718582153,\n",
       "   -0.011521784588694572,\n",
       "   0.17687785625457764,\n",
       "   -0.5471401810646057,\n",
       "   0.4997972548007965,\n",
       "   0.5758668780326843,\n",
       "   -0.008184595964848995,\n",
       "   -0.01020260900259018,\n",
       "   0.12731948494911194,\n",
       "   -0.045491114258766174,\n",
       "   -0.05481705442070961,\n",
       "   0.7858955264091492,\n",
       "   0.03676271438598633,\n",
       "   -0.2523181140422821,\n",
       "   -0.0520784854888916,\n",
       "   0.4017384350299835,\n",
       "   0.015422739088535309,\n",
       "   -0.43250298500061035,\n",
       "   -0.6307573914527893,\n",
       "   0.10409259051084518,\n",
       "   -0.550857663154602,\n",
       "   0.250611811876297,\n",
       "   -0.7649783492088318,\n",
       "   0.5634005665779114,\n",
       "   0.2590898871421814,\n",
       "   0.04744134843349457,\n",
       "   0.025843016803264618,\n",
       "   -0.45434531569480896,\n",
       "   0.15765462815761566,\n",
       "   -0.522941529750824,\n",
       "   0.35720252990722656,\n",
       "   0.08482243120670319,\n",
       "   0.10434768348932266,\n",
       "   -0.17223156988620758,\n",
       "   0.3263716995716095,\n",
       "   -0.07457449287176132,\n",
       "   0.25614988803863525,\n",
       "   -0.20128610730171204,\n",
       "   -0.05744701623916626,\n",
       "   0.18718795478343964,\n",
       "   0.32431158423423767,\n",
       "   -0.29233530163764954,\n",
       "   0.24213245511054993,\n",
       "   -0.3256370425224304,\n",
       "   -0.5171032547950745,\n",
       "   0.5864723920822144,\n",
       "   -0.9867941737174988,\n",
       "   -0.297823965549469,\n",
       "   -0.22253185510635376,\n",
       "   0.4641244411468506,\n",
       "   -0.36717548966407776,\n",
       "   0.18604372441768646,\n",
       "   -0.07757490873336792,\n",
       "   0.04644867777824402,\n",
       "   0.08029451221227646,\n",
       "   -0.37335193157196045,\n",
       "   -0.0827518180012703,\n",
       "   0.04682181030511856,\n",
       "   -0.4816420376300812,\n",
       "   -0.6080593466758728,\n",
       "   0.7808125019073486,\n",
       "   0.004602951928973198,\n",
       "   0.48728907108306885,\n",
       "   0.07176937162876129,\n",
       "   -0.20549024641513824,\n",
       "   0.230545312166214,\n",
       "   -0.08261385560035706,\n",
       "   0.26670947670936584,\n",
       "   -0.36862123012542725,\n",
       "   0.20790700614452362,\n",
       "   0.41685056686401367,\n",
       "   -0.12099795043468475,\n",
       "   -0.005754704587161541,\n",
       "   -0.47165215015411377,\n",
       "   -0.6004130244255066,\n",
       "   0.14185187220573425,\n",
       "   0.24783478677272797,\n",
       "   -0.17491239309310913,\n",
       "   0.0939832553267479,\n",
       "   -0.0757443904876709,\n",
       "   -0.18352970480918884,\n",
       "   -0.2479405254125595,\n",
       "   0.1985962986946106,\n",
       "   -0.048390865325927734,\n",
       "   -0.20087799429893494,\n",
       "   -0.28997519612312317,\n",
       "   -0.2920900881290436,\n",
       "   -0.1576336771249771,\n",
       "   -0.17393812537193298,\n",
       "   0.5212512016296387,\n",
       "   0.0027992799878120422,\n",
       "   -0.1152181327342987,\n",
       "   0.5686090588569641,\n",
       "   0.7254939079284668,\n",
       "   -1.0454334020614624,\n",
       "   0.18580208718776703,\n",
       "   0.5087223052978516,\n",
       "   0.2887495756149292,\n",
       "   -0.12014181911945343,\n",
       "   -0.33374300599098206,\n",
       "   -0.27898016571998596,\n",
       "   -0.33108246326446533,\n",
       "   -0.4359247386455536,\n",
       "   -0.8061234951019287,\n",
       "   0.29678961634635925,\n",
       "   0.1756451427936554,\n",
       "   -0.30804768204689026,\n",
       "   0.10638771206140518,\n",
       "   0.2888070344924927,\n",
       "   -0.03798516094684601,\n",
       "   -0.11371472477912903,\n",
       "   0.18010744452476501,\n",
       "   -0.2171563059091568,\n",
       "   -0.8490080237388611,\n",
       "   -0.5200293660163879,\n",
       "   0.6392509341239929,\n",
       "   0.37216824293136597,\n",
       "   0.4477892816066742,\n",
       "   0.004984815139323473,\n",
       "   -0.5795624256134033,\n",
       "   -0.4607101082801819,\n",
       "   -0.0501842126250267,\n",
       "   0.3619033396244049,\n",
       "   -0.03871101886034012,\n",
       "   -0.5729479193687439,\n",
       "   -0.3277941942214966,\n",
       "   0.01590905711054802,\n",
       "   -0.6231561303138733,\n",
       "   -0.11371543258428574,\n",
       "   0.1300823986530304,\n",
       "   0.121617890894413,\n",
       "   -0.057417627424001694,\n",
       "   0.2471897304058075,\n",
       "   -0.4525633752346039,\n",
       "   -0.17439672350883484,\n",
       "   -0.26900702714920044,\n",
       "   -0.09504733234643936,\n",
       "   0.47320809960365295,\n",
       "   -0.07921614497900009,\n",
       "   -0.17598053812980652,\n",
       "   -0.004227899480611086,\n",
       "   0.39032885432243347,\n",
       "   0.36839544773101807,\n",
       "   0.45907580852508545,\n",
       "   0.372559130191803,\n",
       "   0.4673815369606018,\n",
       "   -0.09202737361192703,\n",
       "   -0.047139350324869156,\n",
       "   -0.02166968397796154,\n",
       "   -0.01067407801747322,\n",
       "   -0.26285624504089355,\n",
       "   -0.04701559990644455,\n",
       "   -0.19194316864013672,\n",
       "   -0.21011418104171753,\n",
       "   -0.08524826169013977,\n",
       "   -0.3213309049606323,\n",
       "   0.28054115176200867,\n",
       "   -0.3578859269618988,\n",
       "   0.4713608920574188,\n",
       "   0.020952515304088593,\n",
       "   0.21679642796516418,\n",
       "   0.11991266906261444,\n",
       "   0.4870401620864868,\n",
       "   0.015723906457424164,\n",
       "   -0.07474071532487869,\n",
       "   0.17614033818244934,\n",
       "   -0.5614812970161438,\n",
       "   -0.2298336625099182,\n",
       "   0.17573639750480652,\n",
       "   -0.6686016321182251,\n",
       "   -0.25734996795654297,\n",
       "   -0.2801736295223236,\n",
       "   0.3230658769607544,\n",
       "   -0.8662558197975159,\n",
       "   0.614231526851654,\n",
       "   0.2049928605556488,\n",
       "   0.41967904567718506,\n",
       "   0.2565874755382538,\n",
       "   -0.3012479841709137,\n",
       "   0.5664309859275818,\n",
       "   0.42797207832336426,\n",
       "   -0.06222652271389961,\n",
       "   0.4013037085533142,\n",
       "   -0.11381098628044128,\n",
       "   0.03217349201440811,\n",
       "   -0.4732302725315094,\n",
       "   -0.30588480830192566,\n",
       "   -0.03218203783035278,\n",
       "   -0.38531193137168884,\n",
       "   0.645941972732544,\n",
       "   0.045085661113262177,\n",
       "   -0.06170353293418884,\n",
       "   0.38781091570854187,\n",
       "   0.09355364739894867,\n",
       "   0.6372922658920288,\n",
       "   0.24056534469127655,\n",
       "   -0.003538466989994049,\n",
       "   0.4737617075443268,\n",
       "   0.04744156077504158,\n",
       "   0.2823473811149597,\n",
       "   -0.16813507676124573,\n",
       "   -0.8622673749923706,\n",
       "   -0.20753313601016998,\n",
       "   -0.8606031537055969,\n",
       "   -0.2791644036769867,\n",
       "   1.1827244758605957,\n",
       "   -0.232455313205719,\n",
       "   0.2258244901895523,\n",
       "   -0.19172105193138123,\n",
       "   0.45858681201934814,\n",
       "   -0.3879395127296448,\n",
       "   -0.6489843726158142,\n",
       "   0.05028463900089264,\n",
       "   0.0007897168397903442,\n",
       "   -0.6255218386650085,\n",
       "   -0.2706289291381836,\n",
       "   0.3225235641002655,\n",
       "   0.2522123456001282,\n",
       "   -0.16726459562778473,\n",
       "   0.03014998883008957,\n",
       "   -0.30435454845428467,\n",
       "   0.019072094932198524,\n",
       "   0.4379342198371887,\n",
       "   -0.03912598639726639,\n",
       "   -0.6419829726219177,\n",
       "   0.36825403571128845,\n",
       "   0.3227722942829132,\n",
       "   0.39554765820503235,\n",
       "   -0.4164432883262634,\n",
       "   -0.1389288604259491,\n",
       "   0.34179842472076416,\n",
       "   -0.46470656991004944,\n",
       "   0.44815418124198914,\n",
       "   0.1664818525314331,\n",
       "   -0.12956532835960388,\n",
       "   -0.30280429124832153,\n",
       "   0.25494223833084106,\n",
       "   -0.04078932851552963,\n",
       "   -0.32036444544792175,\n",
       "   -0.6121864318847656,\n",
       "   -0.390857458114624,\n",
       "   -0.3095926344394684,\n",
       "   0.277930349111557,\n",
       "   0.08609674870967865,\n",
       "   0.10871454328298569,\n",
       "   -0.2614288330078125,\n",
       "   -0.40568509697914124,\n",
       "   0.014902200549840927,\n",
       "   0.2827588617801666,\n",
       "   0.476077675819397,\n",
       "   -0.4082110524177551,\n",
       "   0.060469888150691986,\n",
       "   -0.09930209815502167,\n",
       "   -0.005339223891496658,\n",
       "   0.04056210815906525,\n",
       "   -0.020960047841072083,\n",
       "   0.04225371778011322,\n",
       "   0.19403867423534393,\n",
       "   -0.16550999879837036,\n",
       "   -0.026312492787837982,\n",
       "   0.2763356864452362,\n",
       "   0.04080692678689957,\n",
       "   -0.03403616324067116,\n",
       "   -0.41647469997406006,\n",
       "   0.09210731834173203,\n",
       "   -0.11870814114809036,\n",
       "   0.7137213945388794,\n",
       "   -0.018477296456694603,\n",
       "   0.8696321845054626,\n",
       "   -0.0678965225815773,\n",
       "   0.15488165616989136,\n",
       "   -0.19119779765605927,\n",
       "   0.13617177307605743,\n",
       "   -0.2600694000720978,\n",
       "   0.24030618369579315,\n",
       "   -0.2703893184661865,\n",
       "   -0.4293617308139801,\n",
       "   -0.6342840790748596,\n",
       "   -0.7949785590171814,\n",
       "   -0.033998146653175354,\n",
       "   -0.4401685297489166,\n",
       "   -0.42362886667251587,\n",
       "   -0.5722863078117371,\n",
       "   0.02871844731271267,\n",
       "   -0.3551662266254425,\n",
       "   0.23839180171489716,\n",
       "   0.05603954941034317,\n",
       "   -0.35834380984306335,\n",
       "   0.08771742880344391,\n",
       "   0.2978260815143585,\n",
       "   -0.42064279317855835,\n",
       "   -0.6134310960769653,\n",
       "   -0.3106149435043335,\n",
       "   -0.36819180846214294,\n",
       "   0.008005868643522263,\n",
       "   -1.1116646528244019,\n",
       "   0.3165922164916992,\n",
       "   0.041804708540439606,\n",
       "   0.5331408977508545,\n",
       "   -0.13732528686523438,\n",
       "   -0.11776567995548248,\n",
       "   0.5778920650482178,\n",
       "   0.4515659511089325,\n",
       "   0.8493815660476685,\n",
       "   0.1239340677857399,\n",
       "   0.7081283926963806,\n",
       "   0.6008546948432922,\n",
       "   0.16986556351184845,\n",
       "   1.044695496559143,\n",
       "   0.04611200466752052,\n",
       "   -0.9326905608177185,\n",
       "   0.028310563415288925,\n",
       "   0.38459211587905884,\n",
       "   -0.17224502563476562,\n",
       "   -0.1473868489265442,\n",
       "   -0.6794405579566956,\n",
       "   0.05287397652864456,\n",
       "   -0.3958609998226166,\n",
       "   -0.23576271533966064,\n",
       "   0.03321192041039467,\n",
       "   -0.25991448760032654,\n",
       "   1.113265872001648,\n",
       "   0.15778297185897827,\n",
       "   0.40022677183151245,\n",
       "   0.3910433053970337,\n",
       "   0.050805386155843735,\n",
       "   -0.23026400804519653,\n",
       "   -0.023122912272810936,\n",
       "   0.14389856159687042,\n",
       "   -0.6486462950706482,\n",
       "   0.1317054182291031,\n",
       "   -0.032086968421936035,\n",
       "   0.13205313682556152,\n",
       "   0.17821766436100006,\n",
       "   -0.47252357006073,\n",
       "   -0.06971994787454605,\n",
       "   0.18372926115989685,\n",
       "   0.2469252347946167,\n",
       "   0.3128528892993927,\n",
       "   -0.6673088073730469,\n",
       "   1.0399268865585327,\n",
       "   0.3275768756866455,\n",
       "   0.054062098264694214,\n",
       "   0.4357714354991913,\n",
       "   0.39679932594299316,\n",
       "   -0.07823659479618073,\n",
       "   -0.2924972176551819,\n",
       "   0.038146235048770905,\n",
       "   -0.02392338216304779,\n",
       "   -0.012193933129310608,\n",
       "   -0.10499203950166702,\n",
       "   0.4143120050430298,\n",
       "   0.11188878118991852,\n",
       "   0.15597552061080933,\n",
       "   -0.2224598079919815,\n",
       "   0.43737536668777466,\n",
       "   0.24591407179832458,\n",
       "   0.2015191912651062,\n",
       "   0.36282750964164734,\n",
       "   0.06501858681440353,\n",
       "   -0.054843612015247345,\n",
       "   0.7511823177337646,\n",
       "   0.6218433380126953,\n",
       "   0.08290030062198639,\n",
       "   0.0075834281742572784,\n",
       "   -0.16646941006183624,\n",
       "   -0.020261544734239578,\n",
       "   0.26508593559265137,\n",
       "   -0.300065279006958,\n",
       "   1.0192819833755493,\n",
       "   -0.25232934951782227,\n",
       "   0.549095630645752,\n",
       "   -0.19170357286930084,\n",
       "   -0.3033817410469055,\n",
       "   -0.24649706482887268,\n",
       "   -0.08461534231901169,\n",
       "   0.022817734628915787,\n",
       "   0.18356749415397644,\n",
       "   0.09961255639791489,\n",
       "   0.15479303896427155,\n",
       "   0.38340234756469727,\n",
       "   -0.2092294543981552,\n",
       "   0.3950154781341553,\n",
       "   0.5017166137695312,\n",
       "   -0.14048145711421967,\n",
       "   0.3479825258255005,\n",
       "   -0.006774917244911194,\n",
       "   -1.2905385494232178,\n",
       "   0.25725093483924866,\n",
       "   0.30709829926490784,\n",
       "   -0.2646239697933197,\n",
       "   -0.10250478237867355,\n",
       "   -8.461233139038086,\n",
       "   0.42074066400527954,\n",
       "   -0.06890013813972473,\n",
       "   0.3591923713684082,\n",
       "   0.3437115550041199,\n",
       "   0.6763902902603149,\n",
       "   0.25267893075942993,\n",
       "   -0.6744294166564941,\n",
       "   0.016844281926751137,\n",
       "   -0.03893870860338211,\n",
       "   -0.3312195837497711,\n",
       "   -0.3340364098548889,\n",
       "   0.5931397676467896,\n",
       "   -0.06325129419565201,\n",
       "   0.3547215163707733,\n",
       "   0.11524170637130737,\n",
       "   -0.08524591475725174,\n",
       "   0.23076748847961426,\n",
       "   0.6242291927337646,\n",
       "   -0.3788490295410156,\n",
       "   -0.07490306347608566,\n",
       "   0.45675206184387207,\n",
       "   0.250792533159256,\n",
       "   0.14842335879802704,\n",
       "   -0.0861436203122139,\n",
       "   0.13647133111953735,\n",
       "   0.322485089302063,\n",
       "   0.01915932446718216,\n",
       "   0.1271650642156601,\n",
       "   -0.08718592673540115,\n",
       "   -0.1409441977739334,\n",
       "   0.07003942131996155,\n",
       "   -0.14966759085655212,\n",
       "   -0.16681696474552155,\n",
       "   0.38829901814460754,\n",
       "   -0.3328096866607666,\n",
       "   0.050803523510694504,\n",
       "   -0.18961966037750244,\n",
       "   0.8266006112098694,\n",
       "   -0.4771984815597534,\n",
       "   -0.05515509471297264,\n",
       "   -0.2259300947189331,\n",
       "   -0.32767951488494873,\n",
       "   -0.22564692795276642,\n",
       "   0.2699670195579529,\n",
       "   0.11049847304821014,\n",
       "   0.2023540735244751,\n",
       "   -0.18657276034355164,\n",
       "   -0.6951897144317627,\n",
       "   0.39505788683891296,\n",
       "   0.5411190986633301,\n",
       "   0.2780720889568329,\n",
       "   0.4399767816066742,\n",
       "   0.23380489647388458,\n",
       "   -0.6291671991348267,\n",
       "   -0.21259233355522156,\n",
       "   -0.20704708993434906,\n",
       "   0.7813320755958557,\n",
       "   0.06425867974758148,\n",
       "   -0.24195259809494019,\n",
       "   0.19828522205352783,\n",
       "   -0.38711753487586975,\n",
       "   1.0662951469421387,\n",
       "   -0.010818123817443848,\n",
       "   -0.16491752862930298,\n",
       "   0.4206258952617645,\n",
       "   0.008915700018405914,\n",
       "   0.36211642622947693,\n",
       "   0.0666760578751564,\n",
       "   -0.2655066251754761,\n",
       "   -0.12798306345939636,\n",
       "   -0.4062163531780243,\n",
       "   0.15574215352535248,\n",
       "   -0.8797309994697571,\n",
       "   0.10944844782352448,\n",
       "   -1.0463296175003052,\n",
       "   -0.6048272848129272,\n",
       "   -0.057759691029787064,\n",
       "   -0.32270190119743347,\n",
       "   0.4113864600658417,\n",
       "   -0.27129366993904114,\n",
       "   0.5715925693511963,\n",
       "   0.2749098241329193,\n",
       "   -0.6586942076683044,\n",
       "   0.03106853738427162,\n",
       "   0.11090236902236938,\n",
       "   -0.24582545459270477,\n",
       "   0.24054738879203796,\n",
       "   -0.14132297039031982,\n",
       "   0.06177724897861481,\n",
       "   0.08935689181089401,\n",
       "   0.4066144526004791,\n",
       "   -0.01121426373720169,\n",
       "   -0.02183905616402626,\n",
       "   0.1442028433084488,\n",
       "   0.2681197226047516,\n",
       "   -0.20947521924972534,\n",
       "   -0.22786855697631836,\n",
       "   0.06600234657526016,\n",
       "   0.00026354286819696426,\n",
       "   -0.6900690197944641,\n",
       "   -0.0825660452246666,\n",
       "   -0.46704423427581787,\n",
       "   0.4112973213195801,\n",
       "   -0.12397675961256027,\n",
       "   -0.30216071009635925,\n",
       "   0.46779945492744446,\n",
       "   -0.6343554258346558,\n",
       "   0.475957453250885,\n",
       "   0.35090604424476624,\n",
       "   -0.4111045002937317,\n",
       "   0.48059192299842834,\n",
       "   0.9691581726074219,\n",
       "   -0.6198943257331848,\n",
       "   0.17560723423957825,\n",
       "   0.46370163559913635,\n",
       "   0.6311012506484985,\n",
       "   -0.11954216659069061,\n",
       "   0.2885432243347168,\n",
       "   -0.618180513381958,\n",
       "   -0.2643483579158783,\n",
       "   -0.31100648641586304,\n",
       "   0.20582854747772217,\n",
       "   -0.0826258435845375,\n",
       "   0.5847298502922058,\n",
       "   -0.3460044264793396,\n",
       "   0.10258404910564423,\n",
       "   -0.08332202583551407,\n",
       "   0.3845914304256439,\n",
       "   0.016180098056793213,\n",
       "   0.20843800902366638,\n",
       "   -0.018626999109983444,\n",
       "   -0.2424991875886917,\n",
       "   0.7938211560249329,\n",
       "   0.2790285348892212,\n",
       "   0.01920473575592041,\n",
       "   -0.17275644838809967,\n",
       "   -0.48921331763267517,\n",
       "   -0.8415650129318237,\n",
       "   -0.3021143078804016,\n",
       "   -0.530153751373291,\n",
       "   -0.35059675574302673,\n",
       "   0.8121252059936523,\n",
       "   0.4684761166572571,\n",
       "   0.717430830001831,\n",
       "   -0.4090860188007355,\n",
       "   0.1586109697818756,\n",
       "   0.25298964977264404,\n",
       "   0.03455616906285286,\n",
       "   -0.3481197655200958,\n",
       "   -0.3595319390296936,\n",
       "   0.12215230613946915,\n",
       "   0.8088839650154114,\n",
       "   0.644175112247467,\n",
       "   0.5998246669769287,\n",
       "   -0.7174041271209717,\n",
       "   0.5150079131126404,\n",
       "   0.5524469017982483,\n",
       "   0.03858555108308792,\n",
       "   0.09241287410259247,\n",
       "   0.17091433703899384,\n",
       "   -0.08983199298381805,\n",
       "   0.22865751385688782,\n",
       "   1.0073150396347046,\n",
       "   -0.15557792782783508,\n",
       "   0.3272562026977539,\n",
       "   -0.25700482726097107,\n",
       "   0.5124134421348572,\n",
       "   0.12970761954784393,\n",
       "   0.41511499881744385,\n",
       "   -0.06049402058124542,\n",
       "   -0.6515082716941833,\n",
       "   0.4593746066093445,\n",
       "   0.3152361512184143,\n",
       "   -0.06234544515609741,\n",
       "   -0.41681694984436035,\n",
       "   0.0852564349770546,\n",
       "   -0.31925567984580994,\n",
       "   0.46335792541503906,\n",
       "   -0.06699152290821075,\n",
       "   0.32003432512283325,\n",
       "   0.4879392683506012,\n",
       "   -0.37413695454597473,\n",
       "   -0.18712761998176575,\n",
       "   -0.8024081587791443,\n",
       "   -0.1254296898841858,\n",
       "   0.2942427098751068,\n",
       "   0.20694296061992645,\n",
       "   0.08666446805000305,\n",
       "   -0.29756537079811096,\n",
       "   0.07736911624670029,\n",
       "   0.2699859142303467,\n",
       "   -0.09897076338529587,\n",
       "   0.1542820781469345,\n",
       "   0.06912295520305634,\n",
       "   0.21941184997558594,\n",
       "   -0.023836608976125717,\n",
       "   0.3267638087272644,\n",
       "   -0.06583912670612335,\n",
       "   -0.22021928429603577,\n",
       "   0.10840201377868652,\n",
       "   0.2581135630607605,\n",
       "   -0.339489221572876,\n",
       "   -0.3111613094806671,\n",
       "   -0.3627331554889679,\n",
       "   -0.14684966206550598,\n",
       "   0.10231979191303253,\n",
       "   0.6670395135879517,\n",
       "   -0.07141974568367004,\n",
       "   0.23693786561489105,\n",
       "   0.03896526247262955],\n",
       "  [-0.2580370604991913,\n",
       "   -0.026581015437841415,\n",
       "   -0.0002092607319355011,\n",
       "   0.09458528459072113,\n",
       "   0.16868820786476135,\n",
       "   -0.07083006203174591,\n",
       "   0.13221120834350586,\n",
       "   -0.0899353101849556,\n",
       "   -0.3115687370300293,\n",
       "   0.3077642321586609,\n",
       "   -0.09806548058986664,\n",
       "   0.041830819100141525,\n",
       "   -0.3141549229621887,\n",
       "   -0.15492747724056244,\n",
       "   -0.5019869804382324,\n",
       "   -0.6071441173553467,\n",
       "   0.1316123902797699,\n",
       "   -0.5406482219696045,\n",
       "   -0.052896905690431595,\n",
       "   -0.07065165787935257,\n",
       "   0.05446391552686691,\n",
       "   -0.14147238433361053,\n",
       "   -0.2332577258348465,\n",
       "   -0.1581633985042572,\n",
       "   0.141606867313385,\n",
       "   -0.5551354289054871,\n",
       "   -0.12598870694637299,\n",
       "   0.7447517514228821,\n",
       "   -0.20747995376586914,\n",
       "   0.4583289921283722,\n",
       "   0.28302493691444397,\n",
       "   -0.22520415484905243,\n",
       "   0.49485957622528076,\n",
       "   0.023368654772639275,\n",
       "   0.03805501013994217,\n",
       "   0.24787379801273346,\n",
       "   -0.2894931435585022,\n",
       "   0.5485414862632751,\n",
       "   0.07099645584821701,\n",
       "   0.3927096426486969,\n",
       "   0.16960197687149048,\n",
       "   0.264077752828598,\n",
       "   0.05051039159297943,\n",
       "   0.3119933307170868,\n",
       "   -0.7102112770080566,\n",
       "   -0.17547529935836792,\n",
       "   -0.06917272508144379,\n",
       "   -0.5312820672988892,\n",
       "   -0.279775470495224,\n",
       "   0.10280562937259674,\n",
       "   0.5880904197692871,\n",
       "   -0.13363148272037506,\n",
       "   -0.20380619168281555,\n",
       "   -0.10658538341522217,\n",
       "   0.11379888653755188,\n",
       "   -0.48533153533935547,\n",
       "   0.15005221962928772,\n",
       "   0.12578897178173065,\n",
       "   -0.24781030416488647,\n",
       "   0.3740178644657135,\n",
       "   -0.060732897371053696,\n",
       "   -0.15053246915340424,\n",
       "   0.009287873283028603,\n",
       "   -0.053448066115379333,\n",
       "   0.05636999011039734,\n",
       "   0.04451411962509155,\n",
       "   0.320636123418808,\n",
       "   -0.41704148054122925,\n",
       "   0.20449063181877136,\n",
       "   0.20016176998615265,\n",
       "   -0.19750767946243286,\n",
       "   -0.10165688395500183,\n",
       "   -0.2779814302921295,\n",
       "   -0.2658194303512573,\n",
       "   0.06533129513263702,\n",
       "   -0.08551526814699173,\n",
       "   -0.35670459270477295,\n",
       "   0.06505731493234634,\n",
       "   0.1420602798461914,\n",
       "   0.1788654327392578,\n",
       "   0.26966896653175354,\n",
       "   0.14091083407402039,\n",
       "   -0.2091679871082306,\n",
       "   0.5363059043884277,\n",
       "   0.10246141254901886,\n",
       "   0.15151482820510864,\n",
       "   -0.2999197244644165,\n",
       "   -0.013457500375807285,\n",
       "   -0.11866385489702225,\n",
       "   0.2297435849905014,\n",
       "   0.33061856031417847,\n",
       "   -0.5195226669311523,\n",
       "   0.5238828063011169,\n",
       "   -0.05531317740678787,\n",
       "   -0.12541653215885162,\n",
       "   -0.14465802907943726,\n",
       "   -0.48311647772789,\n",
       "   -0.36931127309799194,\n",
       "   -0.9509909749031067,\n",
       "   0.08079420030117035,\n",
       "   -0.11169785261154175,\n",
       "   0.46738162636756897,\n",
       "   -0.04734361916780472,\n",
       "   -0.20623493194580078,\n",
       "   0.06429440528154373,\n",
       "   0.1678919941186905,\n",
       "   -0.5912737846374512,\n",
       "   -0.8214200735092163,\n",
       "   -0.3872551918029785,\n",
       "   0.6589664816856384,\n",
       "   0.036764077842235565,\n",
       "   0.33561888337135315,\n",
       "   -0.31255006790161133,\n",
       "   0.28288301825523376,\n",
       "   -0.41041624546051025,\n",
       "   -0.7497533559799194,\n",
       "   -0.00389902014285326,\n",
       "   0.1420295089483261,\n",
       "   0.11027432233095169,\n",
       "   0.16176815330982208,\n",
       "   0.25062334537506104,\n",
       "   -0.05005014315247536,\n",
       "   -0.15158098936080933,\n",
       "   0.10352286696434021,\n",
       "   0.0014176624827086926,\n",
       "   -0.12921540439128876,\n",
       "   -0.21233132481575012,\n",
       "   -0.4723092317581177,\n",
       "   0.26507022976875305,\n",
       "   -0.16920647025108337,\n",
       "   -0.21612724661827087,\n",
       "   0.11385954916477203,\n",
       "   -0.10639376938343048,\n",
       "   -0.23700034618377686,\n",
       "   0.23395103216171265,\n",
       "   0.13658855855464935,\n",
       "   -0.5958528518676758,\n",
       "   -0.09192994982004166,\n",
       "   -1.182631015777588,\n",
       "   -0.4507284164428711,\n",
       "   -0.2806835472583771,\n",
       "   0.3938455283641815,\n",
       "   0.6745368242263794,\n",
       "   -0.2152608036994934,\n",
       "   0.14130251109600067,\n",
       "   -0.1315607726573944,\n",
       "   -0.519188642501831,\n",
       "   0.0002762153744697571,\n",
       "   0.2990078032016754,\n",
       "   0.11295786499977112,\n",
       "   -0.16458913683891296,\n",
       "   -0.25166842341423035,\n",
       "   0.21291090548038483,\n",
       "   -0.03473491966724396,\n",
       "   -0.3240460455417633,\n",
       "   -0.037548407912254333,\n",
       "   -0.16651824116706848,\n",
       "   0.00605648010969162,\n",
       "   0.5024864673614502,\n",
       "   0.6025906205177307,\n",
       "   0.1768019050359726,\n",
       "   -0.8164889812469482,\n",
       "   -0.13730904459953308,\n",
       "   0.4232575297355652,\n",
       "   0.1715061366558075,\n",
       "   -0.24418793618679047,\n",
       "   -0.06814238429069519,\n",
       "   0.465314656496048,\n",
       "   -0.3865964114665985,\n",
       "   0.11799435317516327,\n",
       "   0.332793653011322,\n",
       "   -0.23555684089660645,\n",
       "   0.5027729272842407,\n",
       "   0.3515697717666626,\n",
       "   0.10051744431257248,\n",
       "   -0.09703275561332703,\n",
       "   0.31536510586738586,\n",
       "   -0.02800457924604416,\n",
       "   -0.04500792920589447,\n",
       "   -0.24231785535812378,\n",
       "   0.23741883039474487,\n",
       "   -0.49472931027412415,\n",
       "   0.17074504494667053,\n",
       "   -0.5892704725265503,\n",
       "   0.30517756938934326,\n",
       "   -0.43086981773376465,\n",
       "   -0.029568377882242203,\n",
       "   -0.6974026560783386,\n",
       "   0.1611996442079544,\n",
       "   0.0651339516043663,\n",
       "   0.07700635492801666,\n",
       "   -0.3220256268978119,\n",
       "   -0.500199556350708,\n",
       "   -0.0679861307144165,\n",
       "   -0.988978385925293,\n",
       "   0.36374858021736145,\n",
       "   -0.05773200839757919,\n",
       "   0.2952459454536438,\n",
       "   0.07275567203760147,\n",
       "   0.20161941647529602,\n",
       "   -0.005035527050495148,\n",
       "   0.15082710981369019,\n",
       "   -0.30186593532562256,\n",
       "   -0.031036194413900375,\n",
       "   -0.13230670988559723,\n",
       "   0.21461248397827148,\n",
       "   -0.7408348321914673,\n",
       "   0.42254751920700073,\n",
       "   -0.17302963137626648,\n",
       "   -0.6659635901451111,\n",
       "   -0.15661180019378662,\n",
       "   -0.649267852306366,\n",
       "   0.09099511057138443,\n",
       "   -0.012517961673438549,\n",
       "   -0.5251415967941284,\n",
       "   -0.28792187571525574,\n",
       "   -0.22010566294193268,\n",
       "   0.1430017203092575,\n",
       "   -0.04812409728765488,\n",
       "   -0.13265085220336914,\n",
       "   0.4828251302242279,\n",
       "   0.22073161602020264,\n",
       "   0.3868749141693115,\n",
       "   -0.009590959176421165,\n",
       "   -0.3682829439640045,\n",
       "   0.33374422788619995,\n",
       "   0.34180986881256104,\n",
       "   0.5150430798530579,\n",
       "   0.47151321172714233,\n",
       "   -0.17704205214977264,\n",
       "   -0.16076122224330902,\n",
       "   0.1384742259979248,\n",
       "   -0.008947335183620453,\n",
       "   -0.41638651490211487,\n",
       "   0.26521843671798706,\n",
       "   0.06732242554426193,\n",
       "   -0.061457857489585876,\n",
       "   0.11447271704673767,\n",
       "   -0.7358317375183105,\n",
       "   -0.5847707986831665,\n",
       "   -0.042866338044404984,\n",
       "   -0.20643961429595947,\n",
       "   0.29114875197410583,\n",
       "   0.44633203744888306,\n",
       "   -0.10909519344568253,\n",
       "   0.030716506764292717,\n",
       "   -0.4020671844482422,\n",
       "   0.7018198370933533,\n",
       "   0.1723988801240921,\n",
       "   0.17631103098392487,\n",
       "   -0.3587036728858948,\n",
       "   -0.17862965166568756,\n",
       "   -0.2556285560131073,\n",
       "   0.1994876265525818,\n",
       "   -0.3929445147514343,\n",
       "   -0.4331340193748474,\n",
       "   -0.43034446239471436,\n",
       "   0.5622184872627258,\n",
       "   0.5689756274223328,\n",
       "   -0.741593599319458,\n",
       "   0.47149258852005005,\n",
       "   0.08371029794216156,\n",
       "   0.5340214967727661,\n",
       "   0.2788606882095337,\n",
       "   -0.09496860206127167,\n",
       "   -0.11151666194200516,\n",
       "   -0.1551421880722046,\n",
       "   -0.43747401237487793,\n",
       "   -0.8668133020401001,\n",
       "   0.7113153338432312,\n",
       "   0.23860403895378113,\n",
       "   0.22847682237625122,\n",
       "   0.5276827216148376,\n",
       "   0.5207661390304565,\n",
       "   -0.14396093785762787,\n",
       "   0.17295026779174805,\n",
       "   0.10046222805976868,\n",
       "   -0.2184866964817047,\n",
       "   -0.052547387778759,\n",
       "   -0.2255544662475586,\n",
       "   0.28051093220710754,\n",
       "   0.5794097185134888,\n",
       "   0.09984743595123291,\n",
       "   0.22698992490768433,\n",
       "   -0.6186575889587402,\n",
       "   0.2805478274822235,\n",
       "   0.066996268928051,\n",
       "   0.6560158729553223,\n",
       "   -0.10874068737030029,\n",
       "   0.2053752988576889,\n",
       "   -0.3896099627017975,\n",
       "   -0.0402965173125267,\n",
       "   -0.4295680522918701,\n",
       "   0.01829955354332924,\n",
       "   -0.015278266742825508,\n",
       "   0.06062787398695946,\n",
       "   -0.10542841255664825,\n",
       "   0.3536378741264343,\n",
       "   -0.44537535309791565,\n",
       "   -0.32608118653297424,\n",
       "   0.19720390439033508,\n",
       "   0.11971237510442734,\n",
       "   0.09464432299137115,\n",
       "   -0.016126755625009537,\n",
       "   -0.17233753204345703,\n",
       "   -0.06811167299747467,\n",
       "   0.202928364276886,\n",
       "   -0.018775207921862602,\n",
       "   -0.4276672601699829,\n",
       "   0.7741924524307251,\n",
       "   0.4600662291049957,\n",
       "   -0.07403800636529922,\n",
       "   -0.3844033181667328,\n",
       "   -0.6232850551605225,\n",
       "   0.40760546922683716,\n",
       "   -0.4934273064136505,\n",
       "   -0.284254789352417,\n",
       "   -0.36117327213287354,\n",
       "   -0.050056032836437225,\n",
       "   -0.11414412409067154,\n",
       "   -0.24655510485172272,\n",
       "   0.11512168496847153,\n",
       "   -0.42984774708747864,\n",
       "   0.5296781659126282,\n",
       "   0.043837770819664,\n",
       "   -0.06048689782619476,\n",
       "   -0.019612204283475876,\n",
       "   0.397010862827301,\n",
       "   0.43416911363601685,\n",
       "   -0.1776561737060547,\n",
       "   -0.4391702711582184,\n",
       "   -0.304360568523407,\n",
       "   0.32540634274482727,\n",
       "   0.5063279867172241,\n",
       "   0.15033048391342163,\n",
       "   0.6072775721549988,\n",
       "   0.011618871241807938,\n",
       "   0.03056725300848484,\n",
       "   -0.5295032858848572,\n",
       "   0.5136761665344238,\n",
       "   0.2867604196071625,\n",
       "   0.8457043170928955,\n",
       "   0.42558395862579346,\n",
       "   -0.12779207527637482,\n",
       "   0.3217325508594513,\n",
       "   0.1889231652021408,\n",
       "   -0.11684362590312958,\n",
       "   -0.21690304577350616,\n",
       "   -0.16602890193462372,\n",
       "   -0.3157046139240265,\n",
       "   -0.47001883387565613,\n",
       "   -0.4744364321231842,\n",
       "   -0.3219751715660095,\n",
       "   0.2920228838920593,\n",
       "   0.9017189741134644,\n",
       "   0.1279602348804474,\n",
       "   0.12219718843698502,\n",
       "   0.20404087007045746,\n",
       "   0.41343697905540466,\n",
       "   -0.053206268697977066,\n",
       "   0.32263684272766113,\n",
       "   0.17612679302692413,\n",
       "   -0.14216041564941406,\n",
       "   0.43504032492637634,\n",
       "   0.004195110872387886,\n",
       "   0.06537945568561554,\n",
       "   -0.11007703095674515,\n",
       "   0.13539518415927887,\n",
       "   -0.03001801297068596,\n",
       "   -0.1072521060705185,\n",
       "   0.9718596935272217,\n",
       "   -0.06090155616402626,\n",
       "   0.21223564445972443,\n",
       "   0.18304240703582764,\n",
       "   0.16434594988822937,\n",
       "   -0.15211181342601776,\n",
       "   -0.2818071246147156,\n",
       "   0.12234926223754883,\n",
       "   -0.6682166457176208,\n",
       "   -0.5247048139572144,\n",
       "   -0.06638253480195999,\n",
       "   -0.11201821267604828,\n",
       "   0.38740992546081543,\n",
       "   0.2163330465555191,\n",
       "   0.3654481768608093,\n",
       "   0.008620152249932289,\n",
       "   -0.3068380355834961,\n",
       "   0.2709978222846985,\n",
       "   -0.33681634068489075,\n",
       "   -0.3318989872932434,\n",
       "   -0.039687056094408035,\n",
       "   -0.09623833000659943,\n",
       "   0.47542598843574524,\n",
       "   -0.24192781746387482,\n",
       "   -0.02896854653954506,\n",
       "   -0.25039976835250854,\n",
       "   -0.5193383693695068,\n",
       "   -0.09433285146951675,\n",
       "   0.08773978054523468,\n",
       "   -0.1476134955883026,\n",
       "   -0.1388455629348755,\n",
       "   0.15040192008018494,\n",
       "   -0.20549172163009644,\n",
       "   -0.15395480394363403,\n",
       "   -0.5282682776451111,\n",
       "   -0.37486571073532104,\n",
       "   -0.020211976021528244,\n",
       "   0.5675985217094421,\n",
       "   -0.20962335169315338,\n",
       "   -0.17298905551433563,\n",
       "   -0.2957686185836792,\n",
       "   -0.537595808506012,\n",
       "   0.1096680760383606,\n",
       "   0.3378792703151703,\n",
       "   0.4128841161727905,\n",
       "   -0.38690832257270813,\n",
       "   -0.38926154375076294,\n",
       "   -0.43024882674217224,\n",
       "   0.28501373529434204,\n",
       "   -0.1110994815826416,\n",
       "   -0.1803652048110962,\n",
       "   0.14165163040161133,\n",
       "   0.1307246834039688,\n",
       "   0.008704394102096558,\n",
       "   0.1414642333984375,\n",
       "   0.34556901454925537,\n",
       "   -0.0028865598142147064,\n",
       "   0.07913028448820114,\n",
       "   -0.8071607947349548,\n",
       "   0.13351571559906006,\n",
       "   0.3262892961502075,\n",
       "   0.009851525537669659,\n",
       "   0.1400599628686905,\n",
       "   0.22691941261291504,\n",
       "   0.02689693123102188,\n",
       "   -0.0760609358549118,\n",
       "   -0.2973061203956604,\n",
       "   0.2645648121833801,\n",
       "   0.3723449110984802,\n",
       "   0.15931203961372375,\n",
       "   0.2342642992734909,\n",
       "   -0.10927243530750275,\n",
       "   -0.17913377285003662,\n",
       "   -1.0576950311660767,\n",
       "   -0.20106425881385803,\n",
       "   0.16288095712661743,\n",
       "   0.22478853166103363,\n",
       "   0.23962782323360443,\n",
       "   0.4226122498512268,\n",
       "   -0.0597689263522625,\n",
       "   0.03742917999625206,\n",
       "   -0.14040426909923553,\n",
       "   0.4226796627044678,\n",
       "   0.012030534446239471,\n",
       "   0.06350349634885788,\n",
       "   -0.6553400158882141,\n",
       "   -0.2147091180086136,\n",
       "   -0.09395287930965424,\n",
       "   0.19316449761390686,\n",
       "   -0.16022202372550964,\n",
       "   -0.9528682231903076,\n",
       "   0.3430085778236389,\n",
       "   -0.10820533335208893,\n",
       "   0.191792830824852,\n",
       "   0.28331559896469116,\n",
       "   0.14835909008979797,\n",
       "   0.3826121985912323,\n",
       "   -0.048493392765522,\n",
       "   0.09964068233966827,\n",
       "   0.23071028292179108,\n",
       "   0.3187341094017029,\n",
       "   0.025291351601481438,\n",
       "   0.7659033536911011,\n",
       "   0.34675905108451843,\n",
       "   0.1733313798904419,\n",
       "   -0.4499678909778595,\n",
       "   0.007897356525063515,\n",
       "   0.6057370901107788,\n",
       "   -0.4392393231391907,\n",
       "   -0.6469762921333313,\n",
       "   -0.721840500831604,\n",
       "   0.24914853274822235,\n",
       "   -0.28670427203178406,\n",
       "   -0.4173502027988434,\n",
       "   0.5088405013084412,\n",
       "   0.09954096376895905,\n",
       "   1.201871633529663,\n",
       "   -0.05984175205230713,\n",
       "   -0.45122548937797546,\n",
       "   0.4939948618412018,\n",
       "   -0.14932101964950562,\n",
       "   0.48132210969924927,\n",
       "   0.12358343601226807,\n",
       "   0.18168024718761444,\n",
       "   -0.5983607172966003,\n",
       "   -0.14105753600597382,\n",
       "   -0.2880704998970032,\n",
       "   -0.07949530333280563,\n",
       "   -0.17483192682266235,\n",
       "   0.008431482128798962,\n",
       "   -0.1526171863079071,\n",
       "   -0.3572884202003479,\n",
       "   -0.13240289688110352,\n",
       "   0.34068581461906433,\n",
       "   -0.3324260115623474,\n",
       "   0.5240832567214966,\n",
       "   -0.49184536933898926,\n",
       "   0.2479841411113739,\n",
       "   0.1355980634689331,\n",
       "   -0.33832961320877075,\n",
       "   0.24623939394950867,\n",
       "   -0.6132379174232483,\n",
       "   0.295767605304718,\n",
       "   0.20410646498203278,\n",
       "   0.46706199645996094,\n",
       "   -0.19984889030456543,\n",
       "   0.5938974022865295,\n",
       "   0.3161364197731018,\n",
       "   -0.11597705632448196,\n",
       "   -0.062203023582696915,\n",
       "   0.5259625911712646,\n",
       "   0.044962555170059204,\n",
       "   0.11060675978660583,\n",
       "   -0.40935277938842773,\n",
       "   0.10711948573589325,\n",
       "   0.39742186665534973,\n",
       "   0.8131206035614014,\n",
       "   0.06368795037269592,\n",
       "   0.014794699847698212,\n",
       "   0.14655686914920807,\n",
       "   -0.20495329797267914,\n",
       "   0.5492190718650818,\n",
       "   0.1661285012960434,\n",
       "   -0.37950238585472107,\n",
       "   0.9487369060516357,\n",
       "   -0.3462059497833252,\n",
       "   -0.1204109787940979,\n",
       "   0.1707438826560974,\n",
       "   -0.10987572371959686,\n",
       "   0.43470752239227295,\n",
       "   -0.3553902208805084,\n",
       "   0.353230357170105,\n",
       "   0.1697053760290146,\n",
       "   0.23507066071033478,\n",
       "   0.03813432157039642,\n",
       "   1.064010739326477,\n",
       "   -0.3930937647819519,\n",
       "   0.2580021619796753,\n",
       "   0.08430927246809006,\n",
       "   0.2009057253599167,\n",
       "   0.6940435767173767,\n",
       "   0.20404422283172607,\n",
       "   -0.4077288508415222,\n",
       "   -0.09767656028270721,\n",
       "   0.3229098916053772,\n",
       "   -0.005515586584806442,\n",
       "   -0.44827720522880554,\n",
       "   -8.640650749206543,\n",
       "   0.6765814423561096,\n",
       "   -0.10054391622543335,\n",
       "   -0.026228591799736023,\n",
       "   0.13484510779380798,\n",
       "   0.2414715588092804,\n",
       "   0.17126473784446716,\n",
       "   -0.47975069284439087,\n",
       "   -0.21327745914459229,\n",
       "   -0.32257524132728577,\n",
       "   -0.12264887988567352,\n",
       "   -0.017789416015148163,\n",
       "   0.054067425429821014,\n",
       "   -0.08259554952383041,\n",
       "   0.40824273228645325,\n",
       "   -0.34142300486564636,\n",
       "   0.13753506541252136,\n",
       "   0.09191043674945831,\n",
       "   0.6397036910057068,\n",
       "   -0.3687806725502014,\n",
       "   -0.16832715272903442,\n",
       "   0.029923072084784508,\n",
       "   -0.05979721248149872,\n",
       "   0.7439448833465576,\n",
       "   -0.20347030460834503,\n",
       "   0.8705406188964844,\n",
       "   0.35516923666000366,\n",
       "   0.14089378714561462,\n",
       "   0.44728127121925354,\n",
       "   -0.1266111433506012,\n",
       "   -0.26788657903671265,\n",
       "   0.3026527166366577,\n",
       "   -0.07628771662712097,\n",
       "   -0.171425923705101,\n",
       "   -0.013351199217140675,\n",
       "   0.05043739080429077,\n",
       "   0.08545905351638794,\n",
       "   -0.21878398954868317,\n",
       "   -0.3192085027694702,\n",
       "   -0.8778048753738403,\n",
       "   0.23840367794036865,\n",
       "   -0.39746958017349243,\n",
       "   -0.09175156056880951,\n",
       "   0.1525871902704239,\n",
       "   0.5591398477554321,\n",
       "   0.31252747774124146,\n",
       "   -0.03051839768886566,\n",
       "   0.6418724060058594,\n",
       "   0.12537813186645508,\n",
       "   0.25318285822868347,\n",
       "   0.4925338625907898,\n",
       "   0.4097660183906555,\n",
       "   0.20669707655906677,\n",
       "   0.3484807014465332,\n",
       "   -0.12764927744865417,\n",
       "   0.1588560938835144,\n",
       "   -0.397964745759964,\n",
       "   0.6651546955108643,\n",
       "   0.0840437114238739,\n",
       "   -0.8051371574401855,\n",
       "   -0.15659789741039276,\n",
       "   -0.17412704229354858,\n",
       "   0.6929265260696411,\n",
       "   0.07764758914709091,\n",
       "   -0.4472368359565735,\n",
       "   -0.11799927055835724,\n",
       "   -0.42847275733947754,\n",
       "   -0.07523144036531448,\n",
       "   0.04148450121283531,\n",
       "   -0.5630865693092346,\n",
       "   -0.5472521781921387,\n",
       "   -0.2646229863166809,\n",
       "   0.04557168483734131,\n",
       "   -0.3560698628425598,\n",
       "   0.29251089692115784,\n",
       "   -0.6638208627700806,\n",
       "   -0.1317272186279297,\n",
       "   0.10895010828971863,\n",
       "   -0.34364333748817444,\n",
       "   0.38191214203834534,\n",
       "   -0.18354420363903046,\n",
       "   0.340742826461792,\n",
       "   0.43707871437072754,\n",
       "   -0.6676289439201355,\n",
       "   -0.10450421273708344,\n",
       "   0.010519087314605713,\n",
       "   -0.4491731524467468,\n",
       "   0.4352806806564331,\n",
       "   0.25668013095855713,\n",
       "   -0.09171677380800247,\n",
       "   -0.26746585965156555,\n",
       "   0.0978890135884285,\n",
       "   -0.05004033446311951,\n",
       "   -0.03508389741182327,\n",
       "   0.07424032688140869,\n",
       "   -0.04703884571790695,\n",
       "   -0.14557473361492157,\n",
       "   0.19091582298278809,\n",
       "   -0.004856714978814125,\n",
       "   0.9004969000816345,\n",
       "   -0.3904283046722412,\n",
       "   -0.20269885659217834,\n",
       "   -0.428933322429657,\n",
       "   0.09756722301244736,\n",
       "   -0.08463997393846512,\n",
       "   -0.1271873563528061,\n",
       "   0.11188575625419617,\n",
       "   -0.24016332626342773,\n",
       "   -0.011290337890386581,\n",
       "   -0.030286414548754692,\n",
       "   -0.1716756522655487,\n",
       "   0.4323950707912445,\n",
       "   0.7464092969894409,\n",
       "   -0.029980439692735672,\n",
       "   -0.09443026781082153,\n",
       "   0.4587255120277405,\n",
       "   0.3875080645084381,\n",
       "   -0.21265220642089844,\n",
       "   -0.07020820677280426,\n",
       "   -0.2500966787338257,\n",
       "   -0.34350502490997314,\n",
       "   -0.6404615044593811,\n",
       "   0.4385434091091156,\n",
       "   -0.3225851058959961,\n",
       "   0.32572996616363525,\n",
       "   0.41433313488960266,\n",
       "   -0.076567143201828,\n",
       "   0.17829588055610657,\n",
       "   0.7073309421539307,\n",
       "   -0.04004296660423279,\n",
       "   0.6580303907394409,\n",
       "   0.17214646935462952,\n",
       "   -0.4292680323123932,\n",
       "   0.13446944952011108,\n",
       "   0.17528891563415527,\n",
       "   -0.21408991515636444,\n",
       "   0.0015280337538570166,\n",
       "   -0.19579201936721802,\n",
       "   -0.35962235927581787,\n",
       "   -0.12266519665718079,\n",
       "   -0.6709005236625671,\n",
       "   -0.013883143663406372,\n",
       "   0.018594728782773018,\n",
       "   0.35368651151657104,\n",
       "   0.6433117389678955,\n",
       "   -0.2519233226776123,\n",
       "   0.06898944079875946,\n",
       "   -0.14182919263839722,\n",
       "   -0.51372891664505,\n",
       "   -0.45439162850379944,\n",
       "   -0.1687382161617279,\n",
       "   0.18941254913806915,\n",
       "   0.6594247817993164,\n",
       "   0.24381333589553833,\n",
       "   0.6103264689445496,\n",
       "   -0.293788880109787,\n",
       "   0.33716273307800293,\n",
       "   1.062076449394226,\n",
       "   -0.3390970230102539,\n",
       "   -0.07685025781393051,\n",
       "   -0.20470888912677765,\n",
       "   -0.09979225695133209,\n",
       "   0.2805849611759186,\n",
       "   0.24900397658348083,\n",
       "   -0.14818865060806274,\n",
       "   0.27832847833633423,\n",
       "   -0.15085242688655853,\n",
       "   0.1657445728778839,\n",
       "   0.10170674324035645,\n",
       "   0.2509639859199524,\n",
       "   0.058350276201963425,\n",
       "   -0.2439677119255066,\n",
       "   0.3004995584487915,\n",
       "   0.17075994610786438,\n",
       "   0.16095508635044098,\n",
       "   -0.2560756504535675,\n",
       "   0.46377429366111755,\n",
       "   0.05866324156522751,\n",
       "   0.20186054706573486,\n",
       "   0.09791434556245804,\n",
       "   0.9664278626441956,\n",
       "   0.5513371229171753,\n",
       "   -0.0750991627573967,\n",
       "   0.029834911227226257,\n",
       "   -0.4979154169559479,\n",
       "   0.03459085896611214,\n",
       "   0.4288223385810852,\n",
       "   0.26169896125793457,\n",
       "   -0.04570074751973152,\n",
       "   -0.10850796103477478,\n",
       "   0.2415260374546051,\n",
       "   0.2013421207666397,\n",
       "   -0.3603683412075043,\n",
       "   0.48495861887931824,\n",
       "   -0.12864528596401215,\n",
       "   0.20942260324954987,\n",
       "   -0.2292463779449463,\n",
       "   -0.35657331347465515,\n",
       "   0.14567643404006958,\n",
       "   -0.014473788440227509,\n",
       "   -0.08789676427841187,\n",
       "   -0.14937156438827515,\n",
       "   0.3680244982242584,\n",
       "   -0.32440993189811707,\n",
       "   -0.3256990313529968,\n",
       "   0.41314929723739624,\n",
       "   0.24584858119487762,\n",
       "   -0.16232867538928986,\n",
       "   0.4430101811885834,\n",
       "   0.2159249633550644,\n",
       "   0.2870866656303406],\n",
       "  [0.2704886794090271,\n",
       "   0.1566823124885559,\n",
       "   -0.21538905799388885,\n",
       "   -0.1682988405227661,\n",
       "   0.6598247289657593,\n",
       "   -0.24779370427131653,\n",
       "   0.3411739468574524,\n",
       "   0.015212547034025192,\n",
       "   -0.15180672705173492,\n",
       "   0.2622115910053253,\n",
       "   0.005026798229664564,\n",
       "   0.3260762095451355,\n",
       "   -0.17043808102607727,\n",
       "   0.13932707905769348,\n",
       "   -0.8387945890426636,\n",
       "   0.03736183047294617,\n",
       "   0.28406983613967896,\n",
       "   0.005601232871413231,\n",
       "   -0.02556341141462326,\n",
       "   0.15819965302944183,\n",
       "   0.07068724185228348,\n",
       "   -0.15886619687080383,\n",
       "   0.2419627159833908,\n",
       "   0.0526663213968277,\n",
       "   0.1822461485862732,\n",
       "   0.2875427007675171,\n",
       "   0.3267199993133545,\n",
       "   0.8491674065589905,\n",
       "   -0.17913688719272614,\n",
       "   0.5304557681083679,\n",
       "   0.3548372983932495,\n",
       "   0.21336053311824799,\n",
       "   0.319774329662323,\n",
       "   -0.024873118847608566,\n",
       "   -0.11033980548381805,\n",
       "   0.03818224370479584,\n",
       "   -0.19125667214393616,\n",
       "   0.2971961498260498,\n",
       "   -0.23124311864376068,\n",
       "   0.12048661708831787,\n",
       "   -0.04031930863857269,\n",
       "   -0.1144927367568016,\n",
       "   -0.02399587631225586,\n",
       "   0.0030200500041246414,\n",
       "   -0.0179576575756073,\n",
       "   -0.13359926640987396,\n",
       "   -0.32513442635536194,\n",
       "   -0.1704351156949997,\n",
       "   0.15771669149398804,\n",
       "   0.32675594091415405,\n",
       "   -0.008188805542886257,\n",
       "   -0.5808804035186768,\n",
       "   -0.24822098016738892,\n",
       "   -0.4027474522590637,\n",
       "   -0.06865239143371582,\n",
       "   -0.16874267160892487,\n",
       "   -0.041234374046325684,\n",
       "   -0.17978882789611816,\n",
       "   -0.5146223306655884,\n",
       "   0.3718055784702301,\n",
       "   0.12456697970628738,\n",
       "   0.16852335631847382,\n",
       "   0.6325780153274536,\n",
       "   0.11719654500484467,\n",
       "   -0.1868644654750824,\n",
       "   0.2513582110404968,\n",
       "   0.0014542806893587112,\n",
       "   -0.12507790327072144,\n",
       "   0.19940760731697083,\n",
       "   -0.1260051131248474,\n",
       "   0.009528331458568573,\n",
       "   0.021739132702350616,\n",
       "   -0.03098301589488983,\n",
       "   -0.39722877740859985,\n",
       "   0.4722896218299866,\n",
       "   0.000599399209022522,\n",
       "   0.28818070888519287,\n",
       "   -0.34668880701065063,\n",
       "   -0.35326284170150757,\n",
       "   -0.032103367149829865,\n",
       "   0.4062965512275696,\n",
       "   0.27041903138160706,\n",
       "   0.09890832006931305,\n",
       "   0.3141527473926544,\n",
       "   0.2502535581588745,\n",
       "   -0.010308269411325455,\n",
       "   0.16103708744049072,\n",
       "   0.19846481084823608,\n",
       "   0.1569751352071762,\n",
       "   0.34240254759788513,\n",
       "   -0.0988336056470871,\n",
       "   -0.24419960379600525,\n",
       "   0.06396070122718811,\n",
       "   0.1631624400615692,\n",
       "   -0.41740143299102783,\n",
       "   0.24066618084907532,\n",
       "   -0.4262520670890808,\n",
       "   0.22274009883403778,\n",
       "   -0.03753102198243141,\n",
       "   0.04393406957387924,\n",
       "   -0.13467518985271454,\n",
       "   -0.19136378169059753,\n",
       "   0.2777547240257263,\n",
       "   -0.1386456936597824,\n",
       "   0.1524628847837448,\n",
       "   -0.27373969554901123,\n",
       "   -0.5061299800872803,\n",
       "   -0.7519994378089905,\n",
       "   -0.5665057897567749,\n",
       "   0.35712677240371704,\n",
       "   0.05391548201441765,\n",
       "   0.6686204671859741,\n",
       "   0.40178149938583374,\n",
       "   -0.036407504230737686,\n",
       "   -0.49917834997177124,\n",
       "   -0.12251250445842743,\n",
       "   0.15181942284107208,\n",
       "   0.34845608472824097,\n",
       "   -0.04023643955588341,\n",
       "   -0.3909124732017517,\n",
       "   0.4183999001979828,\n",
       "   0.3448883593082428,\n",
       "   -0.025989536195993423,\n",
       "   -0.22647324204444885,\n",
       "   0.027883607894182205,\n",
       "   -0.0025201458483934402,\n",
       "   -0.18043550848960876,\n",
       "   -0.14192940294742584,\n",
       "   0.23908860981464386,\n",
       "   0.13966432213783264,\n",
       "   -0.2786041498184204,\n",
       "   -0.41809314489364624,\n",
       "   -0.005834324285387993,\n",
       "   0.4547346234321594,\n",
       "   0.04873117059469223,\n",
       "   0.12085504084825516,\n",
       "   -0.32972195744514465,\n",
       "   0.03415318951010704,\n",
       "   -0.9670757055282593,\n",
       "   0.07584864646196365,\n",
       "   -0.4497148096561432,\n",
       "   0.10873766988515854,\n",
       "   0.46784088015556335,\n",
       "   -0.1628398448228836,\n",
       "   -0.04060189053416252,\n",
       "   0.08491174876689911,\n",
       "   -0.20084330439567566,\n",
       "   -0.0955774262547493,\n",
       "   0.39163637161254883,\n",
       "   0.2993851900100708,\n",
       "   -0.38535359501838684,\n",
       "   0.18919913470745087,\n",
       "   0.048753585666418076,\n",
       "   0.06813309341669083,\n",
       "   -0.2973161041736603,\n",
       "   -0.11096075177192688,\n",
       "   0.2712802290916443,\n",
       "   -0.7036789655685425,\n",
       "   0.8165759444236755,\n",
       "   0.24340441823005676,\n",
       "   -0.29430124163627625,\n",
       "   -0.45955371856689453,\n",
       "   -0.42438700795173645,\n",
       "   0.363910973072052,\n",
       "   0.25375375151634216,\n",
       "   -0.6121051907539368,\n",
       "   -0.18234893679618835,\n",
       "   0.2944416403770447,\n",
       "   -0.11774744838476181,\n",
       "   0.6756160259246826,\n",
       "   0.4874635636806488,\n",
       "   0.23089975118637085,\n",
       "   -0.23773148655891418,\n",
       "   -0.06251415610313416,\n",
       "   0.1624113768339157,\n",
       "   0.33460405468940735,\n",
       "   0.25556814670562744,\n",
       "   -0.24654434621334076,\n",
       "   0.30701255798339844,\n",
       "   0.04956749081611633,\n",
       "   0.05037713795900345,\n",
       "   -0.04486864432692528,\n",
       "   -0.27035561203956604,\n",
       "   -0.2105223536491394,\n",
       "   -0.13134117424488068,\n",
       "   -0.23428790271282196,\n",
       "   0.14279904961585999,\n",
       "   -0.7091219425201416,\n",
       "   0.6887481212615967,\n",
       "   -0.0779346227645874,\n",
       "   0.07293063402175903,\n",
       "   -0.3115409314632416,\n",
       "   -0.7176701426506042,\n",
       "   -0.1034473329782486,\n",
       "   -0.840051531791687,\n",
       "   0.17481407523155212,\n",
       "   0.31539952754974365,\n",
       "   0.20919573307037354,\n",
       "   0.07392890006303787,\n",
       "   0.7409322261810303,\n",
       "   -0.16797995567321777,\n",
       "   0.11457137763500214,\n",
       "   -0.06842182576656342,\n",
       "   0.07183711975812912,\n",
       "   0.056102920323610306,\n",
       "   0.021548796445131302,\n",
       "   -0.36041802167892456,\n",
       "   0.5176116228103638,\n",
       "   -0.18345887959003448,\n",
       "   -0.4168425500392914,\n",
       "   -0.1785297840833664,\n",
       "   -0.34320127964019775,\n",
       "   0.10425812005996704,\n",
       "   0.0008234090637415648,\n",
       "   0.3267163038253784,\n",
       "   -0.3826327621936798,\n",
       "   0.17936156690120697,\n",
       "   -0.3918708860874176,\n",
       "   0.17378753423690796,\n",
       "   0.08455376327037811,\n",
       "   -0.3839455246925354,\n",
       "   -0.022221334278583527,\n",
       "   0.6806154847145081,\n",
       "   -0.13604961335659027,\n",
       "   -0.03499992936849594,\n",
       "   0.7893762588500977,\n",
       "   -0.4820176959037781,\n",
       "   0.44465112686157227,\n",
       "   0.4689665138721466,\n",
       "   -0.015164092183113098,\n",
       "   0.19094814360141754,\n",
       "   0.28033992648124695,\n",
       "   -0.02647395431995392,\n",
       "   -0.38848569989204407,\n",
       "   -0.280535489320755,\n",
       "   0.5652729868888855,\n",
       "   -0.1907440721988678,\n",
       "   -0.18350915610790253,\n",
       "   -0.2625311017036438,\n",
       "   -0.5773023366928101,\n",
       "   0.17415225505828857,\n",
       "   -0.005111483857035637,\n",
       "   0.6079854369163513,\n",
       "   -0.05640910565853119,\n",
       "   -0.17008338868618011,\n",
       "   -0.11082789301872253,\n",
       "   -0.1114121824502945,\n",
       "   0.4051685929298401,\n",
       "   -0.11774897575378418,\n",
       "   0.2437858283519745,\n",
       "   -0.32409176230430603,\n",
       "   -0.07127013057470322,\n",
       "   -0.3878590762615204,\n",
       "   -0.05410388112068176,\n",
       "   0.23277127742767334,\n",
       "   -0.21814367175102234,\n",
       "   0.5615976452827454,\n",
       "   -0.11108849942684174,\n",
       "   0.05532734841108322,\n",
       "   -0.5107297897338867,\n",
       "   0.5725879669189453,\n",
       "   0.013936668634414673,\n",
       "   0.2869582772254944,\n",
       "   -0.1621047854423523,\n",
       "   -0.07882056385278702,\n",
       "   -0.11265058815479279,\n",
       "   -0.48599931597709656,\n",
       "   -0.049304328858852386,\n",
       "   -0.5126754641532898,\n",
       "   0.4313808083534241,\n",
       "   0.09454551339149475,\n",
       "   -0.09012667834758759,\n",
       "   0.386542409658432,\n",
       "   0.14490285515785217,\n",
       "   -0.40296992659568787,\n",
       "   0.5476503968238831,\n",
       "   0.3105768859386444,\n",
       "   -0.43489915132522583,\n",
       "   0.13938434422016144,\n",
       "   -0.10260443389415741,\n",
       "   0.35837966203689575,\n",
       "   0.6478508710861206,\n",
       "   -0.33212023973464966,\n",
       "   0.5317214727401733,\n",
       "   -0.5623787045478821,\n",
       "   -0.5293629765510559,\n",
       "   0.1615431159734726,\n",
       "   0.7328401207923889,\n",
       "   0.14152468740940094,\n",
       "   -0.11595308780670166,\n",
       "   0.04774976521730423,\n",
       "   -0.33966290950775146,\n",
       "   -0.2776387929916382,\n",
       "   0.5422871112823486,\n",
       "   0.3527224361896515,\n",
       "   0.06423962861299515,\n",
       "   -0.32937929034233093,\n",
       "   0.5074755549430847,\n",
       "   -0.426339328289032,\n",
       "   -0.27202534675598145,\n",
       "   0.20316465198993683,\n",
       "   -0.025758519768714905,\n",
       "   0.33031216263771057,\n",
       "   0.04340098053216934,\n",
       "   -0.6946107149124146,\n",
       "   0.631272554397583,\n",
       "   0.10478425025939941,\n",
       "   0.22381383180618286,\n",
       "   0.24453724920749664,\n",
       "   0.490062952041626,\n",
       "   0.4695611298084259,\n",
       "   0.22519251704216003,\n",
       "   -0.10173334181308746,\n",
       "   -0.033450283110141754,\n",
       "   -0.14090260863304138,\n",
       "   -0.3817376494407654,\n",
       "   -0.04903392493724823,\n",
       "   0.01266505103558302,\n",
       "   0.0009605591767467558,\n",
       "   0.030627213418483734,\n",
       "   -0.17692919075489044,\n",
       "   0.49908170104026794,\n",
       "   -0.4158218801021576,\n",
       "   0.22434118390083313,\n",
       "   -0.05617360770702362,\n",
       "   0.13888558745384216,\n",
       "   -0.093697190284729,\n",
       "   0.487576425075531,\n",
       "   -0.1011454313993454,\n",
       "   -0.22522152960300446,\n",
       "   -0.10824268311262131,\n",
       "   -0.10733242332935333,\n",
       "   -0.10540486872196198,\n",
       "   0.3956370949745178,\n",
       "   -0.11213983595371246,\n",
       "   0.06164196878671646,\n",
       "   0.024327319115400314,\n",
       "   0.47014352679252625,\n",
       "   -0.5488750338554382,\n",
       "   0.25769609212875366,\n",
       "   0.015597447752952576,\n",
       "   -0.3209717273712158,\n",
       "   0.11687397956848145,\n",
       "   -0.3522021472454071,\n",
       "   -0.11630909144878387,\n",
       "   0.31123822927474976,\n",
       "   0.39252203702926636,\n",
       "   0.03631673753261566,\n",
       "   0.3067030608654022,\n",
       "   -0.2366953194141388,\n",
       "   -0.3549073040485382,\n",
       "   0.08126778900623322,\n",
       "   -0.17264586687088013,\n",
       "   -0.29193365573883057,\n",
       "   0.3777898848056793,\n",
       "   0.43065038323402405,\n",
       "   0.06798220425844193,\n",
       "   0.09067808836698532,\n",
       "   0.1806594729423523,\n",
       "   -0.12497542798519135,\n",
       "   -0.1802048683166504,\n",
       "   -0.2707483172416687,\n",
       "   0.05156522989273071,\n",
       "   -0.0996243953704834,\n",
       "   -0.20803341269493103,\n",
       "   -0.057090289890766144,\n",
       "   -0.5023208260536194,\n",
       "   -0.2920582592487335,\n",
       "   0.06015899032354355,\n",
       "   0.23459912836551666,\n",
       "   0.5431785583496094,\n",
       "   -0.3666068911552429,\n",
       "   0.5095208287239075,\n",
       "   0.3262036144733429,\n",
       "   0.30905354022979736,\n",
       "   -0.03678470104932785,\n",
       "   -0.024213619530200958,\n",
       "   0.2630684971809387,\n",
       "   0.1829446405172348,\n",
       "   -0.8472191095352173,\n",
       "   0.1861332356929779,\n",
       "   0.010789856314659119,\n",
       "   -0.1913791298866272,\n",
       "   0.048625648021698,\n",
       "   0.22032694518566132,\n",
       "   -0.07183392345905304,\n",
       "   0.058266062289476395,\n",
       "   -0.3075858950614929,\n",
       "   0.5217293500900269,\n",
       "   -0.29008105397224426,\n",
       "   0.2820415496826172,\n",
       "   -0.05290251225233078,\n",
       "   0.2970666289329529,\n",
       "   -0.3682680130004883,\n",
       "   0.1739921271800995,\n",
       "   0.03266352415084839,\n",
       "   -0.6018847227096558,\n",
       "   0.5473198294639587,\n",
       "   0.2560291886329651,\n",
       "   -0.34285062551498413,\n",
       "   0.16273319721221924,\n",
       "   0.25029078125953674,\n",
       "   -0.2144850790500641,\n",
       "   0.2665657699108124,\n",
       "   -0.6228355169296265,\n",
       "   -0.322553813457489,\n",
       "   -0.016143951565027237,\n",
       "   0.42706817388534546,\n",
       "   0.013884201645851135,\n",
       "   -0.928333580493927,\n",
       "   0.05445301532745361,\n",
       "   -0.4405582547187805,\n",
       "   0.28376665711402893,\n",
       "   0.8677456378936768,\n",
       "   0.18354621529579163,\n",
       "   -0.15333053469657898,\n",
       "   0.17678894102573395,\n",
       "   -0.21311818063259125,\n",
       "   0.02845628187060356,\n",
       "   0.22995437681674957,\n",
       "   -0.16822466254234314,\n",
       "   -0.47774189710617065,\n",
       "   -0.25052139163017273,\n",
       "   -0.2404152750968933,\n",
       "   0.44111698865890503,\n",
       "   0.20260241627693176,\n",
       "   -0.060364797711372375,\n",
       "   -0.24747230112552643,\n",
       "   -0.21246221661567688,\n",
       "   -0.0035353824496269226,\n",
       "   -0.0622434988617897,\n",
       "   -0.07603651285171509,\n",
       "   0.06815055012702942,\n",
       "   0.36106282472610474,\n",
       "   -0.019025608897209167,\n",
       "   0.2320886105298996,\n",
       "   -0.057168252766132355,\n",
       "   0.18874657154083252,\n",
       "   0.052220091223716736,\n",
       "   -0.2804723381996155,\n",
       "   0.1886344850063324,\n",
       "   -0.5492249131202698,\n",
       "   0.05304458737373352,\n",
       "   -0.8842459917068481,\n",
       "   0.22142449021339417,\n",
       "   -0.26619669795036316,\n",
       "   -0.1766321063041687,\n",
       "   0.24325670301914215,\n",
       "   -0.12930549681186676,\n",
       "   -0.11929599195718765,\n",
       "   0.4341117739677429,\n",
       "   0.09164784103631973,\n",
       "   0.3453877866268158,\n",
       "   0.20361275970935822,\n",
       "   0.3434422016143799,\n",
       "   0.08104641735553741,\n",
       "   -0.36214160919189453,\n",
       "   -0.13482385873794556,\n",
       "   -0.6124111413955688,\n",
       "   0.16778460144996643,\n",
       "   -0.24922023713588715,\n",
       "   0.45567208528518677,\n",
       "   -2.0850449800491333e-05,\n",
       "   0.2646808624267578,\n",
       "   0.210994690656662,\n",
       "   0.3289873003959656,\n",
       "   -0.29379814863204956,\n",
       "   0.7859099507331848,\n",
       "   -0.06815662235021591,\n",
       "   0.2621147632598877,\n",
       "   0.09641887247562408,\n",
       "   0.3679993450641632,\n",
       "   0.13283714652061462,\n",
       "   0.9933958649635315,\n",
       "   -0.03778510540723801,\n",
       "   0.3191245496273041,\n",
       "   0.1793188899755478,\n",
       "   -0.19380727410316467,\n",
       "   -0.3432970941066742,\n",
       "   0.22006292641162872,\n",
       "   -0.329424649477005,\n",
       "   0.16755318641662598,\n",
       "   -0.3149672746658325,\n",
       "   0.03364793583750725,\n",
       "   0.09576815366744995,\n",
       "   0.27099281549453735,\n",
       "   0.9067674875259399,\n",
       "   0.24268940091133118,\n",
       "   0.11451918631792068,\n",
       "   0.41060948371887207,\n",
       "   0.2744537889957428,\n",
       "   0.10705803334712982,\n",
       "   -0.03656607121229172,\n",
       "   -0.22545522451400757,\n",
       "   0.20897772908210754,\n",
       "   0.49156811833381653,\n",
       "   0.08622303605079651,\n",
       "   0.22099992632865906,\n",
       "   0.4244517683982849,\n",
       "   0.1381080001592636,\n",
       "   -0.29599255323410034,\n",
       "   0.04071444272994995,\n",
       "   -0.13757967948913574,\n",
       "   0.164602592587471,\n",
       "   -0.6933709979057312,\n",
       "   0.46627819538116455,\n",
       "   -0.028909839689731598,\n",
       "   0.0657513216137886,\n",
       "   -0.03242658078670502,\n",
       "   0.0625639259815216,\n",
       "   -0.13140769302845,\n",
       "   -0.5594701170921326,\n",
       "   -0.3002561926841736,\n",
       "   -0.1959654539823532,\n",
       "   -0.13473686575889587,\n",
       "   0.1879425346851349,\n",
       "   0.4960440695285797,\n",
       "   0.10643720626831055,\n",
       "   0.12519927322864532,\n",
       "   -0.03961000218987465,\n",
       "   0.39036276936531067,\n",
       "   0.10821356624364853,\n",
       "   -0.17056864500045776,\n",
       "   0.0803183764219284,\n",
       "   0.6037553548812866,\n",
       "   -0.25391685962677,\n",
       "   0.4437236189842224,\n",
       "   0.26769381761550903,\n",
       "   -0.16261041164398193,\n",
       "   -0.01800655573606491,\n",
       "   -0.31490805745124817,\n",
       "   -0.0989469587802887,\n",
       "   0.13392488658428192,\n",
       "   -0.37458527088165283,\n",
       "   1.1765952110290527,\n",
       "   0.2271738052368164,\n",
       "   0.13422483205795288,\n",
       "   -0.1414569914340973,\n",
       "   -0.052575141191482544,\n",
       "   -0.12496037781238556,\n",
       "   0.0035108570009469986,\n",
       "   0.7553285360336304,\n",
       "   0.10820457339286804,\n",
       "   0.31130868196487427,\n",
       "   0.007499172352254391,\n",
       "   0.4806962311267853,\n",
       "   0.10182788968086243,\n",
       "   0.531169056892395,\n",
       "   0.06917552649974823,\n",
       "   0.1388348788022995,\n",
       "   0.3566826581954956,\n",
       "   0.40600600838661194,\n",
       "   -0.7701282501220703,\n",
       "   0.43623149394989014,\n",
       "   0.6099730730056763,\n",
       "   0.5601497888565063,\n",
       "   -0.517615795135498,\n",
       "   -8.882248878479004,\n",
       "   0.08591849356889725,\n",
       "   0.19002863764762878,\n",
       "   0.0005193501710891724,\n",
       "   0.060141414403915405,\n",
       "   0.15132899582386017,\n",
       "   0.3351224660873413,\n",
       "   -0.21233238279819489,\n",
       "   0.3100104331970215,\n",
       "   0.16984966397285461,\n",
       "   -0.09123256057500839,\n",
       "   -0.16460020840168,\n",
       "   0.4975760877132416,\n",
       "   -0.5497260093688965,\n",
       "   0.039794981479644775,\n",
       "   0.0012766644358634949,\n",
       "   -0.14203688502311707,\n",
       "   -0.0694410428404808,\n",
       "   0.16114671528339386,\n",
       "   -0.22323757410049438,\n",
       "   -0.619208812713623,\n",
       "   -0.029662000015378,\n",
       "   0.1993045061826706,\n",
       "   0.25604596734046936,\n",
       "   0.0004924740642309189,\n",
       "   0.23917829990386963,\n",
       "   -0.25220319628715515,\n",
       "   -0.19692417979240417,\n",
       "   -0.06634712219238281,\n",
       "   -0.37312954664230347,\n",
       "   -0.24087361991405487,\n",
       "   -0.2192090004682541,\n",
       "   -0.1971619725227356,\n",
       "   -0.01518610306084156,\n",
       "   0.048405490815639496,\n",
       "   -0.2503746449947357,\n",
       "   0.10146388411521912,\n",
       "   0.17023923993110657,\n",
       "   -0.14209234714508057,\n",
       "   -0.37562963366508484,\n",
       "   -0.23042599856853485,\n",
       "   0.16234327852725983,\n",
       "   0.4302489757537842,\n",
       "   -0.10344833135604858,\n",
       "   0.19775883853435516,\n",
       "   -0.11231027543544769,\n",
       "   0.42236584424972534,\n",
       "   0.23827405273914337,\n",
       "   0.08778645098209381,\n",
       "   0.4836826026439667,\n",
       "   0.5206186175346375,\n",
       "   0.30554208159446716,\n",
       "   -0.06565023213624954,\n",
       "   -0.16468869149684906,\n",
       "   -0.5692189931869507,\n",
       "   -0.11385385692119598,\n",
       "   -0.13268160820007324,\n",
       "   0.38718873262405396,\n",
       "   -0.3004990518093109,\n",
       "   0.0009195404127240181,\n",
       "   -0.183148592710495,\n",
       "   -0.4069943130016327,\n",
       "   0.40062153339385986,\n",
       "   -0.32803574204444885,\n",
       "   -0.3620862364768982,\n",
       "   0.06971757113933563,\n",
       "   -0.6421958208084106,\n",
       "   0.14855174720287323,\n",
       "   0.06987316906452179,\n",
       "   -0.0810796469449997,\n",
       "   -0.530967116355896,\n",
       "   -0.5628446936607361,\n",
       "   -0.17616531252861023,\n",
       "   0.29762566089630127,\n",
       "   -0.04058805853128433,\n",
       "   -0.4840908348560333,\n",
       "   -0.6053864359855652,\n",
       "   0.15387138724327087,\n",
       "   -0.3144035041332245,\n",
       "   0.019195493310689926,\n",
       "   -0.8628644347190857,\n",
       "   0.8346043229103088,\n",
       "   0.5519158244132996,\n",
       "   0.0058332644402980804,\n",
       "   -0.0809682309627533,\n",
       "   -0.3248246908187866,\n",
       "   -0.15598809719085693,\n",
       "   0.15913918614387512,\n",
       "   -0.06319830566644669,\n",
       "   0.3458784520626068,\n",
       "   0.0007766233757138252,\n",
       "   -0.34493666887283325,\n",
       "   -0.198076993227005,\n",
       "   -0.050007015466690063,\n",
       "   -0.11227385699748993,\n",
       "   0.05673523247241974,\n",
       "   -0.3678136169910431,\n",
       "   -0.3305043578147888,\n",
       "   0.3967895209789276,\n",
       "   -0.3602333068847656,\n",
       "   -0.6372108459472656,\n",
       "   -0.12956301867961884,\n",
       "   -0.26282450556755066,\n",
       "   0.10332492738962173,\n",
       "   -0.20214855670928955,\n",
       "   -0.3992137908935547,\n",
       "   0.31066542863845825,\n",
       "   -0.4703993499279022,\n",
       "   0.005400069057941437,\n",
       "   -0.16022594273090363,\n",
       "   0.1288691759109497,\n",
       "   0.26433131098747253,\n",
       "   0.4062960743904114,\n",
       "   -0.5570682287216187,\n",
       "   -0.23155690729618073,\n",
       "   0.04284469783306122,\n",
       "   0.9930727481842041,\n",
       "   0.010312322527170181,\n",
       "   0.05796196311712265,\n",
       "   -0.052084848284721375,\n",
       "   -0.07781287282705307,\n",
       "   -0.5032780766487122,\n",
       "   0.24828630685806274,\n",
       "   0.33959928154945374,\n",
       "   0.16209416091442108,\n",
       "   0.05484549328684807,\n",
       "   0.04987585172057152,\n",
       "   0.050220876932144165,\n",
       "   0.09724556654691696,\n",
       "   -0.14259430766105652,\n",
       "   0.551369309425354,\n",
       "   0.01114986278116703,\n",
       "   -0.011863507330417633,\n",
       "   0.3522879481315613,\n",
       "   0.23027735948562622,\n",
       "   -0.27351289987564087,\n",
       "   0.04976114630699158,\n",
       "   -0.39139774441719055,\n",
       "   -0.30829304456710815,\n",
       "   -0.10738890618085861,\n",
       "   -0.5475696325302124,\n",
       "   -0.040237292647361755,\n",
       "   0.41573086380958557,\n",
       "   0.06600780785083771,\n",
       "   0.2638890743255615,\n",
       "   -0.4750364422798157,\n",
       "   -0.053101275116205215,\n",
       "   0.1339903473854065,\n",
       "   -0.1496349573135376,\n",
       "   0.12503552436828613,\n",
       "   -0.4453997015953064,\n",
       "   -0.020008210092782974,\n",
       "   0.10009966790676117,\n",
       "   -0.08962764590978622,\n",
       "   0.28157705068588257,\n",
       "   -0.143597811460495,\n",
       "   0.19875942170619965,\n",
       "   0.06333236396312714,\n",
       "   -0.23100942373275757,\n",
       "   0.05210765451192856,\n",
       "   -0.3070213794708252,\n",
       "   0.12845642864704132,\n",
       "   -0.019364716485142708,\n",
       "   0.10840274393558502,\n",
       "   -0.2854314148426056,\n",
       "   0.09791941940784454,\n",
       "   -0.3957376182079315,\n",
       "   0.3237033188343048,\n",
       "   0.4067329168319702,\n",
       "   -0.3031899631023407,\n",
       "   0.2642916440963745,\n",
       "   0.18984979391098022,\n",
       "   -0.14861300587654114,\n",
       "   0.22332783043384552,\n",
       "   0.029322480782866478,\n",
       "   -0.31486085057258606,\n",
       "   0.12900410592556,\n",
       "   -0.567405641078949,\n",
       "   -0.34739887714385986,\n",
       "   -0.1631484031677246,\n",
       "   0.10660899430513382,\n",
       "   -0.20729075372219086,\n",
       "   -0.45603445172309875,\n",
       "   -0.14815977215766907,\n",
       "   -0.26166900992393494,\n",
       "   0.1407368779182434,\n",
       "   0.027411293238401413,\n",
       "   0.15168021619319916,\n",
       "   0.15766385197639465,\n",
       "   0.17745599150657654,\n",
       "   -0.04731868952512741,\n",
       "   0.22014392912387848,\n",
       "   -0.6324774026870728,\n",
       "   0.07844533026218414,\n",
       "   -0.05542334169149399,\n",
       "   -0.06431859731674194,\n",
       "   -0.1477411389350891,\n",
       "   0.29430773854255676,\n",
       "   -0.28860440850257874,\n",
       "   0.049443937838077545,\n",
       "   -0.00862819328904152,\n",
       "   -0.16417992115020752,\n",
       "   -0.30130037665367126,\n",
       "   -0.41882914304733276,\n",
       "   0.10776631534099579,\n",
       "   -0.4128093719482422,\n",
       "   0.1078711524605751,\n",
       "   -0.05177167430520058,\n",
       "   -0.08718062937259674,\n",
       "   0.3165355920791626,\n",
       "   -0.023058179765939713],\n",
       "  [0.9966174364089966,\n",
       "   0.5051442980766296,\n",
       "   0.07963521778583527,\n",
       "   1.0646257400512695,\n",
       "   -0.08049014955759048,\n",
       "   -0.6290385723114014,\n",
       "   0.42979392409324646,\n",
       "   0.47899511456489563,\n",
       "   0.11691708862781525,\n",
       "   0.04389798641204834,\n",
       "   -0.2538793385028839,\n",
       "   0.21232903003692627,\n",
       "   -1.1743446588516235,\n",
       "   -0.17467395961284637,\n",
       "   -1.2236237525939941,\n",
       "   -0.5252817869186401,\n",
       "   -0.26662784814834595,\n",
       "   0.008941667154431343,\n",
       "   0.4210277795791626,\n",
       "   0.4982356131076813,\n",
       "   -0.08733610808849335,\n",
       "   0.22606462240219116,\n",
       "   -0.47304093837738037,\n",
       "   -0.03792530670762062,\n",
       "   0.2222665548324585,\n",
       "   -0.3419158160686493,\n",
       "   0.6340208649635315,\n",
       "   2.098271131515503,\n",
       "   0.028519004583358765,\n",
       "   1.1393636465072632,\n",
       "   0.21930727362632751,\n",
       "   -0.0049718525260686874,\n",
       "   0.4880185127258301,\n",
       "   0.5020009279251099,\n",
       "   -0.534113347530365,\n",
       "   0.49867409467697144,\n",
       "   -0.32173654437065125,\n",
       "   0.7203019261360168,\n",
       "   -0.7215859889984131,\n",
       "   0.5728513598442078,\n",
       "   -0.1572570651769638,\n",
       "   0.4148956835269928,\n",
       "   0.7037239074707031,\n",
       "   -0.8287935256958008,\n",
       "   0.7197133898735046,\n",
       "   -0.5470276474952698,\n",
       "   -0.6034752130508423,\n",
       "   0.560438334941864,\n",
       "   -1.0745939016342163,\n",
       "   1.1847889423370361,\n",
       "   -0.1005386933684349,\n",
       "   -0.7975969314575195,\n",
       "   0.21009472012519836,\n",
       "   0.42202651500701904,\n",
       "   0.7596237063407898,\n",
       "   -0.6827673316001892,\n",
       "   -0.1840945929288864,\n",
       "   0.28066638112068176,\n",
       "   -0.4203241765499115,\n",
       "   0.2819864749908447,\n",
       "   -0.35841190814971924,\n",
       "   0.11121608316898346,\n",
       "   1.0599948167800903,\n",
       "   0.2628476917743683,\n",
       "   -1.1338151693344116,\n",
       "   0.2573998272418976,\n",
       "   0.7782739400863647,\n",
       "   -0.5635042190551758,\n",
       "   -0.1302998960018158,\n",
       "   -0.1581408977508545,\n",
       "   1.0779780149459839,\n",
       "   0.08294996619224548,\n",
       "   -0.10979308933019638,\n",
       "   -0.7740588188171387,\n",
       "   0.2853124141693115,\n",
       "   -0.822665274143219,\n",
       "   0.23389533162117004,\n",
       "   -0.17730805277824402,\n",
       "   -0.6278400421142578,\n",
       "   0.5158249735832214,\n",
       "   0.6460161805152893,\n",
       "   0.5094990134239197,\n",
       "   -0.6028516888618469,\n",
       "   -0.2548487186431885,\n",
       "   0.9487493634223938,\n",
       "   1.5683315992355347,\n",
       "   0.061843954026699066,\n",
       "   -0.8710124492645264,\n",
       "   -0.0690435841679573,\n",
       "   -0.2558552324771881,\n",
       "   0.43586084246635437,\n",
       "   -0.3820732533931732,\n",
       "   0.13903577625751495,\n",
       "   -0.05646514520049095,\n",
       "   -0.16141124069690704,\n",
       "   -0.0704694464802742,\n",
       "   -0.8051486015319824,\n",
       "   -0.06709542125463486,\n",
       "   -0.606891393661499,\n",
       "   0.4388501048088074,\n",
       "   -0.7309324741363525,\n",
       "   0.369233101606369,\n",
       "   0.12787210941314697,\n",
       "   0.6440837383270264,\n",
       "   0.3531390130519867,\n",
       "   -0.2366170585155487,\n",
       "   -0.11988410353660583,\n",
       "   -1.799263834953308,\n",
       "   -0.14372359216213226,\n",
       "   1.20278000831604,\n",
       "   -0.5309541821479797,\n",
       "   0.32936859130859375,\n",
       "   -0.023165680468082428,\n",
       "   -0.8793085813522339,\n",
       "   -0.28262513875961304,\n",
       "   0.5906850695610046,\n",
       "   0.16178840398788452,\n",
       "   0.2459067702293396,\n",
       "   0.1546247899532318,\n",
       "   0.23513929545879364,\n",
       "   0.3180631995201111,\n",
       "   0.6830692291259766,\n",
       "   -0.4423268437385559,\n",
       "   0.24310708045959473,\n",
       "   0.22383780777454376,\n",
       "   0.6882480978965759,\n",
       "   -0.7403788566589355,\n",
       "   -0.7198994159698486,\n",
       "   0.6861620545387268,\n",
       "   0.5845162272453308,\n",
       "   -0.35918939113616943,\n",
       "   -0.34829291701316833,\n",
       "   0.8548253774642944,\n",
       "   0.6806478500366211,\n",
       "   0.5622520446777344,\n",
       "   -0.3217054307460785,\n",
       "   0.02165713906288147,\n",
       "   0.14444062113761902,\n",
       "   -5.382903575897217,\n",
       "   -0.7796138525009155,\n",
       "   -0.8941155672073364,\n",
       "   -0.005600038915872574,\n",
       "   1.3077343702316284,\n",
       "   -0.6629664301872253,\n",
       "   0.08605962991714478,\n",
       "   0.20400306582450867,\n",
       "   -0.38359522819519043,\n",
       "   0.4012031555175781,\n",
       "   -0.4312448799610138,\n",
       "   0.39637696743011475,\n",
       "   -0.6387621760368347,\n",
       "   -0.24249882996082306,\n",
       "   0.03979301080107689,\n",
       "   -0.8439086079597473,\n",
       "   -0.43808475136756897,\n",
       "   -0.5735238790512085,\n",
       "   0.2764884829521179,\n",
       "   0.055800799280405045,\n",
       "   0.510498583316803,\n",
       "   0.051193125545978546,\n",
       "   -0.6289900541305542,\n",
       "   -0.7949182987213135,\n",
       "   -0.4085780680179596,\n",
       "   -0.5195651650428772,\n",
       "   0.5105635523796082,\n",
       "   -0.6848457455635071,\n",
       "   0.36681491136550903,\n",
       "   0.20423373579978943,\n",
       "   -1.0965386629104614,\n",
       "   1.0480029582977295,\n",
       "   0.45618000626564026,\n",
       "   -0.8430494070053101,\n",
       "   -0.8177800178527832,\n",
       "   0.08612027764320374,\n",
       "   0.5965108871459961,\n",
       "   0.886560320854187,\n",
       "   0.3464907109737396,\n",
       "   0.0013876110315322876,\n",
       "   0.1656966656446457,\n",
       "   -0.5799510478973389,\n",
       "   0.2188311070203781,\n",
       "   0.4020370543003082,\n",
       "   -0.228602334856987,\n",
       "   -0.42904579639434814,\n",
       "   -0.36325302720069885,\n",
       "   -0.7682752013206482,\n",
       "   0.15228940546512604,\n",
       "   0.3736918568611145,\n",
       "   1.6406636238098145,\n",
       "   0.11247271299362183,\n",
       "   0.24283495545387268,\n",
       "   -0.6825174689292908,\n",
       "   -1.272889494895935,\n",
       "   0.2696470320224762,\n",
       "   -0.5900668501853943,\n",
       "   -0.7516140937805176,\n",
       "   -0.9755030274391174,\n",
       "   -0.15209777653217316,\n",
       "   0.38236886262893677,\n",
       "   1.0102925300598145,\n",
       "   -0.5913687944412231,\n",
       "   -0.013176646083593369,\n",
       "   -0.4437865614891052,\n",
       "   0.06705745309591293,\n",
       "   -0.07608424127101898,\n",
       "   -0.13592155277729034,\n",
       "   -0.30840864777565,\n",
       "   0.059305738657712936,\n",
       "   -0.30219870805740356,\n",
       "   -1.9560683965682983,\n",
       "   -0.2783449590206146,\n",
       "   0.28361132740974426,\n",
       "   0.46100151538848877,\n",
       "   -0.20101264119148254,\n",
       "   0.2601589560508728,\n",
       "   -0.7664259076118469,\n",
       "   -0.5993602871894836,\n",
       "   -0.05550770089030266,\n",
       "   0.024367474019527435,\n",
       "   -0.24750083684921265,\n",
       "   0.4292958080768585,\n",
       "   0.02141553908586502,\n",
       "   0.24583399295806885,\n",
       "   -0.8333989977836609,\n",
       "   0.2704014778137207,\n",
       "   0.8741837739944458,\n",
       "   1.2351624965667725,\n",
       "   0.4104235768318176,\n",
       "   0.9193057417869568,\n",
       "   0.2944320738315582,\n",
       "   0.9814876317977905,\n",
       "   0.33622246980667114,\n",
       "   -0.33557289838790894,\n",
       "   -1.0878212451934814,\n",
       "   -0.25340816378593445,\n",
       "   0.5105876326560974,\n",
       "   0.09396032243967056,\n",
       "   0.41363099217414856,\n",
       "   -0.7450944185256958,\n",
       "   -0.1608928143978119,\n",
       "   -0.3382852077484131,\n",
       "   -0.030272698029875755,\n",
       "   0.716738760471344,\n",
       "   -0.5249875783920288,\n",
       "   -0.016395291313529015,\n",
       "   0.33758506178855896,\n",
       "   0.03130849450826645,\n",
       "   0.7587048411369324,\n",
       "   0.11407068371772766,\n",
       "   0.37892040610313416,\n",
       "   -0.0591590516269207,\n",
       "   0.009812712669372559,\n",
       "   -0.43507421016693115,\n",
       "   -0.14671269059181213,\n",
       "   0.7179778814315796,\n",
       "   -0.196400448679924,\n",
       "   0.6415849328041077,\n",
       "   0.3163124918937683,\n",
       "   -0.06818804144859314,\n",
       "   -0.8220939040184021,\n",
       "   0.11204160004854202,\n",
       "   -0.14812272787094116,\n",
       "   0.5145481824874878,\n",
       "   -0.19566605985164642,\n",
       "   -0.1641717255115509,\n",
       "   0.12814487516880035,\n",
       "   -0.2784128487110138,\n",
       "   0.36980319023132324,\n",
       "   -0.4235665798187256,\n",
       "   0.47679463028907776,\n",
       "   0.3625189960002899,\n",
       "   -0.5520097017288208,\n",
       "   -0.04420379176735878,\n",
       "   0.4539164900779724,\n",
       "   -0.07958543300628662,\n",
       "   1.0332525968551636,\n",
       "   0.576861560344696,\n",
       "   -0.14286500215530396,\n",
       "   -0.33160555362701416,\n",
       "   -1.1887608766555786,\n",
       "   -0.275004506111145,\n",
       "   0.5394753217697144,\n",
       "   -0.3295271694660187,\n",
       "   1.3745579719543457,\n",
       "   -0.9100767970085144,\n",
       "   0.6111764907836914,\n",
       "   0.05957869067788124,\n",
       "   0.7276811599731445,\n",
       "   -0.25699755549430847,\n",
       "   -0.09530439972877502,\n",
       "   -0.07372821867465973,\n",
       "   -0.720729649066925,\n",
       "   -2.1627891063690186,\n",
       "   -0.6804242730140686,\n",
       "   0.3307885527610779,\n",
       "   -0.642206072807312,\n",
       "   -0.5625905990600586,\n",
       "   0.15342697501182556,\n",
       "   -0.7942056655883789,\n",
       "   -0.2707172632217407,\n",
       "   0.41012999415397644,\n",
       "   0.4917691946029663,\n",
       "   0.10126239061355591,\n",
       "   -0.1739734411239624,\n",
       "   -6.409374237060547,\n",
       "   -0.0465899221599102,\n",
       "   0.3637163043022156,\n",
       "   0.4133487641811371,\n",
       "   -0.5569247603416443,\n",
       "   0.10336010158061981,\n",
       "   1.4915560483932495,\n",
       "   0.3220341205596924,\n",
       "   -0.3418821096420288,\n",
       "   0.5405741930007935,\n",
       "   0.09602487832307816,\n",
       "   -1.044679045677185,\n",
       "   0.05905488133430481,\n",
       "   -0.2988535165786743,\n",
       "   -0.7720314264297485,\n",
       "   0.2632547914981842,\n",
       "   0.4404214918613434,\n",
       "   1.3648991584777832,\n",
       "   -0.5464110374450684,\n",
       "   0.6322763562202454,\n",
       "   -0.9537040591239929,\n",
       "   -0.06843611598014832,\n",
       "   -0.7914627194404602,\n",
       "   0.9899361729621887,\n",
       "   0.08796036243438721,\n",
       "   -1.2708131074905396,\n",
       "   0.8117462992668152,\n",
       "   -0.5911760330200195,\n",
       "   -0.16479745507240295,\n",
       "   -0.3419404625892639,\n",
       "   -0.8356943130493164,\n",
       "   -0.25207066535949707,\n",
       "   -0.2521522343158722,\n",
       "   0.33939534425735474,\n",
       "   -0.807784914970398,\n",
       "   0.363781213760376,\n",
       "   0.9284149408340454,\n",
       "   0.24213510751724243,\n",
       "   -0.03469134122133255,\n",
       "   -0.7579535841941833,\n",
       "   -0.05295697599649429,\n",
       "   -0.051278989762067795,\n",
       "   -0.12210528552532196,\n",
       "   -0.14752377569675446,\n",
       "   -0.35739970207214355,\n",
       "   0.14026403427124023,\n",
       "   -0.4730013906955719,\n",
       "   -0.142839252948761,\n",
       "   -0.29832202196121216,\n",
       "   -1.0388339757919312,\n",
       "   0.21773694455623627,\n",
       "   -0.05868050456047058,\n",
       "   0.3419227600097656,\n",
       "   0.2445371448993683,\n",
       "   -0.4015963077545166,\n",
       "   1.1249364614486694,\n",
       "   0.2955513596534729,\n",
       "   0.23193152248859406,\n",
       "   0.8642787337303162,\n",
       "   -0.2646958827972412,\n",
       "   -1.0354658365249634,\n",
       "   0.07946772128343582,\n",
       "   -0.5151576995849609,\n",
       "   0.009705320000648499,\n",
       "   0.004450127482414246,\n",
       "   0.7084026336669922,\n",
       "   1.5800021886825562,\n",
       "   -0.12735974788665771,\n",
       "   0.12497904896736145,\n",
       "   -0.21149307489395142,\n",
       "   0.42236924171447754,\n",
       "   0.44377046823501587,\n",
       "   -0.1232391744852066,\n",
       "   -0.1702251136302948,\n",
       "   -0.3763629198074341,\n",
       "   -2.5258710384368896,\n",
       "   0.12171755731105804,\n",
       "   0.20285764336585999,\n",
       "   0.46400734782218933,\n",
       "   -0.32885023951530457,\n",
       "   0.336689293384552,\n",
       "   0.07762812077999115,\n",
       "   -0.10144305974245071,\n",
       "   -0.07687695324420929,\n",
       "   0.6196879148483276,\n",
       "   0.6219303607940674,\n",
       "   0.7304757237434387,\n",
       "   -0.17463237047195435,\n",
       "   0.41307035088539124,\n",
       "   -0.849121630191803,\n",
       "   -1.587298035621643,\n",
       "   -0.2681038975715637,\n",
       "   -0.9491370916366577,\n",
       "   0.4100208878517151,\n",
       "   -0.5752220749855042,\n",
       "   -0.3670580983161926,\n",
       "   -0.7260391712188721,\n",
       "   -0.15508481860160828,\n",
       "   -0.6914819478988647,\n",
       "   0.05365833640098572,\n",
       "   -0.6965550780296326,\n",
       "   -0.44687068462371826,\n",
       "   -0.06434652209281921,\n",
       "   0.523719072341919,\n",
       "   -0.046263910830020905,\n",
       "   0.2958785891532898,\n",
       "   -0.7005380988121033,\n",
       "   -0.7283095717430115,\n",
       "   0.2924773097038269,\n",
       "   1.0486490726470947,\n",
       "   0.7302672863006592,\n",
       "   -1.591712236404419,\n",
       "   0.18623001873493195,\n",
       "   0.1252819448709488,\n",
       "   0.5190648436546326,\n",
       "   0.6232795715332031,\n",
       "   0.7696936726570129,\n",
       "   -0.255489319562912,\n",
       "   0.06644070148468018,\n",
       "   -0.4419969916343689,\n",
       "   1.312817096710205,\n",
       "   0.49452197551727295,\n",
       "   0.5364680886268616,\n",
       "   -0.24784496426582336,\n",
       "   0.029972385615110397,\n",
       "   0.43332839012145996,\n",
       "   -0.009823152795433998,\n",
       "   0.3251960873603821,\n",
       "   -0.6948184370994568,\n",
       "   0.6346619725227356,\n",
       "   -0.7995099425315857,\n",
       "   -0.09101244807243347,\n",
       "   -0.6642324924468994,\n",
       "   0.6526351571083069,\n",
       "   -0.7232746481895447,\n",
       "   -0.5824558138847351,\n",
       "   -0.022724859416484833,\n",
       "   -0.9558691382408142,\n",
       "   -0.30484631657600403,\n",
       "   -1.6428202390670776,\n",
       "   -0.16110721230506897,\n",
       "   -0.5148823857307434,\n",
       "   -0.11817041039466858,\n",
       "   0.45777904987335205,\n",
       "   0.8849976658821106,\n",
       "   -1.0239155292510986,\n",
       "   -0.1493055373430252,\n",
       "   0.13701802492141724,\n",
       "   -0.19286970794200897,\n",
       "   -0.44693315029144287,\n",
       "   0.5568046569824219,\n",
       "   0.22600218653678894,\n",
       "   0.9051110148429871,\n",
       "   0.09258218854665756,\n",
       "   -0.10196719318628311,\n",
       "   1.2752870321273804,\n",
       "   -1.374114990234375,\n",
       "   1.2203528881072998,\n",
       "   -1.004939079284668,\n",
       "   1.2225937843322754,\n",
       "   5.112768650054932,\n",
       "   0.4303187131881714,\n",
       "   0.055199481546878815,\n",
       "   1.0985199213027954,\n",
       "   -0.6836614608764648,\n",
       "   -0.05134095251560211,\n",
       "   -0.47957083582878113,\n",
       "   0.29901859164237976,\n",
       "   1.5985428094863892,\n",
       "   1.5668606758117676,\n",
       "   -0.40182429552078247,\n",
       "   -0.0742759108543396,\n",
       "   -0.5434641242027283,\n",
       "   0.5273312330245972,\n",
       "   -0.12276144325733185,\n",
       "   0.6446511745452881,\n",
       "   0.3823080360889435,\n",
       "   0.24053949117660522,\n",
       "   -0.6423050165176392,\n",
       "   -0.4800145626068115,\n",
       "   1.1436494588851929,\n",
       "   0.10598547756671906,\n",
       "   0.776611864566803,\n",
       "   -0.029250934720039368,\n",
       "   -0.016469448804855347,\n",
       "   0.5901608467102051,\n",
       "   0.2668986916542053,\n",
       "   0.15016469359397888,\n",
       "   -0.7249335646629333,\n",
       "   0.39419087767601013,\n",
       "   0.444866418838501,\n",
       "   0.8646471500396729,\n",
       "   -0.05879289284348488,\n",
       "   0.29690271615982056,\n",
       "   -0.18510495126247406,\n",
       "   0.3629436790943146,\n",
       "   0.1307174414396286,\n",
       "   -0.38489478826522827,\n",
       "   -0.13767047226428986,\n",
       "   0.4585375189781189,\n",
       "   0.09381779283285141,\n",
       "   1.0631771087646484,\n",
       "   0.06681814789772034,\n",
       "   -0.589547872543335,\n",
       "   -0.1401919722557068,\n",
       "   -1.2560516595840454,\n",
       "   -0.4526905119419098,\n",
       "   -1.610230565071106,\n",
       "   -0.1578630954027176,\n",
       "   -0.03284400328993797,\n",
       "   -0.16954612731933594,\n",
       "   0.0979795902967453,\n",
       "   0.952345073223114,\n",
       "   0.11758267879486084,\n",
       "   0.4550604522228241,\n",
       "   0.19058780372142792,\n",
       "   -0.4279687702655792,\n",
       "   -0.460875540971756,\n",
       "   -1.606892466545105,\n",
       "   -0.011250399053096771,\n",
       "   0.10770085453987122,\n",
       "   0.08881396055221558,\n",
       "   0.880770206451416,\n",
       "   0.6028737425804138,\n",
       "   0.47142690420150757,\n",
       "   0.11569830775260925,\n",
       "   -1.0927019119262695,\n",
       "   -0.5552639961242676,\n",
       "   -0.18707235157489777,\n",
       "   -0.18943840265274048,\n",
       "   1.259255051612854,\n",
       "   -0.08302220702171326,\n",
       "   -0.42439913749694824,\n",
       "   0.004647590219974518,\n",
       "   0.12677431106567383,\n",
       "   0.2844533920288086,\n",
       "   0.17360252141952515,\n",
       "   0.9757001399993896,\n",
       "   0.014935921877622604,\n",
       "   -0.36958110332489014,\n",
       "   -0.10539422929286957,\n",
       "   0.4223754405975342,\n",
       "   -0.22875690460205078,\n",
       "   1.0328474044799805,\n",
       "   0.2999250292778015,\n",
       "   0.495915025472641,\n",
       "   0.419792115688324,\n",
       "   0.8442886471748352,\n",
       "   -1.7034052610397339,\n",
       "   0.5264274477958679,\n",
       "   1.2397980690002441,\n",
       "   0.08535733819007874,\n",
       "   -0.938204824924469,\n",
       "   -1.5194206237792969,\n",
       "   0.4530244767665863,\n",
       "   0.14635029435157776,\n",
       "   0.1871829330921173,\n",
       "   -0.20729348063468933,\n",
       "   0.3681424856185913,\n",
       "   0.7737179398536682,\n",
       "   0.043796904385089874,\n",
       "   0.4584108591079712,\n",
       "   0.4226538836956024,\n",
       "   -0.7509166598320007,\n",
       "   0.566909670829773,\n",
       "   0.23469796776771545,\n",
       "   -0.0466155931353569,\n",
       "   -0.33819371461868286,\n",
       "   0.1843033730983734,\n",
       "   -0.03263075277209282,\n",
       "   -0.7767750024795532,\n",
       "   0.8298141360282898,\n",
       "   0.7776126861572266,\n",
       "   0.5364073514938354,\n",
       "   0.48094019293785095,\n",
       "   -0.07273322343826294,\n",
       "   0.2522662281990051,\n",
       "   0.516394317150116,\n",
       "   0.7864488363265991,\n",
       "   -0.3075467646121979,\n",
       "   -0.3332897424697876,\n",
       "   0.6611075401306152,\n",
       "   -0.758349597454071,\n",
       "   -0.42717641592025757,\n",
       "   0.43606436252593994,\n",
       "   -0.5050908327102661,\n",
       "   -0.0008140788413584232,\n",
       "   0.2606757879257202,\n",
       "   -1.3681902885437012,\n",
       "   0.16441215574741364,\n",
       "   -0.0479171946644783,\n",
       "   0.046414535492658615,\n",
       "   -1.6265342235565186,\n",
       "   -0.6035731434822083,\n",
       "   0.1639946848154068,\n",
       "   -0.3901687264442444,\n",
       "   0.9699592590332031,\n",
       "   0.5005286335945129,\n",
       "   0.03405506908893585,\n",
       "   -0.8668604493141174,\n",
       "   0.24439825117588043,\n",
       "   0.18158777058124542,\n",
       "   0.49089697003364563,\n",
       "   5.3771796226501465,\n",
       "   0.25533998012542725,\n",
       "   -0.5807623863220215,\n",
       "   -0.8583124279975891,\n",
       "   -0.407050222158432,\n",
       "   0.44609177112579346,\n",
       "   -0.30213916301727295,\n",
       "   0.7278067469596863,\n",
       "   0.4642859995365143,\n",
       "   -0.6722910404205322,\n",
       "   -0.48416537046432495,\n",
       "   -0.5266540050506592,\n",
       "   1.877713680267334,\n",
       "   -0.15462012588977814,\n",
       "   -1.058833122253418,\n",
       "   0.9768964052200317,\n",
       "   -0.9349130988121033,\n",
       "   0.2663831114768982,\n",
       "   0.607673168182373,\n",
       "   -0.6981247663497925,\n",
       "   -1.0044283866882324,\n",
       "   -0.28439250588417053,\n",
       "   0.36444219946861267,\n",
       "   -0.10854566842317581,\n",
       "   -0.10744287073612213,\n",
       "   -0.16358986496925354,\n",
       "   -1.2864919900894165,\n",
       "   0.21947762370109558,\n",
       "   -1.0823615789413452,\n",
       "   -0.34271109104156494,\n",
       "   -0.6361173391342163,\n",
       "   0.3854292631149292,\n",
       "   1.1239668130874634,\n",
       "   -0.3336483836174011,\n",
       "   0.20754103362560272,\n",
       "   -1.048984408378601,\n",
       "   -0.00686383992433548,\n",
       "   0.58448326587677,\n",
       "   0.33713021874427795,\n",
       "   0.788657546043396,\n",
       "   -0.16389814019203186,\n",
       "   -0.4167416989803314,\n",
       "   -0.569507360458374,\n",
       "   0.22339162230491638,\n",
       "   -1.2867093086242676,\n",
       "   -0.2652667164802551,\n",
       "   -0.2036411613225937,\n",
       "   0.365551233291626,\n",
       "   0.3810228705406189,\n",
       "   0.8225757479667664,\n",
       "   -1.0550488233566284,\n",
       "   -0.3023073673248291,\n",
       "   -0.8102003335952759,\n",
       "   -0.3459678590297699,\n",
       "   0.4030509293079376,\n",
       "   -0.5622062683105469,\n",
       "   -0.00959731824696064,\n",
       "   -0.2411854863166809,\n",
       "   -0.4373782277107239,\n",
       "   0.13732905685901642,\n",
       "   -0.08907999098300934,\n",
       "   -0.2204999327659607,\n",
       "   0.8654419183731079,\n",
       "   0.27824515104293823,\n",
       "   -0.05349445343017578,\n",
       "   0.7777315378189087,\n",
       "   0.7855008244514465,\n",
       "   -0.47951826453208923,\n",
       "   0.1498071849346161,\n",
       "   -0.19033139944076538,\n",
       "   0.42830798029899597,\n",
       "   -1.2399771213531494,\n",
       "   0.32876279950141907,\n",
       "   -0.30284592509269714,\n",
       "   0.09259089827537537,\n",
       "   -0.3208233714103699,\n",
       "   0.24108876287937164,\n",
       "   -0.5625876188278198,\n",
       "   -0.06965991854667664,\n",
       "   -0.337421715259552,\n",
       "   1.1417269706726074,\n",
       "   0.24445368349552155,\n",
       "   -0.7591788172721863,\n",
       "   -0.016099710017442703,\n",
       "   0.5024922490119934,\n",
       "   -0.40348029136657715,\n",
       "   0.007404977455735207,\n",
       "   -0.8329911231994629,\n",
       "   -0.907986044883728,\n",
       "   -0.1495819091796875,\n",
       "   -0.8839245438575745,\n",
       "   0.08374504745006561,\n",
       "   0.6902536749839783,\n",
       "   0.26954859495162964,\n",
       "   1.088568925857544,\n",
       "   -0.47279059886932373,\n",
       "   -0.17973703145980835,\n",
       "   -0.47555652260780334,\n",
       "   -0.3599909245967865,\n",
       "   0.7644367218017578,\n",
       "   -0.07562963664531708,\n",
       "   0.2373383343219757,\n",
       "   0.1548161506652832,\n",
       "   -0.8402556777000427,\n",
       "   0.16034619510173798,\n",
       "   -0.5869197249412537,\n",
       "   0.4438444972038269,\n",
       "   0.337654173374176,\n",
       "   -0.775662899017334,\n",
       "   -0.3447464406490326,\n",
       "   -1.3999539613723755,\n",
       "   -0.3533532917499542,\n",
       "   0.1645326167345047,\n",
       "   -0.6445149779319763,\n",
       "   -0.3925289809703827,\n",
       "   0.4821475148200989,\n",
       "   -0.6707291603088379,\n",
       "   -0.3503226637840271,\n",
       "   0.584531307220459,\n",
       "   0.49622565507888794,\n",
       "   -0.05925367400050163,\n",
       "   -0.3247276842594147,\n",
       "   0.38811177015304565,\n",
       "   0.8882660269737244,\n",
       "   -0.019735662266612053,\n",
       "   0.010698140598833561,\n",
       "   0.1989993453025818,\n",
       "   -0.0585014782845974,\n",
       "   0.62278151512146,\n",
       "   -0.31818100810050964,\n",
       "   0.13129015266895294,\n",
       "   -0.1555279642343521,\n",
       "   0.02386537939310074,\n",
       "   -0.5985135436058044,\n",
       "   0.3148360550403595,\n",
       "   -0.09440964460372925,\n",
       "   0.460196852684021,\n",
       "   0.6278019547462463,\n",
       "   -0.5441294312477112,\n",
       "   0.3171499967575073,\n",
       "   -0.9902046322822571,\n",
       "   -0.4453067183494568,\n",
       "   -0.775679349899292,\n",
       "   1.148485541343689,\n",
       "   0.5179712772369385,\n",
       "   0.3967010974884033,\n",
       "   -0.23297946155071259,\n",
       "   -1.2480071783065796,\n",
       "   -0.3433878421783447,\n",
       "   0.9208337664604187,\n",
       "   0.4773139953613281,\n",
       "   -0.5381410121917725,\n",
       "   0.6531146764755249,\n",
       "   -0.30533501505851746,\n",
       "   -0.27540236711502075,\n",
       "   0.32471922039985657,\n",
       "   0.429799884557724,\n",
       "   0.36469027400016785,\n",
       "   -0.16158261895179749,\n",
       "   0.6605653762817383,\n",
       "   -0.16736279428005219]]]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huggingface/transformers repository lists the other pipeline functions, such as ner extraction, sequence classification, and masking. You are encouraged to explore them. \n",
    "https://github.com/huggingface/transformers#quick-tour-of-pipelines\n",
    "\n",
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "they          2,027\n",
      "are           2,024\n",
      "using         2,478\n",
      "artificial    7,976\n",
      "intelligence  4,454\n",
      "algorithms   13,792\n",
      "to            2,000\n",
      "find          2,424\n",
      "the           1,996\n",
      "best          2,190\n",
      "ways          3,971\n",
      "to            2,000\n",
      "eat           4,521\n",
      "burger       15,890\n",
      "##s           2,015\n",
      "with          2,007\n",
      "artificial    7,976\n",
      "sweet         4,086\n",
      "##ener       24,454\n",
      "##s           2,015\n",
      "and           1,998\n",
      "drink         4,392\n",
      "tea           5,572\n",
      "with          2,007\n",
      "artificial    7,976\n",
      "sweet         4,086\n",
      "##ener       24,454\n",
      "##s           2,015\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"They are using artificial intelligence algorithms to find the best ways to eat burgers with artificial sweeteners and drink tea with artificial sweeteners.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_embedding(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = output[0]\n",
    "sentence_embedding = output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]\n",
    "\n",
    "sentence_embedding_1 = np.mean(token_vecs, axis=0)\n",
    "\n",
    "len(sentence_embedding_0), len(sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 they\n",
      "2 are\n",
      "3 using\n",
      "4 artificial\n",
      "5 intelligence\n",
      "6 algorithms\n",
      "7 to\n",
      "8 find\n",
      "9 the\n",
      "10 best\n",
      "11 ways\n",
      "12 to\n",
      "13 eat\n",
      "14 burger\n",
      "15 ##s\n",
      "16 with\n",
      "17 artificial\n",
      "18 sweet\n",
      "19 ##ener\n",
      "20 ##s\n",
      "21 and\n",
      "22 drink\n",
      "23 tea\n",
      "24 with\n",
      "25 artificial\n",
      "26 sweet\n",
      "27 ##ener\n",
      "28 ##s\n",
      "29 .\n",
      "30 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"artificial\".\n",
      "\n",
      "artificial intelligence    [ 0.26776117  0.7260678  -0.00269581  0.41311178  0.56672966]\n",
      "artificial sweetener    [-0.00727491  0.38049778  0.24973139  0.3719149   0.48560435]\n",
      "artificial sweetener    [0.05890159 0.35162345 0.21265224 0.49069265 0.12243742]\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"artificial\".')\n",
    "print('')\n",
    "print(\"artificial intelligence   \", str(token_vecs[4][:5]))\n",
    "print(\"artificial sweetener   \", str(token_vecs[17][:5]))\n",
    "print(\"artificial sweetener   \", str(token_vecs[25][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.55\n",
      "Vector similarity for *different* meanings:  0.44\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cosine similarity between the word artificial \n",
    "# in \"artificial sweetener\" vs \"artificial intelligence\" (different meanings).\n",
    "diff_artificial = 1 - cosine(token_vecs[10], token_vecs[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"artificial sweetener\" vs \"artificial sweetener\" (same meaning).\n",
    "same_artificial = 1 - cosine(token_vecs[10], token_vecs[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_artificial)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.013912596739828587"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9798370003700256}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9959348440170288}]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997702836990356}]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.999405562877655}]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"algorithmic bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9590974450111389}]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9837853312492371}]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can detect a small difference between 'artificial' in 'artificial sweetener' and 'artificial intelligence.'\n",
    "#It is interesting to see the positive and negative scores.\n",
    "#I know in surveys, tech companies like Google and Nvidia are well-liked, so their positivity makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation using BERT\n",
    "\n",
    "The last method which we will explore is text generation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
    "\n",
    "We will be using the built in generate function, but the example file has more detailed code which allows you to set the seed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749f957d4d1a471d93456e1bf7994db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=665.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c75c29eddf46bea1473bc7bf2efca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1042301.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4f424cd67f4289a2c0012c37a9e3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd16ce78d894902a2cbda2b0085cb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1355256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5c76ec77f54fdc9dfd31f33f78a4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=548118077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing that we like to do more than analyse data all day long and then try to figure out what's going on.\n",
      "\n",
      "\"We're not going to be able to do that. We're not going to be able to do that. We\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. A little creepy, and as we can see, far from perfect: GPT doesn't alwats work out flawlessly, but it sometimes can, and we will try and see if fine-tuning helps. We are going to tune the model on a complete dataset of Trump tweets, as they have a set of distinctive, highly identifiable qualities.\n",
    "\n",
    "### Creating a domain-specific language model\n",
    "\n",
    "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. We will be training models to perform two tasks - one is to create a BERT with an \"accent\", by traning a model with english news data from the UK, from the US, and from India. We will also train a language generation model with a bunch of Trump tweets. \n",
    "\n",
    "We can train models specifically over a certain domain to make its language generation similar to that domain. \n",
    "[run_language modelling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), followed by [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py). I've downloaded these files and added them to this directory so we can run them through the notebook. You are encouraged to look at these files to get a rough idea of what is going on.\n",
    "\n",
    "### Loading Data \n",
    "\n",
    "We want to now get our Trump tweets and our English news datasets ready. The data the scripts expect is just a text file with relevant data. We load the Trump tweets and then write them to disk as train and test files with only data. I leave the original dataframes in case you would like to use it for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../data/trump_tweets\"):\n",
    "    dfs.append(pd.read_json(\"../data/trump_tweets/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550441250965708800</td>\n",
       "      <td>\"@ronmeier123: @Macys Your APPAREL is UNPARALL...</td>\n",
       "      <td>2014-12-31 23:59:55+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550441111513493504</td>\n",
       "      <td>\"@gillule4: @realDonaldTrump incredible experi...</td>\n",
       "      <td>2014-12-31 23:59:22+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550440752254562304</td>\n",
       "      <td>\"@JobSnarechs: Negotiation tip #1: The worst t...</td>\n",
       "      <td>2014-12-31 23:57:56+00:00</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550440620792492032</td>\n",
       "      <td>\"@joelmch2os: @realDonaldTrump announce your p...</td>\n",
       "      <td>2014-12-31 23:57:25+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550440523094577152</td>\n",
       "      <td>\"@djspookyshadow: Feeling a deep gratitude for...</td>\n",
       "      <td>2014-12-31 23:57:02+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                source              id_str  \\\n",
       "0  Twitter for Android  550441250965708800   \n",
       "1  Twitter for Android  550441111513493504   \n",
       "2  Twitter for Android  550440752254562304   \n",
       "3  Twitter for Android  550440620792492032   \n",
       "4  Twitter for Android  550440523094577152   \n",
       "\n",
       "                                                text  \\\n",
       "0  \"@ronmeier123: @Macys Your APPAREL is UNPARALL...   \n",
       "1  \"@gillule4: @realDonaldTrump incredible experi...   \n",
       "2  \"@JobSnarechs: Negotiation tip #1: The worst t...   \n",
       "3  \"@joelmch2os: @realDonaldTrump announce your p...   \n",
       "4  \"@djspookyshadow: Feeling a deep gratitude for...   \n",
       "\n",
       "                 created_at  retweet_count  in_reply_to_user_id_str  \\\n",
       "0 2014-12-31 23:59:55+00:00              8                      NaN   \n",
       "1 2014-12-31 23:59:22+00:00              5                      NaN   \n",
       "2 2014-12-31 23:57:56+00:00             33                      NaN   \n",
       "3 2014-12-31 23:57:25+00:00              8                      NaN   \n",
       "4 2014-12-31 23:57:02+00:00              9                      NaN   \n",
       "\n",
       "   favorite_count  is_retweet  \n",
       "0              21       False  \n",
       "1              18       False  \n",
       "2              44       False  \n",
       "3              26       False  \n",
       "4              31       False  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(df['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519    Moments ago, it was my great honor to sign #Ri...\n",
       "565     I hope the Fake News Media keeps talking about...\n",
       "927     RT @TeamTrump: Hillary's policies have made Am...\n",
       "1738    Paul Ryan is far from my first choice, but a v...\n",
       "5846    \"@justinraay: @realDonaldTrump @RealPro4Real @...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text.to_frame().to_csv(r'test_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now used the Google Colab GPUs to train the Trump tweet models. We'll be doing the same for our blog posts too.\n",
    "\n",
    "### GloWBe dataset\n",
    "\n",
    "We'll now load up the GloWbe (Corpus of Global Web-Based English) dataset which have different texts from different countries. We'll try and draw out texts from only the US, UK and India. We'll then save these to disk. Note that this is a Davies Corpora dataset: the full download can be done with the Dropbox link I sent in an announcement a few weeks ago. The whole download is about 3.5 GB but we only need two files, which are anout 250 MB each. The other files might be useful for your research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"C://Downloads/GloWbE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the exact name of the files\n",
    "us = \"/text_us_blog_jfy.zip\"\n",
    "gb = \"/text_gb_blog_akq.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"us_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_gb_blog_akq.zip\n"
     ]
    }
   ],
   "source": [
    "gb_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"gb_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary with document ids mapping to text. Since we don't need any information but the text, we can just save these to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-be66db63272e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mus_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "' '.join(list(us_texts.values())[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_texts(texts, file_name):\n",
    "    text = []\n",
    "    for doc in list(texts.values()):\n",
    "        text.append(' '.join(doc).replace(\"< h >\", \"\").replace(\"< p >\", \"\"))\n",
    "    train_text, test_text = train_test_split(text, test_size=0.2)\n",
    "    with open(file_name + \"_train\", 'w') as f:\n",
    "        for item in train_text:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(file_name + \"_test\", 'w') as f:\n",
    "        for item in test_text:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_texts(us_texts, \"us_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_texts(gb_texts, \"gb_blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the training and testing files for both US and GB blogs in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING - SHIFT TO GOOGLE COLAB OR GPU ENABLED MACHINE\n",
    "\n",
    "The [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) walks you through the process of fine-tuning models, as we did before for the classification task. Move now to the colab file to fine tune your models. Once you downloaded all the models and their information, place those files in the directory of the HW to use them as demonstrated below. \n",
    "\n",
    "\n",
    "\n",
    "### Running Scripts\n",
    "\n",
    "We use the scripts to do language modelling and text generation. The following cells run the code as if you would have run it in a terminal. I trained all of these models using the Googlr Colab file, and then saved the models to disk.\n",
    "\n",
    "#### Trump GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=train_text_trump --do_eval --eval_data_file=test_text_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_UK --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "### Loading and using models\n",
    "\n",
    "Let us now load the four models we have and see how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now - let us see what our Trump Tweet Bot looks like!\n",
    "You can generate text via command line using the command below. You can also load a model once it is saved - I trained my model using Google Colab, downloaded the model, and am loading it again via the command below. Note that you have to download all the files in your folder of the fine-tuned model to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_generation.py --model_type=gpt2 --model_name_or_path=output_trump_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_trump_gpt\")\n",
    "model_trump = AutoModelWithLMHead.from_pretrained(\"output_trump_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a disaster for the United States. He is a total loser. He is a total loser!\"\n",
      "\"\"\"@jeff_mcclaren: @realDonaldTrump @realDonaldTrump @foxandfriends @megynkelly @\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - our Trump bot is nasty, so we know our model trained well. What happens if we try the same sentence for our non-fine tuned model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a very good president,\" said Sen. John McCain (R-Ariz.). \"He's going to be a very good president. He's going to be a very good president. He's going to be a very\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite the contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C://NOW/aiTextsWithTokens.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_pickle('C://NOW/aiTextsWithTokensSample.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>publisher</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>reduced_tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1336549</td>\n",
       "      <td>984.0</td>\n",
       "      <td>10-01-04</td>\n",
       "      <td>US</td>\n",
       "      <td>Project Syndicate</td>\n",
       "      <td>http://www.project-syndicate.org/commentary/gr...</td>\n",
       "      <td>Grandmasters and Global Growth</td>\n",
       "      <td>&lt;p&gt; Kenneth Rogoff , Professor of Economics an...</td>\n",
       "      <td>[&lt;, p, &gt;, Kenneth, Rogoff, Professor, of, Econ...</td>\n",
       "      <td>[&lt;, p, &gt;, kenneth, rogoff, professor, economic...</td>\n",
       "      <td>[professor, public, policy, university, bank, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[&lt;, p], [&gt;], [Kenneth, Rogoff, Professor, of,...</td>\n",
       "      <td>[[&lt;, p], [&gt;], [kenneth, rogoff, professor, eco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1336930</td>\n",
       "      <td>958.0</td>\n",
       "      <td>10-01-04</td>\n",
       "      <td>GB</td>\n",
       "      <td>HiddenWires</td>\n",
       "      <td>http://hiddenwires.co.uk/resourcesarticles2010...</td>\n",
       "      <td>The Hydra Project - Middleware for Easy Networ...</td>\n",
       "      <td>&lt;p&gt; Networked sensors and devices have huge po...</td>\n",
       "      <td>[&lt;, p, &gt;, Networked, sensors, and, devices, ha...</td>\n",
       "      <td>[&lt;, p, &gt;, network, sensor, device, huge, poten...</td>\n",
       "      <td>[network, device, huge, potential, ensure, dif...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[&lt;, p, &gt;], [Networked, sensors, and, devices,...</td>\n",
       "      <td>[[&lt;, p, &gt;], [networked, sensors, devices, huge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1338856</td>\n",
       "      <td>4781.0</td>\n",
       "      <td>10-01-06</td>\n",
       "      <td>US</td>\n",
       "      <td>Westword</td>\n",
       "      <td>http://www.westword.com/news/lego-universe-col...</td>\n",
       "      <td>LEGO Universe - colorful plastic's answer to W...</td>\n",
       "      <td>&lt;h&gt; LEGO Universe ? colorful plastic 's answer...</td>\n",
       "      <td>[&lt;, h, &gt;, LEGO, Universe, colorful, plastic', ...</td>\n",
       "      <td>[&lt;, h, &gt;, lego, universe, colorful, plastic, a...</td>\n",
       "      <td>[answer, base, black, way, form, little, build...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[[&lt;, h], [&gt;, LEGO, Universe], [colorful, plast...</td>\n",
       "      <td>[[&lt;, h], [&gt;, lego, universe], [colorful, plast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1340445</td>\n",
       "      <td>1569.0</td>\n",
       "      <td>10-01-07</td>\n",
       "      <td>US</td>\n",
       "      <td>Popular Science</td>\n",
       "      <td>http://www.popsci.com/gadgets/article/2010-01/...</td>\n",
       "      <td>Exclusive: Inside Project Natal's Brain</td>\n",
       "      <td>&lt;h&gt; Exclusive : Inside Project Natal 's Brain ...</td>\n",
       "      <td>[&lt;, h, &gt;, Exclusive, Inside, Project, Natal', ...</td>\n",
       "      <td>[&lt;, h, &gt;, exclusive, inside, project, natal, b...</td>\n",
       "      <td>[project, brain, project, let, control, game, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[[&lt;, h], [&gt;], [Exclusive], [Inside, Project, N...</td>\n",
       "      <td>[[&lt;, h], [&gt;], [exclusive], [inside, project, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1342810</td>\n",
       "      <td>280.0</td>\n",
       "      <td>10-01-09</td>\n",
       "      <td>US</td>\n",
       "      <td>PC Magazine</td>\n",
       "      <td>http://www.pcmag.com/article2/0,2817,2357928,0...</td>\n",
       "      <td>Roxxxy the 'Sex Robot' Debuts at AVN Porn Show</td>\n",
       "      <td>&lt;h&gt; Roxxxy the ' Sex Robot ' Debuts at AVN Por...</td>\n",
       "      <td>[&lt;, h, &gt;, Roxxxy, the, Sex, Robot, Debuts, at,...</td>\n",
       "      <td>[&lt;, h, &gt;, roxxxy, sex, robot, debut, avn, porn...</td>\n",
       "      <td>[robot, robot, robot, meet, engine, programme,...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>[[&lt;, h], [&gt;], [Roxxxy], [the, Sex, Robot, Debu...</td>\n",
       "      <td>[[&lt;, h], [&gt;], [roxxxy], [sex, robot, debuts, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72946</th>\n",
       "      <td>62224699</td>\n",
       "      <td>636.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>ZA</td>\n",
       "      <td>capetownetc.com</td>\n",
       "      <td>https://www.capetownetc.com/news/4ir-commissio...</td>\n",
       "      <td>4IR commission recommends radical changes to e...</td>\n",
       "      <td>&lt;p&gt; President Cyril Ramaphosa 's 4IR commissio...</td>\n",
       "      <td>[&lt;, p, &gt;, President, Cyril, Ramaphosa', \"'s\", ...</td>\n",
       "      <td>[&lt;, p, &gt;, president, cyril, ramaphosa, 4ir, co...</td>\n",
       "      <td>[president, commission, release, statement, we...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, p, &gt;, President, Cyril, Ramaphosa, 's, 4I...</td>\n",
       "      <td>[[&lt;, p, &gt;, president, cyril, ramaphosa, 4ir, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72947</th>\n",
       "      <td>62224797</td>\n",
       "      <td>457.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2oceansvibe.com</td>\n",
       "      <td>https://www.2oceansvibe.com/2020/10/30/how-sas...</td>\n",
       "      <td>How SA's New `Buy Now, Pay Later' Business Mod...</td>\n",
       "      <td>&lt;p&gt; While it might seem like a good idea to pa...</td>\n",
       "      <td>[&lt;, p, &gt;, While, it, might, seem, like, a, goo...</td>\n",
       "      <td>[&lt;, p, &gt;, like, good, idea, pay, credit, cours...</td>\n",
       "      <td>[good, idea, pay, credit, course, month, year,...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, p], [&gt;, While, it, might, seem, like, a, ...</td>\n",
       "      <td>[[&lt;, p], [&gt;, like, good, idea, pay, credit, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72948</th>\n",
       "      <td>62224321</td>\n",
       "      <td>649.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>ZA</td>\n",
       "      <td>themediaonline.co.za</td>\n",
       "      <td>https://themediaonline.co.za/2020/10/nothing-s...</td>\n",
       "      <td>Wavemaker's new bespoke OS delivers 'provocati...</td>\n",
       "      <td>&lt;h&gt; Wavemaker 's new bespoke OS delivers ' pro...</td>\n",
       "      <td>[&lt;, h, &gt;, Wavemaker', \"'s\", 'new, bespoke, OS,...</td>\n",
       "      <td>[&lt;, h, &gt;, wavemaker, new, bespeak, os, deliver...</td>\n",
       "      <td>[deliver, plan, launch, news, different, point...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, h], [&gt;], [Wavemaker, 's, new, bespoke, OS...</td>\n",
       "      <td>[[&lt;, h], [&gt;], [wavemaker, new, bespoke, os, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72949</th>\n",
       "      <td>52056036</td>\n",
       "      <td>547.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>NG</td>\n",
       "      <td>independent.ng</td>\n",
       "      <td>https://www.independent.ng/the-increasing-infl...</td>\n",
       "      <td>The Increasing Influence of Cryptocurrency in ...</td>\n",
       "      <td>&lt;h&gt; The Increasing Influence of Cryptocurrency...</td>\n",
       "      <td>[&lt;, h, &gt;, The, Increasing, Influence, of, Cryp...</td>\n",
       "      <td>[&lt;, h, &gt;, increase, influence, cryptocurrency,...</td>\n",
       "      <td>[increase, economic, economic, make, shift, me...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, h], [&gt;], [The, Increasing, Influence, of,...</td>\n",
       "      <td>[[&lt;, h], [&gt;], [increasing, influence, cryptocu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72950</th>\n",
       "      <td>52056515</td>\n",
       "      <td>2028.0</td>\n",
       "      <td>20-10-31</td>\n",
       "      <td>NG</td>\n",
       "      <td>proshareng.com</td>\n",
       "      <td>https://www.proshareng.com/news/DATA%20&amp;amp;%2...</td>\n",
       "      <td>AI in Foreign Exchange Trading (Forex) - Curre...</td>\n",
       "      <td>&lt;h&gt; AI in Foreign Exchange Trading ( Forex ) -...</td>\n",
       "      <td>[&lt;, h, &gt;, AI, in, Foreign, Exchange, Trading, ...</td>\n",
       "      <td>[&lt;, h, &gt;, have, foreign, exchange, trade, fore...</td>\n",
       "      <td>[foreign, exchange, trade, current, state, sec...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>[[&lt;, h], [&gt;, AI, in, Foreign, Exchange, Tradin...</td>\n",
       "      <td>[[&lt;, h], [&gt;, ai, foreign, exchange, trading], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72951 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  length      date country             publisher  \\\n",
       "0       1336549   984.0  10-01-04      US     Project Syndicate   \n",
       "1       1336930   958.0  10-01-04      GB           HiddenWires   \n",
       "2       1338856  4781.0  10-01-06      US              Westword   \n",
       "3       1340445  1569.0  10-01-07      US       Popular Science   \n",
       "4       1342810   280.0  10-01-09      US           PC Magazine   \n",
       "...         ...     ...       ...     ...                   ...   \n",
       "72946  62224699   636.0  20-10-31      ZA       capetownetc.com   \n",
       "72947  62224797   457.0  20-10-31      ZA       2oceansvibe.com   \n",
       "72948  62224321   649.0  20-10-31      ZA  themediaonline.co.za   \n",
       "72949  52056036   547.0  20-10-31      NG        independent.ng   \n",
       "72950  52056515  2028.0  20-10-31      NG        proshareng.com   \n",
       "\n",
       "                                                     url  \\\n",
       "0      http://www.project-syndicate.org/commentary/gr...   \n",
       "1      http://hiddenwires.co.uk/resourcesarticles2010...   \n",
       "2      http://www.westword.com/news/lego-universe-col...   \n",
       "3      http://www.popsci.com/gadgets/article/2010-01/...   \n",
       "4      http://www.pcmag.com/article2/0,2817,2357928,0...   \n",
       "...                                                  ...   \n",
       "72946  https://www.capetownetc.com/news/4ir-commissio...   \n",
       "72947  https://www.2oceansvibe.com/2020/10/30/how-sas...   \n",
       "72948  https://themediaonline.co.za/2020/10/nothing-s...   \n",
       "72949  https://www.independent.ng/the-increasing-infl...   \n",
       "72950  https://www.proshareng.com/news/DATA%20&amp;%2...   \n",
       "\n",
       "                                                   title  \\\n",
       "0                         Grandmasters and Global Growth   \n",
       "1      The Hydra Project - Middleware for Easy Networ...   \n",
       "2      LEGO Universe - colorful plastic's answer to W...   \n",
       "3                Exclusive: Inside Project Natal's Brain   \n",
       "4         Roxxxy the 'Sex Robot' Debuts at AVN Porn Show   \n",
       "...                                                  ...   \n",
       "72946  4IR commission recommends radical changes to e...   \n",
       "72947  How SA's New `Buy Now, Pay Later' Business Mod...   \n",
       "72948  Wavemaker's new bespoke OS delivers 'provocati...   \n",
       "72949  The Increasing Influence of Cryptocurrency in ...   \n",
       "72950  AI in Foreign Exchange Trading (Forex) - Curre...   \n",
       "\n",
       "                                                    text  \\\n",
       "0      <p> Kenneth Rogoff , Professor of Economics an...   \n",
       "1      <p> Networked sensors and devices have huge po...   \n",
       "2      <h> LEGO Universe ? colorful plastic 's answer...   \n",
       "3      <h> Exclusive : Inside Project Natal 's Brain ...   \n",
       "4      <h> Roxxxy the ' Sex Robot ' Debuts at AVN Por...   \n",
       "...                                                  ...   \n",
       "72946  <p> President Cyril Ramaphosa 's 4IR commissio...   \n",
       "72947  <p> While it might seem like a good idea to pa...   \n",
       "72948  <h> Wavemaker 's new bespoke OS delivers ' pro...   \n",
       "72949  <h> The Increasing Influence of Cryptocurrency...   \n",
       "72950  <h> AI in Foreign Exchange Trading ( Forex ) -...   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "0      [<, p, >, Kenneth, Rogoff, Professor, of, Econ...   \n",
       "1      [<, p, >, Networked, sensors, and, devices, ha...   \n",
       "2      [<, h, >, LEGO, Universe, colorful, plastic', ...   \n",
       "3      [<, h, >, Exclusive, Inside, Project, Natal', ...   \n",
       "4      [<, h, >, Roxxxy, the, Sex, Robot, Debuts, at,...   \n",
       "...                                                  ...   \n",
       "72946  [<, p, >, President, Cyril, Ramaphosa', \"'s\", ...   \n",
       "72947  [<, p, >, While, it, might, seem, like, a, goo...   \n",
       "72948  [<, h, >, Wavemaker', \"'s\", 'new, bespoke, OS,...   \n",
       "72949  [<, h, >, The, Increasing, Influence, of, Cryp...   \n",
       "72950  [<, h, >, AI, in, Foreign, Exchange, Trading, ...   \n",
       "\n",
       "                                       normalized_tokens  \\\n",
       "0      [<, p, >, kenneth, rogoff, professor, economic...   \n",
       "1      [<, p, >, network, sensor, device, huge, poten...   \n",
       "2      [<, h, >, lego, universe, colorful, plastic, a...   \n",
       "3      [<, h, >, exclusive, inside, project, natal, b...   \n",
       "4      [<, h, >, roxxxy, sex, robot, debut, avn, porn...   \n",
       "...                                                  ...   \n",
       "72946  [<, p, >, president, cyril, ramaphosa, 4ir, co...   \n",
       "72947  [<, p, >, like, good, idea, pay, credit, cours...   \n",
       "72948  [<, h, >, wavemaker, new, bespeak, os, deliver...   \n",
       "72949  [<, h, >, increase, influence, cryptocurrency,...   \n",
       "72950  [<, h, >, have, foreign, exchange, trade, fore...   \n",
       "\n",
       "                                          reduced_tokens  year  month  day  \\\n",
       "0      [professor, public, policy, university, bank, ...    10      1    4   \n",
       "1      [network, device, huge, potential, ensure, dif...    10      1    4   \n",
       "2      [answer, base, black, way, form, little, build...    10      1    6   \n",
       "3      [project, brain, project, let, control, game, ...    10      1    7   \n",
       "4      [robot, robot, robot, meet, engine, programme,...    10      1    9   \n",
       "...                                                  ...   ...    ...  ...   \n",
       "72946  [president, commission, release, statement, we...    20     10   31   \n",
       "72947  [good, idea, pay, credit, course, month, year,...    20     10   31   \n",
       "72948  [deliver, plan, launch, news, different, point...    20     10   31   \n",
       "72949  [increase, economic, economic, make, shift, me...    20     10   31   \n",
       "72950  [foreign, exchange, trade, current, state, sec...    20     10   31   \n",
       "\n",
       "                                         tokenized_sents  \\\n",
       "0      [[<, p], [>], [Kenneth, Rogoff, Professor, of,...   \n",
       "1      [[<, p, >], [Networked, sensors, and, devices,...   \n",
       "2      [[<, h], [>, LEGO, Universe], [colorful, plast...   \n",
       "3      [[<, h], [>], [Exclusive], [Inside, Project, N...   \n",
       "4      [[<, h], [>], [Roxxxy], [the, Sex, Robot, Debu...   \n",
       "...                                                  ...   \n",
       "72946  [[<, p, >, President, Cyril, Ramaphosa, 's, 4I...   \n",
       "72947  [[<, p], [>, While, it, might, seem, like, a, ...   \n",
       "72948  [[<, h], [>], [Wavemaker, 's, new, bespoke, OS...   \n",
       "72949  [[<, h], [>], [The, Increasing, Influence, of,...   \n",
       "72950  [[<, h], [>, AI, in, Foreign, Exchange, Tradin...   \n",
       "\n",
       "                                        normalized_sents  \n",
       "0      [[<, p], [>], [kenneth, rogoff, professor, eco...  \n",
       "1      [[<, p, >], [networked, sensors, devices, huge...  \n",
       "2      [[<, h], [>, lego, universe], [colorful, plast...  \n",
       "3      [[<, h], [>], [exclusive], [inside, project, n...  \n",
       "4      [[<, h], [>], [roxxxy], [sex, robot, debuts, a...  \n",
       "...                                                  ...  \n",
       "72946  [[<, p, >, president, cyril, ramaphosa, 4ir, c...  \n",
       "72947  [[<, p], [>, like, good, idea, pay, credit, co...  \n",
       "72948  [[<, h], [>], [wavemaker, new, bespoke, os, de...  \n",
       "72949  [[<, h], [>], [increasing, influence, cryptocu...  \n",
       "72950  [[<, h], [>, ai, foreign, exchange, trading], ...  \n",
       "\n",
       "[72951 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(sample['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_ai', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text.to_frame().to_csv(r'test_text_ai', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-tuned on Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ai = AutoTokenizer.from_pretrained(\"C://NOW/output_ai_gpt\")\n",
    "model_ai = AutoModelWithLMHead.from_pretrained(\"C://NOW/output_ai_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite activity is to watch a video of a video game and then play it back to the computer. I love watching video games and I love watching video games because they are fun and I love watching them because they are fun and I love watching them\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My favorite activity is\"\n",
    "\n",
    "input = tokenizer_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite activity is to play with your friends. I love to play with my friends. I love to play with my friends. I love to play with my friends. I love to play with my friends. I love to play with my friends.\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My favorite activity is\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution to the world's problems is to create a new kind of economy that works for everyone.\"\n",
      "\n",
      "\"We are not going to be able to do that without the help of the private sector,\" he added.\n",
      "\n",
      "\"We need to\n"
     ]
    }
   ],
   "source": [
    "sequence = \"The solution to the world's problems is\"\n",
    "\n",
    "input = tokenizer_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution to the world's problems is to create a world where people are free to choose their own path.\n",
      "\n",
      "The world is not a place where people are free to choose their own path. It is a place where people are free to choose\n"
     ]
    }
   ],
   "source": [
    "sequence = \"The solution to the world's problems is\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my drink, I would like some artificial sweetener to help me feel more like a person. I think that is what I want to do with my life.\"\n",
      "\n",
      "\"I think that is what I want to do with my life. I\n"
     ]
    }
   ],
   "source": [
    "sequence = \"In my drink, I would like some artificial\"\n",
    "\n",
    "input = tokenizer_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my drink, I would like some artificial sweetener. I don't know if it's a lemon or a lime. I don't know if it's a lemon or a lime. I don't know if it's a lemon or a lime\n"
     ]
    }
   ],
   "source": [
    "sequence = \"In my drink, I would like some artificial\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like some artificial intelligence to be able to do things like this.\"\n",
      "\n",
      "\"I think it would be a great idea to have a system that could do things like this in the future,\" he added.\n",
      "\n",
      "\"I think it would\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I would like some artificial\"\n",
    "\n",
    "input = tokenizer_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like some artificial intelligence to be able to do that.\n",
      "\n",
      "\"I think it's a good idea. I think it's a good idea. I think it's a good idea. I think it's a good idea. I think\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I would like some artificial\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I only fine-tuned with a small number of texts, due to the constraints in Google Collab (takes more time to download\n",
    "#than the runtime available), but the AI-trained text seems more oriented towards AI-adjacent topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check out our UK and GB embeddings - how do you think the two models will differ? Maybe in the way different words relate to each other in the same sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roberta_us_model_embedding = RobertaModel.from_pretrained('roberta_us')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta_us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Do you have your chips with fish or with salsa?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
    "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
    "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
    "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
    "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roberta_gb_model_embedding = RobertaModel.from_pretrained('roberta_gb')\n",
    "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta_gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C://NOW/aiTextsWithTokens.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "US    15875\n",
       "IN     8896\n",
       "SG     8032\n",
       "GB     5788\n",
       "CA     5719\n",
       "ZA     5049\n",
       "AU     4935\n",
       "IE     4692\n",
       "NZ     2601\n",
       "MY     2200\n",
       "??     1985\n",
       "NG     1896\n",
       "PH     1309\n",
       "PK     1194\n",
       "HK     1100\n",
       "KE      547\n",
       "GH      375\n",
       "LK      306\n",
       "BD      235\n",
       "?        98\n",
       "TZ       82\n",
       "JM       30\n",
       "y         4\n",
       "          3\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "USsample = df[df['country'] == 'US'].sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "INsample = df[df['country'] == 'IN'].sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text_us_ai, test_text_us_ai = train_test_split(USsample['text'], test_size=0.2)\n",
    "train_text_in_ai, test_text_in_ai = train_test_split(INsample['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_us_ai.to_frame().to_csv(r'train_text_us_ai', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_us_ai.to_frame().to_csv(r'test_text_us_ai', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_in_ai.to_frame().to_csv(r'train_text_in_ai', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_in_ai.to_frame().to_csv(r'test_text_in_ai', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-tuned on Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_us_ai = AutoTokenizer.from_pretrained(\"C://NOW/output_us_ai_gpt\")\n",
    "model_us_ai = AutoModelWithLMHead.from_pretrained(\"C://NOW/output_us_ai_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_in_ai = AutoTokenizer.from_pretrained(\"C://NOW/output_in_ai_gpt\")\n",
    "model_in_ai = AutoModelWithLMHead.from_pretrained(\"C://NOW/output_in_ai_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite activity is to play with your friends and play with your computer. You can play with your friends and play with your computer. You can play with your friends and play with your computer. You can play with your friends and play with your computer\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My favorite activity is\"\n",
    "\n",
    "input = tokenizer_us_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_us_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_us_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite activity is to play with your friends and play with them on the same day. <p> The best part about playing with friends is that you can play with them on the same day. <p> The best part about playing with friends\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My favorite activity is\"\n",
    "\n",
    "input = tokenizer_in_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_in_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_in_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution to the world's problems is to create a new kind of economy that is more efficient and more sustainable............................\n"
     ]
    }
   ],
   "source": [
    "sequence = \"The solution to the world's problems is\"\n",
    "\n",
    "input = tokenizer_us_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_us_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_us_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution to the world's problems is to create a new kind of economy. <p> The world is now in a new era of digitalisation. The digital economy is transforming the way we live and work. It is transforming the way we work\n"
     ]
    }
   ],
   "source": [
    "sequence = \"The solution to the world's problems is\"\n",
    "\n",
    "input = tokenizer_in_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_in_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_in_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my drink, I would like some artificial sweetener to help me feel more like a drinker. I would like to drink more of it and I would like to drink more of it and I would like to drink more of it and I would\n"
     ]
    }
   ],
   "source": [
    "sequence = \"In my drink, I would like some artificial\"\n",
    "\n",
    "input = tokenizer_us_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_us_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_us_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my drink, I would like some artificial sweeteners and I would like to see how they work in the future. <p> I have a lot of interest in artificial sweeteners. I have been looking at artificial sweeteners for a long time\n"
     ]
    }
   ],
   "source": [
    "sequence = \"In my drink, I would like some artificial\"\n",
    "\n",
    "input = tokenizer_in_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_in_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_in_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like some artificial intelligence to be able to predict the future and predict the future in a way that is more accurate than the human eye can do. I would like to see artificial intelligence be able to predict the future in a way that is more\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I would like some artificial\"\n",
    "\n",
    "input = tokenizer_us_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_us_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_us_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like some artificial intelligence to be able to do the job of a human being. <p> The AI is not just a tool for humans, but also for other animals. It is also a tool for humans to learn, to learn,\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I would like some artificial\"\n",
    "\n",
    "input = tokenizer_in_ai.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_in_ai.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_in_ai.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is not a clear distinction here, perhaps due to the constraints on model size in Google Collab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
